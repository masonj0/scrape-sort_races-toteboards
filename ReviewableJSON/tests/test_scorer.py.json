{
    "filepath": "./tests/test_scorer.py",
    "content": "import pytest\nfrom unittest.mock import patch\nfrom src.paddock_parser.models import Race, Runner\nfrom src.paddock_parser.scorer import RaceScorer, score_races\n\n@pytest.fixture\ndef sample_weights():\n    \"\"\"Fixture for sample scorer weights.\"\"\"\n    return {\n        \"FIELD_SIZE_WEIGHT\": 0.5,\n        \"FAVORITE_ODDS_WEIGHT\": 0.3,\n        \"CONTENTION_WEIGHT\": 0.2,\n    }\n\n@pytest.fixture\ndef race_with_clear_favorite():\n    \"\"\"Fixture for a race with a clear favorite and low contention.\"\"\"\n    return Race(\n        race_id=\"R1\", venue=\"Test\", race_time=\"14:00\", source=\"Test\", race_number=1, is_handicap=False, number_of_runners=8,\n        runners=[\n            Runner(name=\"Favorite\", odds=2.0),\n            Runner(name=\"Runner2\", odds=8.0),\n        ]\n    )\n\n@pytest.fixture\ndef race_with_high_contention():\n    \"\"\"Fixture for a race with high contention (two joint favorites).\"\"\"\n    return Race(\n        race_id=\"R2\", venue=\"Test\", race_time=\"15:00\", source=\"Test\", race_number=2, is_handicap=True, number_of_runners=10,\n        runners=[\n            Runner(name=\"JointFav1\", odds=4.5),\n            Runner(name=\"JointFav2\", odds=4.5),\n        ]\n    )\n\ndef test_race_scorer_initialization(sample_weights):\n    scorer = RaceScorer(weights=sample_weights)\n    assert scorer.weights == sample_weights\n\n@patch('src.paddock_parser.scorer.SCORER_WEIGHTS', {\"FIELD_SIZE_WEIGHT\": 0.5, \"FAVORITE_ODDS_WEIGHT\": 0.3, \"CONTENTION_WEIGHT\": 0.2})\ndef test_race_scorer_initialization_from_config():\n    scorer = RaceScorer()\n    assert scorer.weights[\"FIELD_SIZE_WEIGHT\"] == 0.5\n\ndef test_score_calculation_low_contention(sample_weights, race_with_clear_favorite):\n    scorer = RaceScorer(weights=sample_weights)\n    scores = scorer.score(race_with_clear_favorite)\n    assert scores['field_size_score'] == pytest.approx(1/8)\n    assert scores['favorite_odds_score'] == pytest.approx(2.0)\n    assert scores['contention_score'] == pytest.approx(6.0)\n    expected_score = (1/8 * 0.5) + (2.0 * 0.3) + (6.0 * 0.2)\n    assert scores['total_score'] == pytest.approx(expected_score)\n\ndef test_score_calculation_high_contention(sample_weights, race_with_high_contention):\n    scorer = RaceScorer(weights=sample_weights)\n    scores = scorer.score(race_with_high_contention)\n    assert scores['field_size_score'] == pytest.approx(1/10)\n    assert scores['favorite_odds_score'] == pytest.approx(4.5)\n    assert scores['contention_score'] == pytest.approx(0.0)\n    expected_score = (0.1 * 0.5) + (4.5 * 0.3) + (0.0 * 0.2)\n    assert scores['total_score'] == pytest.approx(expected_score)\n\n@patch('src.paddock_parser.scorer.SCORER_WEIGHTS', {\"FIELD_SIZE_WEIGHT\": 0.5, \"FAVORITE_ODDS_WEIGHT\": 0.3, \"CONTENTION_WEIGHT\": 0.2})\ndef test_score_races_function(race_with_clear_favorite):\n    races = [race_with_clear_favorite]\n    scored_races = score_races(races)\n    expected_score = (1/8 * 0.5) + (2.0 * 0.3) + (6.0 * 0.2)\n    assert hasattr(scored_races[0], 'score')\n    assert hasattr(scored_races[0], 'scores')\n    assert scored_races[0].score == pytest.approx(expected_score)\n    assert scored_races[0].scores['total_score'] == pytest.approx(expected_score)\n"
}