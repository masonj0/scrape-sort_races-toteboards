{
    "filepath": "./tests/ui/test_terminal_ui_rich.py",
    "content": "import pytest\nfrom unittest.mock import patch, call, AsyncMock\nfrom datetime import datetime\nfrom paddock_parser.base import NormalizedRace\nfrom paddock_parser.ui.terminal_ui import TerminalUI\n\n@pytest.fixture\ndef sample_scored_races():\n    \"\"\"Fixture for a list of scored races, now returning NormalizedRace as the pipeline does.\"\"\"\n    race1 = NormalizedRace(\n        race_id=\"R1\", track_name=\"Newmarket\", race_number=1, post_time=datetime(2025, 9, 7, 14, 30),\n        number_of_runners=8, race_type=\"Handicap\"\n    )\n    setattr(race1, 'score', 2.5)\n    setattr(race1, 'scores', {\n        \"total_score\": 2.5, \"field_size_score\": 0.125,\n        \"favorite_odds_score\": 2.0, \"contention_score\": 5.0\n    })\n\n    race2 = NormalizedRace(\n        race_id=\"R2\", track_name=\"Goodwood\", race_number=2, post_time=datetime(2025, 9, 7, 14, 45),\n        number_of_runners=10, race_type=\"Stakes\"\n    )\n    setattr(race2, 'score', 1.8)\n    setattr(race2, 'scores', {\n        \"total_score\": 1.8, \"field_size_score\": 0.1,\n        \"favorite_odds_score\": 4.5, \"contention_score\": 1.0\n    })\n    return [race1, race2]\n\n@patch('paddock_parser.ui.terminal_ui.Table')\n@patch('paddock_parser.ui.terminal_ui.Console')\ndef test_display_scoring_report_uses_rich_table(MockConsole, MockTable, sample_scored_races):\n    mock_console_instance = MockConsole()\n    mock_table_instance = MockTable.return_value\n    ui = TerminalUI(console=mock_console_instance)\n\n    ui.display_scoring_report(sample_scored_races)\n\n    MockTable.assert_called_once_with(title=\"[bold green]Dynamic Scoring Report[/bold green]\")\n    expected_calls = [\n        call(\"Race Time\", style=\"cyan\"),\n        call(\"Venue\", style=\"magenta\"),\n        call(\"Race #\", style=\"white\"),\n        call(\"Runners\", style=\"white\"),\n        call(\"Handicap\", style=\"white\"),\n        call(\"Fav Odds\", style=\"yellow\"),\n        call(\"Contention\", style=\"yellow\"),\n        call(\"Field Size\", style=\"yellow\"),\n        call(\"Total Score\", style=\"bold green\"),\n    ]\n    mock_table_instance.add_column.assert_has_calls(expected_calls, any_order=False)\n    expected_row_calls = [\n        call('14:30', 'Newmarket', '1', '8', 'Yes', '2.00', '5.00', '0.125', '2.50'),\n        call('14:45', 'Goodwood', '2', '10', 'No', '4.50', '1.00', '0.100', '1.80'),\n    ]\n    mock_table_instance.add_row.assert_has_calls(expected_row_calls)\n    mock_console_instance.print.assert_called_once_with(mock_table_instance)\n\n@patch('paddock_parser.ui.terminal_ui.Table')\n@patch('paddock_parser.ui.terminal_ui.analyze_log_file')\n@patch('paddock_parser.ui.terminal_ui.Console')\ndef test_display_log_analysis_report_uses_rich_table(MockConsole, MockAnalyze, MockTable):\n    mock_console_instance = MockConsole()\n    mock_table_instance = MockTable.return_value\n    mock_log_counts = {\"INFO\": 10, \"WARNING\": 2, \"ERROR\": 1}\n    MockAnalyze.return_value = mock_log_counts\n    ui = TerminalUI(console=mock_console_instance)\n    ui.display_log_analysis_report()\n    MockAnalyze.assert_called_once()\n    MockTable.assert_called_once_with(title=\"[bold blue]Log File Analysis[/bold blue]\")\n    expected_column_calls = [\n        call(\"Log Level\", style=\"cyan\"),\n        call(\"Count\", style=\"magenta\", justify=\"right\"),\n    ]\n    mock_table_instance.add_column.assert_has_calls(expected_column_calls, any_order=False)\n    expected_row_calls = [\n        call(\"ERROR\", \"1\"),\n        call(\"INFO\", \"10\"),\n        call(\"WARNING\", \"2\"),\n    ]\n    mock_table_instance.add_row.assert_has_calls(expected_row_calls, any_order=True)\n    mock_console_instance.print.assert_called_with(mock_table_instance)\n\n@pytest.mark.anyio\n@patch('paddock_parser.ui.terminal_ui.run_pipeline', new_callable=AsyncMock)\n@patch('paddock_parser.ui.terminal_ui.TerminalUI.display_scoring_report')\n@patch('paddock_parser.ui.terminal_ui.Console')\nasync def test_run_scoring_report_orchestrates_correctly(MockConsole, MockDisplay, MockRunPipeline, sample_scored_races):\n    mock_console_instance = MockConsole()\n    MockRunPipeline.return_value = sample_scored_races\n    ui = TerminalUI(console=mock_console_instance)\n    await ui._run_scoring_report()\n    mock_console_instance.status.assert_called_once_with(\"Fetching data from providers...\", spinner=\"dots\")\n    MockRunPipeline.assert_called_once()\n    MockDisplay.assert_called_once_with(sample_scored_races)\n"
}