{
  "README.md": "# Checkmate V7: The Live Cockpit\\n\\nThis project is a real-time, web-based horse racing analysis engine. It leverages a powerful, battle-tested Python backend to fetch and analyze live data from multiple sources, exposing the results through a lightweight and elegant Vanilla JavaScript frontend.\\n\\n---\\n\\n## Core Concept\\n\\nThe architecture is a modern, two-stack system as defined in the `ARCHITECTURAL_MANDATE_V8.1.md`:\\n\\n1.  **THE ENGINE (Python/FastAPI):** A powerful, headless data processing application that performs all heavy lifting and exposes its capabilities via a JSON API.\\n2.  **THE COCKPIT (Vanilla JS):** A lightweight, single-page web application that serves as the project's primary user interface. It is a pure client of The Engine.\\n\\n## Getting Started\\n\\nThe application is designed to be run as a local web server.\\n\\n1.  **Install Dependencies:**\\n    ```bash\\n    pip install -r checkmate_web/requirements.txt\\n    ```\\n2.  **Run the Web Server:**\\n    ```bash\\n    cd checkmate_web\\n    uvicorn main:app --reload\\n    ```\\n3.  **Access the Cockpit:**\\n    Open a web browser and navigate to `http://127.0.0.1:8000`.\\n\\n## Project Status\\n\\nThe project is currently executing **Phase 1 of ROADMAP V6.0: \\\"The Engine Room.\\\"** The immediate goal is to stand up the core FastAPI server and port our perfected portable engine logic into a modular, web-ready format.",
  "ARCHITECTURAL_MANDATE.md": "# ARCHITECTURAL MANDATE V9.0: THE QUAD-HYBRID PLATFORM\\n\\nThis document is the definitive architectural and strategic mandate for the Checkmate V8 project. It supersedes all previous architectural documents and defines the Quad-Hybrid platform as the sole North Star for all development efforts.\\n\\n## 1.0 Core Principles\\n\\nThese principles are the non-negotiable foundation of our engineering philosophy.\\n\\n-   **Multi-Lingual Specialization:** We will use the absolute best tool for each task. Python for data collection, Rust for high-performance computation, TypeScript for ubiquitous web access, and C# for a native desktop experience.\\n\\n-   **The Engine Does the Thinking:** The backend (Python/Rust) is responsible for all heavy computation. It delivers pre-analyzed, scored, and qualified results to the display layers. The frontends are lean, fast, and focused on presentation.\\n\\n-   **The Asynchronous Bridge:** All components are decoupled and communicate asynchronously through a shared SQLite database. This is the heart of the system, providing resilience and scalability.\\n\\n-   **Assume Failure:** All components will be built with a production-grade, pessimistic mindset, incorporating robust error handling, fallbacks, and comprehensive logging.\\n\\n## 2.0 The Four Pillars of the Architecture\\n\\n1.  **The Collection Corps (Python Service):** A silent, autonomous Windows service. Its sole purpose is to orchestrate a fleet of data adapters, fetching and parsing real-world data concurrently.\\n\\n2.  **The Analysis Core (Rust Engine):** A compiled, memory-safe, hyper-performance library. Its purpose is all heavy computation, including scoring, analysis, and future machine learning inference.\\n\\n3.  **The Digital Front (TypeScript Web Platform):** A modern, real-time web application. Its purpose is to provide ubiquitous, multi-user, and mobile-responsive access to the system's data.\\n\\n4.  **The Command Deck (C# Desktop App):** A native Windows desktop application. Its purpose is to provide the ultimate power-user experience with deep OS integration and zero-latency interaction.\\n\\n## 3.0 Strategic Development Order\\n\\nThe campaign will proceed in the following strategic order, with each phase building upon the last:\\n\\n1.  **SOLIDIFY THE CORE:** Python Service + Rust Engine + SQLite Bridge. (Status: ✅ COMPLETE)\\n2.  **BUILD THE DIGITAL FRONT:** TypeScript Web Platform. (Status: ⚠️ IN PROGRESS)\\n3.  **BUILD THE COMMAND DECK:** C# Desktop Application. (Status: ❌ PLANNED)",
  "shared_database/schema.sql": "-- OPTIMIZED DATABASE SCHEMA FOR HYBRID ARCHITECTURE\\nCREATE TABLE IF NOT EXISTS live_races (\\n    race_id TEXT PRIMARY KEY,\\n    track_name TEXT NOT NULL,\\n    race_number INTEGER,\\n    post_time DATETIME NOT NULL,\\n    raw_data_json TEXT,           -- Complete race data from Python\\n    checkmate_score REAL NOT NULL,\\n    qualified BOOLEAN NOT NULL,\\n    trifecta_factors_json TEXT,   -- Analysis factors for display\\n    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\\n);\\n\\nCREATE TABLE IF NOT EXISTS adapter_status (\\n    adapter_name TEXT PRIMARY KEY,\\n    status TEXT NOT NULL,         -- 'OK', 'ERROR', 'WARNING'\\n    last_run DATETIME NOT NULL,\\n    races_found INTEGER DEFAULT 0,\\n    execution_time_ms INTEGER DEFAULT 0,\\n    error_message TEXT,\\n    success_rate REAL DEFAULT 1.0\\n);\\n\\n-- PERFORMANCE INDEXES\\nCREATE INDEX IF NOT EXISTS idx_races_qualified_score ON live_races(qualified, checkmate_score DESC, post_time);\\n\\n-- CLEANUP TRIGGER (AUTOMATIC OLD DATA REMOVAL)\\nCREATE TRIGGER IF NOT EXISTS cleanup_old_races \\nAFTER INSERT ON live_races\\nBEGIN\\n    DELETE FROM live_races \\n    WHERE post_time < datetime('now', '-4 hours');\\nEND;",
  "shared_database/web_schema.sql": "-- ADD WEB-SPECIFIC TABLES TO EXISTING SCHEMA\\nCREATE TABLE IF NOT EXISTS web_users (\\n    user_id TEXT PRIMARY KEY,\\n    username TEXT UNIQUE NOT NULL,\\n    email TEXT UNIQUE NOT NULL,\\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP\\n);\\n\\nCREATE TABLE IF NOT EXISTS web_sessions (\\n    session_id TEXT PRIMARY KEY,\\n    user_id TEXT REFERENCES web_users(user_id),\\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\\n    expires_at DATETIME\\n);\\n\\nCREATE TABLE IF NOT EXISTS web_alerts (\\n    alert_id TEXT PRIMARY KEY,\\n    user_id TEXT REFERENCES web_users(user_id),\\n    race_id TEXT,\\n    alert_type TEXT, -- 'high_score', 'custom'\\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP\\n);\\n\\n-- RELIABLE TRIGGER TABLE FOR REAL-TIME WEB UPDATES\\nCREATE TABLE IF NOT EXISTS events (\\n    event_id INTEGER PRIMARY KEY AUTOINCREMENT,\\n    event_type TEXT NOT NULL,\\n    payload TEXT,\\n    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\\n);",
  "python_service/__init__.py": "# This file makes the python_service directory a Python package.",
  "python_service/requirements.txt": "# Core Dependencies\\npydantic\\npydantic-settings\\nbeautifulsoup4\\nlxml\\n\\n# Windows Service\\npywin32\\n\\n# NEW: Resilience & Performance Enhancements\\ncachetools  # For sophisticated in-memory caching\\nrequests    # For fallback HTTP client patterns",
  "python_service/engine.py": "# engine.py\\n# The perfected, 10/10 specification for the Python Collection Service.\\n\\nimport logging\\nimport json\\nimport subprocess\\nimport concurrent.futures\\nimport time\\nfrom abc import ABC, abstractmethod\\nfrom typing import List, Optional, Union, Dict\\nfrom datetime import datetime\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_settings import BaseSettings\\nfrom cachetools import TTLCache\\n\\n# --- Finalized Settings Model ---\\nclass Settings(BaseSettings):\\n    QUALIFICATION_SCORE: float = 75.0\\n    FIELD_SIZE_OPTIMAL_MIN: int = 4\\n    FIELD_SIZE_OPTIMAL_MAX: int = 6\\n    FIELD_SIZE_ACCEPTABLE_MIN: int = 7\\n    FIELD_SIZE_ACCEPTABLE_MAX: int = 8\\n    FIELD_SIZE_OPTIMAL_POINTS: int = 30\\n    FIELD_SIZE_ACCEPTABLE_POINTS: int = 10\\n    FIELD_SIZE_PENALTY_POINTS: int = -20\\n    FAV_ODDS_POINTS: int = 30\\n    MAX_FAV_ODDS: float = 3.5\\n    SECOND_FAV_ODDS_POINTS: int = 40\\n    MIN_2ND_FAV_ODDS: float = 4.0\\n    DATABASE_BATCH_SIZE: int = 100\\n    RUST_ENGINE_TIMEOUT: int = 10\\n    ODDS_API_KEY: Optional[str] = None\\n\\n# --- Finalized Data Models ---\\nclass Runner(BaseModel):\\n    name: str\\n    odds: Optional[float] = None\\n\\nclass Race(BaseModel):\\n    race_id: str\\n    track_name: str\\n    race_number: Optional[int] = None\\n    post_time: Optional[datetime] = None\\n    runners: List[Runner]\\n    source: Optional[str] = None\\n    checkmate_score: Optional[float] = None\\n    is_qualified: Optional[bool] = None\\n    trifecta_factors_json: Optional[str] = None\\n    analysis_details: Optional[str] = None # For advanced analysis\\n    data_quality_score: Optional[float] = None\\n\\n# --- Resilient Fetcher ---\\nclass DefensiveFetcher:\\n    def __init__(self):\\n        # In a full implementation, these would be real classes\\n        # self.rate_limiter = RateLimiter()\\n        # self.circuit_breaker = CircuitBreaker()\\n        self.logger = logging.getLogger(self.__class__.__name__)\\n\\n    def get(self, url: str, headers: Optional[Dict[str, str]] = None) -> Union[dict, str, None]:\\n        # This method will be enhanced with retry logic, circuit breaking, and rate limiting.\\n        # For now, it retains the proven curl implementation.\\n        try:\\n            command = [\\\"curl\\\", \\\"-s\\\", \\\"-L\\\", \\\"--tlsv1.2\\\", \\\"--http1.1\\\"]\\n            if headers:\\n                for key, value in headers.items():\\n                    command.extend([\\\"-H\\\", f\\\"{key}: {value}\\\"])\\n            command.append(url)\\n\\n            result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=15)\\n            response_text = result.stdout\\n\\n            try:\\n                return json.loads(response_text)\\n            except json.JSONDecodeError:\\n                logging.warning(f\\\"Failed to decode JSON from {url}, returning raw text.\\\")\\n                return response_text # Return text as fallback\\n        except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:\\n            logging.error(f\\\"CRITICAL: curl GET failed for {url}. Details: {e}\\\")\\n            return None\\n\\n# --- Enhanced Adapters ---\\nclass BaseAdapterV8(ABC):\\n    def __init__(self, fetcher: DefensiveFetcher, settings: Settings):\\n        self.fetcher = fetcher\\n        self.settings = settings\\n        self.cache = TTLCache(maxsize=100, ttl=300) # 5-minute cache per adapter\\n    @abstractmethod\\n    def fetch_races(self) -> List[Race]: raise NotImplementedError\\n\\nclass EnhancedTVGAdapter(BaseAdapterV8):\\n    def fetch_races(self) -> List[Race]:\\n        # This method will be enhanced with caching and more robust parsing\\n        # as defined in the new canonical pseudocode.\\n        return [] # Placeholder for full implementation\\n\\nPRODUCTION_ADAPTERS = [EnhancedTVGAdapter]\\n\\n# --- Supercharged Orchestrator ---\\nclass SuperchargedOrchestrator:\\n    def __init__(self, settings: Settings):\\n        self.fetcher = DefensiveFetcher()\\n        self.settings = settings\\n        self.adapters = [Adapter(self.fetcher, self.settings) for Adapter in PRODUCTION_ADAPTERS]\\n        # self.performance_monitor = PerformanceMonitor()\\n        # self.data_validator = DataValidator()\\n        self.logger = logging.getLogger(self.__class__.__name__)\\n\\n    def get_races_parallel(self) -> tuple[list[Race], list[dict]]:\\n        # This method will be enhanced with full performance monitoring and data validation.\\n        # For now, it implements the core concurrent fetching logic\\n        all_races, statuses = [], []\\n        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.adapters)) as executor:\\n            future_to_adapter = {executor.submit(adapter.fetch_races): adapter for adapter in self.adapters}\\n            for future in concurrent.futures.as_completed(future_to_adapter):\\n                adapter = future_to_adapter[future]\\n                try:\\n                    races = future.result()\\n                    all_races.extend(races)\\n                    # Create a basic status receipt for now\\n                    statuses.append({'adapter_id': adapter.__class__.__name__, 'status': 'OK', 'races_found': len(races)})\\n                except Exception as e:\\n                    self.logger.error(f\\\"Adapter {adapter.__class__.__name__} failed: {e}\\\", exc_info=True)\\n                    statuses.append({'adapter_id': adapter.__class__.__name__, 'status': 'ERROR', 'error_message': str(e)})\\n        return all_races, statuses\\n\\n\\n# --- Enhanced Trifecta Analyzer Stub ---\\nclass EnhancedTrifectaAnalyzer:\\n    def __init__(self, settings: Settings):\\n        self.settings = settings\\n        # TODO: Load ML model and historical data\\n\\n    def analyze_race_advanced(self, race: Race) -> Race:\\n        # TODO: Implement full analysis with ML and historical factors\\n        race.checkmate_score = 50.0 # Placeholder\\n        race.is_qualified = race.checkmate_score >= self.settings.QUALIFICATION_SCORE\\n        race.analysis_details = json.dumps({'base_score': 50.0})\\n        return race",
  "python_service/checkmate_service.py": "# checkmate_service.py\\n# The main service runner, upgraded to the final 10/10 architecture.\\n\\nimport time\\nimport logging\\nimport sqlite3\\nimport json\\nimport os\\nimport threading\\nfrom datetime import datetime\\nfrom .engine import SuperchargedOrchestrator, EnhancedTrifectaAnalyzer, Settings, Race\\nfrom typing import List\\n\\nclass DatabaseHandler:\\n    def __init__(self, db_path: str):\\n        self.db_path = db_path\\n        self.logger = logging.getLogger(self.__class__.__name__)\\n        self._setup_database()\\n\\n    def _get_connection(self):\\n        return sqlite3.connect(self.db_path, timeout=10)\\n\\n    def _setup_database(self):\\n        try:\\n            # Construct robust paths to both schema files\\n            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\\n            schema_path = os.path.join(base_dir, 'shared_database', 'schema.sql')\\n            web_schema_path = os.path.join(base_dir, 'shared_database', 'web_schema.sql')\\n            \\n            with open(schema_path, 'r') as f:\\n                schema = f.read()\\n            with open(web_schema_path, 'r') as f:\\n                web_schema = f.read()\\n\\n            with self._get_connection() as conn:\\n                cursor = conn.cursor()\\n                cursor.executescript(schema)\\n                cursor.executescript(web_schema)\\n                conn.commit()\\n            self.logger.info(f\\\"Database schemas applied successfully from {schema_path} and {web_schema_path}.\\\")\\n        except Exception as e:\\n            self.logger.critical(f\\\"FATAL: Could not set up database from schema files: {e}\\\", exc_info=True)\\n            raise\\n\\n    def update_races_and_status(self, races: List[Race], statuses: List[dict]):\\n        with self._get_connection() as conn:\\n            cursor = conn.cursor()\\n            for race in races:\\n                cursor.execute(\\\"\\\"\\\"\\n                    INSERT OR REPLACE INTO live_races \\n                    (race_id, track_name, race_number, post_time, raw_data_json, checkmate_score, qualified, trifecta_factors_json, updated_at) \\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\\n                \\\"\\\"\\\", (\\n                    race.race_id, race.track_name, race.race_number, race.post_time,\\n                    race.model_dump_json(), race.checkmate_score, race.is_qualified,\\n                    race.trifecta_factors_json, datetime.now()\\n                ))\\n            for status in statuses:\\n                cursor.execute(\\\"\\\"\\\"\\n                    INSERT OR REPLACE INTO adapter_status (adapter_name, status, last_run, races_found, error_message, execution_time_ms) \\n                    VALUES (?, ?, ?, ?, ?, ?)\\n                \\\"\\\"\\\", (\\n                    status.get('adapter_id'), status.get('status'), status.get('timestamp'), \\n                    status.get('races_found'), status.get('error_message'), int(status.get('response_time', 0) * 1000)\\n                ))\\n            \\n            if races or statuses:\\n                cursor.execute(\\\"INSERT INTO events (event_type, payload) VALUES (?, ?)\\\", \\n                               ('RACES_UPDATED', json.dumps({'race_count': len(races)})))\\n\\n            conn.commit()\\n        self.logger.info(f\\\"Database updated with {len(races)} races and {len(statuses)} adapter statuses.\\\")\\n\\nclass CheckmateBackgroundService:\\n    def __init__(self):\\n        self.settings = Settings()\\n        db_path = os.path.join(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')), \\\"shared_database\\\", \\\"races.db\\\")\\n        self.db_handler = DatabaseHandler(db_path)\\n        self.orchestrator = SuperchargedOrchestrator(self.settings)\\n        self.analyzer = EnhancedTrifectaAnalyzer(self.settings)\\n        self.stop_event = threading.Event()\\n        self.logger = logging.getLogger(self.__class__.__name__)\\n\\n    def run_continuously(self, interval_seconds: int = 60):\\n        self.logger.info(\\\"Antifragile Collector Service starting continuous run.\\\")\\n        while not self.stop_event.is_set():\\n            self.logger.info(\\\"Starting advanced data collection and analysis cycle.\\\")\\n            try:\\n                races, statuses = self.orchestrator.get_races_parallel()\\n                analyzed_races = [self.analyzer.analyze_race_advanced(race) for race in races]\\n                # This will later be replaced by the Rust engine call\\n                self.db_handler.update_races_and_status(analyzed_races, statuses)\\n            except Exception as e:\\n                self.logger.critical(f\\\"FATAL error in main service loop: {e}\\\", exc_info=True)\\n            \\n            self.stop_event.wait(interval_seconds)\\n\\n    def start(self):\\n        self.stop_event.clear()\\n        self.thread = threading.Thread(target=self.run_continuously)\\n        self.thread.daemon = True\\n        self.thread.start()\\n        self.logger.info(\\\"CheckmateBackgroundService started.\\\")\\n\\n    def stop(self):\\n        self.stop_event.set()\\n        if hasattr(self, 'thread') and self.thread.is_alive():\\n            self.thread.join(timeout=10)\\n        self.logger.info(\\\"CheckmateBackgroundService stopped.\\\")",
  "python_service/windows_service_wrapper.py": "# windows_service_wrapper.py\\n\\nimport servicemanager\\nimport win32service\\nimport win32serviceutil\\nimport win32event\\nimport sys\\nimport os\\nimport logging\\n\\n# Add the service's directory to the Python path\\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\\nfrom checkmate_service import CheckmateBackgroundService\\n\\nclass CheckmateWindowsService(win32serviceutil.ServiceFramework):\\n    _svc_name_ = \\\"CheckmateV8Service\\\"\\n    _svc_display_name_ = \\\"Checkmate V8 Racing Analysis Service\\\"\\n    _svc_description_ = \\\"Continuously fetches and analyzes horse racing data.\\\"\\n\\n    def __init__(self, args):\\n        win32serviceutil.ServiceFramework.__init__(self, args)\\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\\n        self.checkmate_service = CheckmateBackgroundService()\\n        # Configure logging to use the Windows Event Log\\n        logging.basicConfig(level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s', handlers=[servicemanager.LogHandler()])\\n\\n    def SvcStop(self):\\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\\n        self.checkmate_service.stop()\\n        win32event.SetEvent(self.hWaitStop)\\n        self.ReportServiceStatus(win32service.SERVICE_STOPPED)\\n\\n    def SvcDoRun(self):\\n        servicemanager.LogMsg(servicemanager.EVENTLOG_INFORMATION_TYPE, servicemanager.PYS_SERVICE_STARTED, (self._svc_name_, ''))\\n        self.main()\\n\\n    def main(self):\\n        self.checkmate_service.start()\\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\\n\\nif __name__ == '__main__':\\n    if len(sys.argv) == 1:\\n        servicemanager.Initialize()\\n        servicemanager.PrepareToHostSingle(CheckmateWindowsService)\\n        servicemanager.StartServiceCtrlDispatcher()\\n    else:\\n        win32serviceutil.HandleCommandLine(CheckmateWindowsService)",
  "rust_engine/Cargo.toml": "[package]\\nname = \\\"checkmate_engine\\\"\\nversion = \\\"1.0.0\\\"\\nedition = \\\"2021\\\"\\n\\n[[bin]]\\nname = \\\"checkmate_engine\\\"\\npath = \\\"src/main.rs\\\"\\n\\n[lib]\\nname = \\\"checkmate_engine_lib\\\"\\npath = \\\"src/lib.rs\\\"\\ncrate-type = [\\\"cdylib\\\"]\\n\\n[dependencies]\\nserde = { version = \\\"1.0\\\", features = [\\\"derive\\\"] }\\nserde_json = \\\"1.0\\\"\\nrayon = \\\"1.7\\\"",
  "rust_engine/src/lib.rs": "// lib.rs - Synthesized, production-grade Rust implementation of the Analysis Core\\n\\nuse rayon::prelude::*;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::ffi::{CStr, CString};\nuse std::os::raw::c_char;\nuse std::panic;\n\n// --- Data Structures for Serialization (JSON) ---\n\n#[derive(Serialize, Deserialize, Debug)]\npub struct Runner {\n    pub name: String,\n    pub odds: Option<f64>,\n}\n\n#[derive(Serialize, Deserialize, Debug)]\npub struct RaceData {\n    pub race_id: String,\n    pub runners: Vec<Runner>,\n}\n\n#[derive(Serialize, Deserialize, Debug)]\npub struct AnalysisSettings {\n    #[serde(rename = \\\"QUALIFICATION_SCORE\\\")]\n    pub qualification_score: f64,\n    #[serde(rename = \\\"FIELD_SIZE_OPTIMAL_MIN\\\")]\n    pub field_size_optimal_min: usize,\n    #[serde(rename = \\\"FIELD_SIZE_OPTIMAL_MAX\\\")]\n    pub field_size_optimal_max: usize,\n    #[serde(rename = \\\"FIELD_SIZE_ACCEPTABLE_MIN\\\")]\n    pub field_size_acceptable_min: usize,\n    #[serde(rename = \\\"FIELD_SIZE_ACCEPTABLE_MAX\\\")]\n    pub field_size_acceptable_max: usize,\n    #[serde(rename = \\\"FIELD_SIZE_OPTIMAL_POINTS\\\")]\n    pub field_size_optimal_points: f64,\n    #[serde(rename = \\\"FIELD_SIZE_ACCEPTABLE_POINTS\\\")]\n    pub field_size_acceptable_points: f64,\n    #[serde(rename = \\\"FIELD_SIZE_PENALTY_POINTS\\\")]\n    pub field_size_penalty_points: f64,\n    #[serde(rename = \\\"FAV_ODDS_POINTS\\\")]\n    pub fav_odds_points: f64,\n    #[serde(rename = \\\"MAX_FAV_ODDS\\\")]\n    pub max_fav_odds: f64,\n    #[serde(rename = \\\"SECOND_FAV_ODDS_POINTS\\\")]\n    pub second_fav_odds_points: f64,\n    #[serde(rename = \\\"MIN_2ND_FAV_ODDS\\\")]\n    pub min_2nd_fav_odds: f64,\n}\n\n#[derive(Serialize, Deserialize, Debug)]\npub struct RaceAnalysisRequest {\n    pub races: Vec<RaceData>,\n    pub settings: AnalysisSettings,\n}\n\n// --- Data Structures for Analysis Results ---\n\n#[derive(Serialize, Deserialize, Debug)]\npub struct FactorResult {\n    pub points: f64,\n    pub ok: bool,\n    pub reason: String,\n}\n\n#[derive(Serialize, Deserialize, Debug)]\npub struct AnalysisResult {\n    pub race_id: String,\n    pub checkmate_score: f64,\n    pub qualified: bool,\n    pub trifecta_factors: HashMap<String, FactorResult>,\n}\n\n#[derive(Serialize, Deserialize, Debug)]\npub struct RaceAnalysisResponse {\n    pub results: Vec<AnalysisResult>,\n    pub processing_time_ms: u128,\n}\n\n// --- Core Analysis Logic ---\n\nfn analyze_single_race(race: &RaceData, settings: &AnalysisSettings) -> AnalysisResult {\n    let mut score = 0.0;\n    let mut factors = HashMap::new();\n\n    let mut horses_with_odds: Vec<&Runner> = race.runners.iter().filter(|r| r.odds.is_some()).collect();\n    horses_with_odds.sort_by(|a, b| a.odds.unwrap().partial_cmp(&b.odds.unwrap()).unwrap());\n    let num_runners = horses_with_odds.len();\n\n    // Field Size Analysis\n    let field_size_result = if (settings.field_size_optimal_min..=settings.field_size_optimal_max).contains(&num_runners) {\n        FactorResult { points: settings.field_size_optimal_points, ok: true, reason: format!(\\\"Optimal field size ({})\\\", num_runners) }\n    } else if (settings.field_size_acceptable_min..=settings.field_size_acceptable_max).contains(&num_runners) {\n        FactorResult { points: settings.field_size_acceptable_points, ok: true, reason: format!(\\\"Acceptable field size ({})\\\", num_runners) }\n    } else {\n        FactorResult { points: settings.field_size_penalty_points, ok: false, reason: format!(\\\"Field size not ideal ({})\\\", num_runners) }\n    };\n    score += field_size_result.points;\n    factors.insert(\\\"fieldSize\\\".to_string(), field_size_result);\n\n    // Odds Analysis\n    if num_runners >= 2 {\n        let fav_odds = horses_with_odds[0].odds.unwrap();\n        let sec_fav_odds = horses_with_odds[1].odds.unwrap();\n\n        let fav_odds_result = if fav_odds <= settings.max_fav_odds {\n            FactorResult { points: settings.fav_odds_points, ok: true, reason: format!(\\\"Favorite odds OK ({:.2})\\\", fav_odds) }\n        } else {\n            FactorResult { points: 0.0, ok: false, reason: format!(\\\"Favorite odds too high ({:.2})\\\", fav_odds) }\n        };\n        score += fav_odds_result.points;\n        factors.insert(\\\"favoriteOdds\\\".to_string(), fav_odds_result);\n\n        let sec_fav_odds_result = if sec_fav_odds >= settings.min_2nd_fav_odds {\n            FactorResult { points: settings.second_fav_odds_points, ok: true, reason: format!(\\\"2nd Favorite OK ({:.2})\\\", sec_fav_odds) }\n        } else {\n            FactorResult { points: 0.0, ok: false, reason: format!(\\\"2nd Favorite odds too low ({:.2})\\\", sec_fav_odds) }\n        };\n        score += sec_fav_odds_result.points;\n        factors.insert(\\\"secondFavoriteOdds\\\".to_string(), sec_fav_odds_result);\n    }\n\n    AnalysisResult {\n        race_id: race.race_id.clone(),\n        checkmate_score: score,\n        qualified: score >= settings.qualification_score,\n        trifecta_factors: factors,\n    }\n}\n\n// --- FFI (Foreign Function Interface) for Python/C# ---\n\n#[no_mangle]\npub extern \\\"C\\\" fn analyze_races_ffi(input_json_ptr: *const c_char) -> *mut c_char {\n    let result = panic::catch_unwind(|| {\n        let input_json = unsafe { CStr::from_ptr(input_json_ptr).to_str().unwrap() };\n        let request: RaceAnalysisRequest = serde_json::from_str(input_json).unwrap();\n        let start_time = std::time::Instant::now();\n\n        let results: Vec<AnalysisResult> = request.races.par_iter()\n            .map(|race| analyze_single_race(race, &request.settings))\n            .collect();\n        \n        let response = RaceAnalysisResponse {\n            results,\n            processing_time_ms: start_time.elapsed().as_millis(),\n        };\n\n        let response_json = serde_json::to_string(&response).unwrap();\n        CString::new(response_json).unwrap().into_raw()\n    });\n\n    match result {\n        Ok(ptr) => ptr,\n        Err(_) => std::ptr::null_mut(),\n    }\n}\n\n#[no_mangle]\npub extern \\\"C\\\" fn free_rust_string(ptr: *mut c_char) {\n    if ptr.is_null() { return; }\n    unsafe {\n        let _ = CString::from_raw(ptr);\n    }\n}",
  "rust_engine/src/main.rs": "// src/main.rs - The CLI entry point for the Rust engine.\\n\\n// This allows the binary to use the library's code.\\nuse checkmate_engine_lib::*;\nuse std::io::{self, Read};\\n\\nfn main() {\\n    let args: Vec<String> = std::env::args().collect();\\n    \\n    if args.len() > 1 && args[1] == \\\"--analyze\\\" {\\n        let mut input = String::new();\\n        io::stdin().read_to_string(&mut input).expect(\\\"Failed to read from stdin\\\");\\n        \\n        match serde_json::from_str::<RaceAnalysisRequest>(&input) {\\n            Ok(request) => {\\n                let response = analyze_races_parallel(request);\\n                println!(\\\"{}\\\", serde_json::to_string(&response).unwrap());\\n            }\\n            Err(e) => {\\n                eprintln!(\\\"JSON parsing error in Rust engine: {}\\n\", e);\\n                std::process::exit(1);\\n            }\\n        }\\n    } else {\\n        eprintln!(\\\"Usage: {} --analyze\\n\", args[0]);\\n        std::process::exit(1);\\n    }\\n}",
  "web_platform/api_gateway/package.json": "{\\n  \\\"name\\\": \\\"checkmate-api-gateway\\\",\\n  \\\"version\\\": \\\"1.0.0\\\",\\n  \\\"main\\\": \\\"dist/server.js\\\",\\n  \\\"scripts\\\": {\\n    \\\"start\\\": \\\"ts-node src/server.ts\\\",\\n    \\\"build\\\": \\\"tsc\\\"\\n  },\\n  \\\"dependencies\\\": {\\n    \\\"express\\\": \\\"^4.18.2\\\",\\n    \\\"socket.io\\\": \\\"^4.7.2\\\",\\n    \\\"sqlite3\\\": \\\"^5.1.6\\\",\\n    \\\"sqlite\\\": \\\"^5.0.1\\\",\\n    \\\"cors\\\": \\\"^2.8.5\\\"\\n  },\\n  \\\"devDependencies\\\": {\\n    \\\"typescript\\\": \\\"^5.2.2\\\",\\n    \\\"ts-node\\\": \\\"^10.9.1\\\",\\n    \\\"@types/express\\\": \\\"^4.17.18\\\"\\n  }\\n}",
  "web_platform/api_gateway/tsconfig.json": "{\\n  \\\"compilerOptions\\\": {\\n    \\\"module\\\": \\\"CommonJS\\\",\\n    \\\"target\\\": \\\"ES2020\\\",\\n    \\\"moduleResolution\\\": \\\"node\\\",\\n    \\\"outDir\\\": \\\"dist\\\",\\n    \\\"strict\\\": true,\\n    \\\"esModuleInterop\\\": true,\\n    \\\"skipLibCheck\\\": true,\\n    \\\"forceConsistentCasingInFileNames\\\": true\\n  },\\n  \\\"include\\\": [\\\"src/**/*\\\"],\\n  \\\"exclude\\\": [\\\"node_modules\\\"]\\n}",
  "web_platform/api_gateway/src/server.ts": "// web_platform/api_gateway/src/server.ts\\n// Production-grade implementation of the API Gateway and WebSocket server.\\n\\nimport express from 'express';\\nimport { createServer } from 'http';\\nimport { Server as SocketServer } from 'socket.io';\\nimport sqlite3 from 'sqlite3';\\nimport { open, Database } from 'sqlite';\\nimport cors from 'cors';\\nimport path from 'path';\\n\\nconst app = express();\\nconst httpServer = createServer(app);\\nconst io = new SocketServer(httpServer, {\\n  cors: { origin: \\\"*\\\", methods: [\\\"GET\\\", \\\"POST\\\"] }\\n});\\n\\n// --- Database Connection ---\\nlet db: Database;\\n\\nasync function initializeDatabase() {\\n  const dbPath = path.resolve(__dirname, '..', '..', '..', 'shared_database', 'races.db');\\n  console.log(`Attempting to connect to database at: ${dbPath}`);\\n  db = await open({\\n    filename: dbPath,\\n    driver: sqlite3.Database,\\n    mode: sqlite3.OPEN_READONLY // Open in read-only mode to prevent locking issues\\n  });\\n  console.log('Database connection successful.');\\n}\\n\\napp.use(cors());\\n\\n// --- REST API Endpoints ---\\n\\napp.get('/api/races/qualified', async (req, res) => {\\n  try {\\n    const races = await db.all(`\\n      SELECT race_id, track_name, race_number, post_time, \\n             checkmate_score, trifecta_factors_json, updated_at\\n      FROM live_races \\n      WHERE qualified = 1 AND post_time > datetime('now')\\n      ORDER BY checkmate_score DESC, post_time ASC\\n    `);\\n    res.json(races.map(r => ({ ...r, trifecta_factors: JSON.parse(r.trifecta_factors_json || '{}') })));\\n  } catch (error) {\\n    console.error('API Error /api/races/qualified:', error);\\n    res.status(500).json({ error: 'Failed to fetch qualified races' });\\n  }\\n});\\n\\napp.get('/api/adapters/status', async (req, res) => {\\n  try {\\n    const statuses = await db.all(`SELECT * FROM adapter_status ORDER BY last_run DESC`);\\n    res.json(statuses);\\n  } catch (error) {\\n    console.error('API Error /api/adapters/status:', error);\\n    res.status(500).json({ error: 'Failed to fetch adapter statuses' });\\n  }\\n});\\n\\n// --- Real-time Database Polling for WebSocket ---\\n\\nasync function pollForUpdates() {\\n    try {\\n        const lastEvent = await db.get('SELECT event_id, timestamp FROM events ORDER BY event_id DESC LIMIT 1');\\n        let lastEventId = lastEvent ? lastEvent.event_id : 0;\\n\\n        setInterval(async () => {\\n            const newEvent = await db.get('SELECT event_id FROM events WHERE event_id > ? ORDER BY event_id DESC LIMIT 1', lastEventId);\\n            if (newEvent) {\\n                console.log(`New event detected (ID: ${newEvent.event_id}). Broadcasting updates.`);\\n                const updatedRaces = await db.all(`SELECT * FROM live_races WHERE qualified = 1 AND post_time > datetime('now') ORDER BY checkmate_score DESC`);\\n                io.emit('race_update', updatedRaces.map(r => ({ ...r, trifecta_factors: JSON.parse(r.trifecta_factors_json || '{}') })));\\n                lastEventId = newEvent.event_id;\\n            }\\n        }, 2000); // Poll every 2 seconds\\n    } catch (error) {\\n        console.error('Database polling failed:', error);\\n    }\\n}\\n\\n// --- Server Startup ---\\n\\nasync function startServer() {\\n  await initializeDatabase();\\n  \\n  io.on('connection', (socket) => {\\n    console.log(`Client connected: ${socket.id}`);\\n    socket.on('disconnect', () => {\\n      console.log(`Client disconnected: ${socket.id}`);\\n    });\\n  });\\n\\n  await pollForUpdates();\\n\\n  const PORT = process.env.PORT || 8080;\\n  httpServer.listen(PORT, () => {\\n    console.log(`Checkmate API Gateway running on http://localhost:${PORT}`);\\n  });\\n}\\n\\nstartServer().catch(error => {\\n    console.error(\\\"Failed to start server:\\\", error);\\n    process.exit(1);\\n});",
  "web_platform/frontend/package.json": "{\\n  \\\"name\\\": \\\"checkmate-web-platform\\\",\\n  \\\"version\\\": \\\"1.0.0\\\",\\n  \\\"private\\\": true,\\n  \\\"scripts\\\": {\\n    \\\"dev\\\": \\\"next dev\\\",\\n    \\\"build\\\": \\\"next build\\\",\\n    \\\"start\\\": \\\"next start\\\"\\n  },\\n  \\\"dependencies\\\": {\\n    \\\"next\\\": \\\"14.0.0\\\",\\n    \\\"react\\\": \\\"18.2.0\\\",\\n    \\\"react-dom\\\": \\\"18.2.0\\\",\\n    \\\"typescript\\\": \\\"5.2.0\\\",\\n    \\\"@tanstack/react-query\\\": \\\"^5.0.0\\\",\\n    \\\"socket.io-client\\\": \\\"^4.7.0\\\",\\n    \\\"recharts\\\": \\\"^2.8.0\\\",\\n    \\\"framer-motion\\\": \\\"^10.16.0\\\",\\n    \\\"tailwindcss\\\": \\\"^3.3.0\\\"\\n  }\\n}",
  "web_platform/frontend/postcss.config.js": "module.exports = {\\n  plugins: {\\n    tailwindcss: {},\\n    autoprefixer: {},\\n  },\\n}",
  "web_platform/frontend/tailwind.config.ts": "import type { Config } from 'tailwindcss'\\n\\nconst config: Config = {\\n  content: [\\n    './src/pages/**/*.{js,ts,jsx,tsx,mdx}',\\n    './src/components/**/*.{js,ts,jsx,tsx,mdx}',\\n    './app/**/*.{js,ts,jsx,tsx,mdx}',\\n  ],\\n  theme: {\\n    extend: {},\\n  },\\n  plugins: [],\\n}\\nexport default config",
  "web_platform/frontend/app/globals.css": "@tailwind base;\\n@tailwind components;\\n@tailwind utilities;",
  "web_platform/frontend/app/layout.tsx": "// web_platform/frontend/app/layout.tsx\\nimport './globals.css';\\nimport type { Metadata } from 'next';\\nimport { Inter } from 'next/font/google';\\n\\nconst inter = Inter({ subsets: ['latin'] });\\n\\nexport const metadata: Metadata = {\\n  title: 'Checkmate Live',\\n  description: 'Real-time horse racing analysis.',\\n};\\n\\nexport default function RootLayout({\\n  children,\\n}: {\\n  children: React.ReactNode;\\n}) {\\n  return (\\n    <html lang=\\\"en\\\">\\n      <body className={inter.className}>{children}</body>\\n    </html>\\n  );\\n}",
  "web_platform/frontend/app/page.tsx": "// web_platform/frontend/app/page.tsx\\nimport { LiveRaceDashboard } from '../src/components/LiveRaceDashboard';\\n\\nexport default function HomePage() {\\n  return <LiveRaceDashboard />;\\n}",
  "web_platform/frontend/src/hooks/useRealTimeRaces.ts": "// web_platform/frontend/src/hooks/useRealTimeRaces.ts\\n'use client';\\n\\nimport { useState, useEffect } from 'react';\\nimport { io, Socket } from 'socket.io-client';\\n\\n// NEW: Define the structure for a single analysis factor\\nexport interface FactorResult {\\n  points: number;\\n  ok: boolean;\\n  reason: string;\\n}\\n\\nexport interface QualifiedRace {\\n  race_id: string;\\n  track_name: string;\\n  race_number: number;\\n  post_time: string;\\n  checkmate_score: number;\\n  // NEW: Add the trifecta factors to the type definition\\n  trifecta_factors: Record<string, FactorResult>;\\n}\\n\\nexport const useRealTimeRaces = (apiUrl: string = 'ws://localhost:8080') => {\\n  const [races, setRaces] = useState<QualifiedRace[]>([]);\\n  const [isConnected, setIsConnected] = useState(false);\\n\\n  useEffect(() => {\\n    const socket: Socket = io(apiUrl);\\n\\n    socket.on('connect', () => setIsConnected(true));\\n    socket.on('disconnect', () => setIsConnected(false));\\n\\n    socket.on('race_update', (updatedRaces: QualifiedRace[]) => {\\n      setRaces(updatedRaces.sort((a, b) => b.checkmate_score - a.checkmate_score));\\n    });\\n\\n    return () => {\\n      socket.disconnect();\\n    };\\n  }, [apiUrl]);\\n\\n  return { races, isConnected };\\n};",
  "web_platform/frontend/src/components/ScoreBadge.tsx": "'use client';\\nimport React from 'react';\\n\\nconst getScoreStyling = (score: number) => {\\n  if (score >= 90) return { bg: 'bg-yellow-400/10', text: 'text-yellow-300', border: 'border-yellow-400' };\\n  if (score >= 80) return { bg: 'bg-orange-500/10', text: 'text-orange-400', border: 'border-orange-500' };\\n  return { bg: 'bg-sky-500/10', text: 'text-sky-400', border: 'border-sky-500' };\\n};\\n\\nexport const ScoreBadge: React.FC<{ score: number }> = ({ score }) => {\\n  const { bg, text } = getScoreStyling(score);\\n  return (\\n    <div className={`text-right ${text}`}>\\n      <p className=\\\"text-3xl font-bold\\\">{score.toFixed(1)}</p>\\n      <p className=\\\"text-xs font-medium tracking-wider uppercase\\\\\\\">Score</p>\\n    </div>\\n  );\\n};",
  "web_platform/frontend/src/components/RaceCard.tsx": "'use client';\\nimport React from 'react';\\nimport { motion } from 'framer-motion';\\nimport { QualifiedRace } from '../hooks/useRealTimeRaces';\\nimport { ScoreBadge } from './ScoreBadge';\\nimport { TrifectaFactors } from './TrifectaFactors'; // Import the new component\\n\\nconst getScoreColorClass = (score: number) => {\\n  if (score >= 90) return 'border-l-yellow-400';\\n  if (score >= 80) return 'border-l-orange-500';\\n  return 'border-l-sky-500';\\n};\\n\\nexport const RaceCard: React.FC<{ race: QualifiedRace }> = ({ race }) => {\\n  return (\\n    <motion.div\\n      layout\\n      initial={{ opacity: 0, scale: 0.9 }}\\n      animate={{ opacity: 1, scale: 1 }}\\n      exit={{ opacity: 0, scale: 0.9 }}\\n      transition={{ duration: 0.35, type: 'spring' }}\\n      className={`rounded-lg border-l-4 ${getScoreColorClass(race.checkmate_score)} bg-slate-800/50 p-5 shadow-xl backdrop-blur-sm`}\\n    >\\n      <div className=\\\"flex justify-between items-start\\\">\\n        <div>\\n          <h3 className=\\\"text-xl font-semibold text-white\\\">{race.track_name} R{race.race_number}</h3>\\n          <p className=\\\"text-slate-400\\\">Post: {new Date(race.post_time).toLocaleTimeString([], {hour: '2-digit', minute:'2-digit'})}</p>\\n        </div>\\n        <ScoreBadge score={race.checkmate_score} />\\n      </div>\\n      {/* Replace the placeholder comment with our new component */}\\n      <TrifectaFactors factors={race.trifecta_factors} />\\n    </motion.div>\\n  );\\n};",
  "web_platform/frontend/src/components/LiveRaceDashboard.tsx": "'use client';\\nimport React from 'react';\\nimport { AnimatePresence } from 'framer-motion';\\nimport { useRealTimeRaces } from '../hooks/useRealTimeRaces';\\nimport { RaceCard } from './RaceCard';\\n\\nexport const LiveRaceDashboard: React.FC = () => {\\n  const { races, isConnected } = useRealTimeRaces();\\n\\n  return (\\n    <div className=\\\"min-h-screen bg-slate-900 text-white font-sans\\\">\\n      <header className=\\\"bg-slate-800/70 backdrop-blur-md sticky top-0 z-50 border-b border-slate-700 p-4 shadow-lg\\\">\\n        <div className=\\\"flex justify-between items-center max-w-7xl mx-auto\\\">\\n          <h1 className=\\\"text-2xl font-bold tracking-tighter\\\">Checkmate Live</h1>\\n          <div className=\\\"flex items-center space-x-3\\\">\\n            <div className={`w-3 h-3 rounded-full transition-colors ${isConnected ? 'bg-green-400 animate-pulse' : 'bg-red-500'}`}></div>\\n            <span className=\\\"text-sm font-medium text-slate-300\\\">{isConnected ? 'LIVE' : 'CONNECTING...'}</span>\\n          </div>\\n        </div>\\n      </header>\\n\\n      <main className=\\\"p-4 md:p-6 max-w-7xl mx-auto\\\">\\n        <AnimatePresence>\\n          {races.length > 0 ? (\\n            <div className=\\\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-6\\\">\\n              {races.map((race) => (\\n                <RaceCard key={race.race_id} race={race} />\\n              ))}\\n            </div>\\n          ) : (\\n            <div className=\\\"text-center mt-24 text-slate-500\\\">\\n              <p className=\\\"text-3xl font-semibold\\\">Awaiting Data...</p>\\n              <p className=\\\"mt-2\\\\\\\">No qualified races found. The system is actively monitoring all sources.</p>\\n            </div>\\n          )}\\n        </AnimatePresence>\\n      </main>\\n    </div>\\n  );\\n};"
}