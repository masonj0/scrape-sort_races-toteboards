{
    "filepath": "src/paddock_parser/adapters/skysports_adapter.py",
    "content": "import asyncio\nimport logging\nimport re\nfrom datetime import date, datetime\nfrom typing import List, Optional\n\nfrom bs4 import BeautifulSoup\n\nfrom ..base import BaseAdapterV3, NormalizedRace, NormalizedRunner\nfrom ..fetcher import get_page_content\n\n\ndef _convert_odds_to_float(odds_str: Optional[str]) -> Optional[float]:\n    \"\"\"Converts fractional odds string to a float.\"\"\"\n    if not odds_str or not isinstance(odds_str, str):\n        return None\n\n    odds_str = odds_str.strip().upper()\n    if odds_str == 'SP':\n        return None\n    if odds_str == 'EVENS':\n        return 2.0\n\n    if '/' in odds_str:\n        try:\n            numerator, denominator = map(int, odds_str.split('/'))\n            if denominator == 0:\n                return None\n            return (numerator / denominator) + 1.0\n        except (ValueError, ZeroDivisionError):\n            return None\n    try:\n        return float(odds_str) + 1.0\n    except (ValueError, TypeError):\n        return None\n\n\nclass SkySportsAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for skysports.com.\n    Fetches the main racecards page to find individual race URLs,\n    then fetches each race detail page to extract full runner information.\n    \"\"\"\n\n    SOURCE_ID = \"skysports\"\n\n    def __init__(self, config=None):\n        super().__init__(config)\n        self.base_url = \"https://www.skysports.com\"\n\n    async def fetch(self) -> List[NormalizedRace]:\n        \"\"\"\n        Fetches all race data by first getting the summary page to find\n        race links, then fetching each of those pages concurrently.\n        \"\"\"\n        index_page_url = f\"{self.base_url}/racing/racecards\"\n        try:\n            index_html = await get_page_content(index_page_url)\n        except Exception as e:\n            logging.error(f\"Failed to fetch the skysports racecards index page: {e}\")\n            return []\n\n        soup = BeautifulSoup(index_html, \"lxml\")\n        meeting_blocks = soup.select(\"div.sdc-site-concertina-block\")\n\n        all_races = []\n        for meeting_block in meeting_blocks:\n            track_name_tag = meeting_block.select_one(\"h3.sdc-site-concertina-block__title > span.sdc-site-concertina-block__title\")\n            track_name = track_name_tag.text.strip() if track_name_tag else \"Unknown Track\"\n\n            race_events = meeting_block.select(\"div.sdc-site-racing-meetings__event\")\n\n            race_urls = []\n            for event in race_events:\n                link_tag = event.select_one(\"a.sdc-site-racing-meetings__event-link\")\n                if link_tag and link_tag.get(\"href\"):\n                    race_urls.append(f\"{self.base_url}{link_tag['href']}\")\n\n            if not race_urls:\n                continue\n\n            tasks = [get_page_content(url) for url in race_urls]\n            race_html_pages = await asyncio.gather(*tasks, return_exceptions=True)\n\n            for i, (html_or_exc, url) in enumerate(zip(race_html_pages, race_urls)):\n                if isinstance(html_or_exc, Exception):\n                    logging.warning(f\"Failed to fetch race page {url}: {html_or_exc}\")\n                    continue\n\n                if html_or_exc:\n                    race = self._parse_race_details(html_or_exc, url, track_name, i + 1)\n                    if race:\n                        all_races.append(race)\n\n        return all_races\n\n    def _parse_race_details(\n        self, html_content: str, url: str, track_name: str, race_number: int\n    ) -> Optional[NormalizedRace]:\n        \"\"\"Parses the race detail page to extract all available data.\"\"\"\n        logging.info(f\"Parsing race details for track: {track_name}, race number: {race_number}\")\n        soup = BeautifulSoup(html_content, \"lxml\")\n\n        try:\n            header_tag = soup.select_one(\"h2.sdc-site-racing-header__name\")\n            header_text = header_tag.text.strip() if header_tag else \"\"\n\n            race_name_tag = soup.select_one(\"h1.sdc-site-racing-header__title\")\n            race_name = race_name_tag.text.strip() if race_name_tag else \"Unknown Race\"\n\n            race_time_match = re.search(r\"(\\d{2}:\\d{2})\", header_text)\n            race_time_str = race_time_match.group(1) if race_time_match else \"00:00\"\n\n            post_time_dt = datetime.combine(\n                date.today(), datetime.strptime(race_time_str, \"%H:%M\").time()\n            )\n\n            race_type = \"Handicap\" if \"handicap\" in header_text.lower() else \"Unknown\"\n\n            runners_list = []\n            runner_items = soup.select(\"div.sdc-site-racing-card__item\")\n            for i, item in enumerate(runner_items):\n                name_tag = item.select_one(\"h4.sdc-site-racing-card__name a\")\n                program_number_tag = item.select_one(\"div.sdc-site-racing-card__number strong\")\n                odds_tag = item.select_one(\".sdc-site-racing-card__odds-sp\")\n\n                name = name_tag.text.strip() if name_tag else None\n                program_number_str = (\n                    program_number_tag.text.strip() if program_number_tag else str(i + 1)\n                )\n                odds_str = odds_tag.text.strip() if odds_tag else None\n                odds_float = _convert_odds_to_float(odds_str)\n\n                if name:\n                    try:\n                        program_number = int(re.search(r\"\\d+\", program_number_str).group())\n                    except (ValueError, AttributeError):\n                        program_number = i + 1\n\n                    runners_list.append(\n                        NormalizedRunner(\n                            name=name, program_number=program_number, odds=odds_float\n                        )\n                    )\n\n            path_parts = [part for part in url.split(\"/\") if part]\n            race_id = path_parts[-1] if path_parts else url\n\n            return NormalizedRace(\n                race_id=race_id,\n                track_name=track_name,\n                post_time=post_time_dt,\n                race_number=race_number,\n                race_type=race_type,\n                runners=runners_list,\n                number_of_runners=len(runners_list),\n            )\n\n        except Exception as e:\n            logging.error(f\"Error parsing race details from {url}: {e}\", exc_info=True)\n            return None\n\n    def parse_races(self, html_content: str) -> List[NormalizedRace]:\n        \"\"\"This method is not used in the new V3 flow but is required by the base class.\"\"\"\n        return []\n"
}