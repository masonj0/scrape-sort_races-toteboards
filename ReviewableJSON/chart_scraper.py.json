{
    "file_path": "chart_scraper.py",
    "content": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Chart Scraper (v3 - Perfected)\n# ==============================================================================\n# This script downloads and parses historical race result charts from Equibase PDF files\n# using a direct-download URL for combined daily charts.\n# ==============================================================================\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom bs4 import BeautifulSoup\nfrom tabula import read_pdf\nimport os\nimport time\nimport pikepdf\n\nclass ChartScraper:\n    \"\"\"Orchestrates the downloading, decrypting, and parsing of combined Equibase PDF charts.\"\"\"\n\n    def __init__(self):\n        self.download_dir = \"results_archive\"\n        self.pdf_dir = os.path.join(self.download_dir, 'pdf')\n        self.unlocked_pdf_dir = os.path.join(self.download_dir, 'pdf_unlocked')\n        self.csv_dir = os.path.join(self.download_dir, 'csv')\n        self.track_summary_url = \"https://www.equibase.com/static/chart/summary/\"\n        self.pdf_url_pattern = \"https://www.equibase.com/static/chart/pdf/{TID}{MMDDYY}{CTRY}.pdf\"\n\n    def _get_yesterday_date(self) -> tuple[str, str, str]:\n        yesterday = datetime.now() - timedelta(days=1)\n        summary_date = yesterday.strftime(\"%Y%m%d\")\n        pdf_chart_date = yesterday.strftime(\"%m%d%y\") # New format for combined chart URL\n        display_date = yesterday.strftime(\"%m/%d/%Y\")\n        return summary_date, pdf_chart_date, display_date\n\n    def _get_yesterday_tracks(self, url_date_format: str) -> list[str]:\n        full_url = f\"{self.track_summary_url}{url_date_format}.html\"\n        print(f\"-> Searching for tracks at: {full_url}\")\n        try:\n            response = requests.get(full_url, timeout=10)\n            response.raise_for_status()\n        except requests.exceptions.RequestException as e:\n            print(f\"Error fetching track summary page: {e}\")\n            return []\n\n        soup = BeautifulSoup(response.content, 'lxml')\n        track_codes = set()\n        for a_tag in soup.find_all('a', href=True):\n            if 'TID=' in a_tag['href']:\n                try:\n                    track_code = a_tag['href'].split('TID=')[1].split('&')[0]\n                    track_codes.add(track_code)\n                except IndexError:\n                    continue\n\n        unique_tracks = sorted(list(track_codes))\n        print(f\"-> Found {len(unique_tracks)} unique tracks: {unique_tracks}\")\n        return unique_tracks\n\n    def _download_and_parse_chart(self, track_code: str, chart_date: str):\n        pdf_url = self.pdf_url_pattern.format(TID=track_code, MMDDYY=chart_date, CTRY='USA')\n        filename_base = f\"{track_code}_{chart_date}_FULL\"\n        pdf_path = os.path.join(self.pdf_dir, f\"{filename_base}.pdf\")\n        unlocked_pdf_path = os.path.join(self.unlocked_pdf_dir, f\"{filename_base}_unlocked.pdf\")\n        csv_path = os.path.join(self.csv_dir, f\"{filename_base}_scraped.csv\")\n\n        print(f\"   - Attempting full chart for {track_code}...\")\n        try:\n            pdf_response = requests.get(pdf_url, stream=True, timeout=20)\n            content_type = pdf_response.headers.get('Content-Type', '')\n            content_length = int(pdf_response.headers.get('Content-Length', 0))\n\n            if 'application/pdf' not in content_type or content_length < 20000: # Increase size threshold for full charts\n                print(\"     -> Not a valid combined PDF (chart may not exist).\")\n                return\n\n            with open(pdf_path, 'wb') as f:\n                f.write(pdf_response.content)\n            print(f\"     -> Downloaded locked PDF to {pdf_path}\")\n\n        except requests.exceptions.RequestException as e:\n            print(f\"     -> Error downloading PDF: {e}\")\n            return\n\n        try:\n            with pikepdf.open(pdf_path, allow_overwriting_input=True) as pdf:\n                pdf.save(unlocked_pdf_path)\n            print(f\"     -> Saved unlocked PDF to {unlocked_pdf_path}\")\n        except Exception as e:\n            print(f\"     -> Failed to unlock PDF with pikepdf: {e}\")\n            return\n\n        try:\n            tables = read_pdf(unlocked_pdf_path, pages='all', multiple_tables=True, lattice=True, silent=True)\n            if not tables:\n                print(\"     -> Tabula found no tables to extract from unlocked PDF.\")\n                return\n\n            combined_df = pd.concat(tables, ignore_index=True)\n            combined_df.to_csv(csv_path, index=False)\n            print(f\"     -> SUCCESSFULLY extracted {len(tables)} tables to {csv_path}\")\n        except Exception as e:\n            print(f\"     -> Error during Tabula PDF scraping: {e}\")\n\n    def run(self):\n        os.makedirs(self.pdf_dir, exist_ok=True)\n        os.makedirs(self.unlocked_pdf_dir, exist_ok=True)\n        os.makedirs(self.csv_dir, exist_ok=True)\n\n        summary_date, chart_date, display_date = self._get_yesterday_date()\n        print(f\"\\\\n--- Starting Perfected Equibase Chart Scraper for: {display_date} ---\")\n\n        tracks = self._get_yesterday_tracks(summary_date)\n        if not tracks:\n            print(\"\\\\n*** No tracks found for yesterday. Halting. ***\")\n            return\n\n        print(\"\\\\n--- Downloading, Unlocking, and Parsing Full Daily Charts ---\")\n        for track in tracks:\n            print(f\"\\\\n[TRACK: {track}]\")\n            self._download_and_parse_chart(track, chart_date)\n            time.sleep(1)\n\n        print(f\"\\\\n--- Scraper Finished! Check the '{self.csv_dir}' folder. ---\")\n\nif __name__ == \"__main__\":\n    scraper = ChartScraper()\n    scraper.run()"
}