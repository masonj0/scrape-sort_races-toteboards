{
    "file_path": "python_service/adapters/at_the_races_adapter.py",
    "content": "# python_service/adapters/at_the_races_adapter.py\n\nimport asyncio\nimport structlog\nimport httpx\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom bs4 import BeautifulSoup, Tag\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\nfrom .utils import parse_odds\n\nlog = structlog.get_logger(__name__)\n\ndef _clean_text(text: Optional[str]) -> Optional[str]:\n    return ' '.join(text.strip().split()) if text else None\n\nclass AtTheRacesAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(source_name=\"AtTheRaces\", base_url=\"https://www.attheraces.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        try:\n            race_links = await self._get_race_links(http_client)\n            tasks = [self._fetch_and_parse_race(link, http_client) for link in race_links]\n            races = [race for race in await asyncio.gather(*tasks) if race]\n            return self._format_response(races, start_time, is_success=True)\n        except Exception as e:\n            return self._format_response([], start_time, is_success=False, error_message=str(e))\n\n    async def _get_race_links(self, http_client: httpx.AsyncClient) -> List[str]:\n        response_html = await self.make_request(http_client, 'GET', '/racecards')\n        if not response_html:\n            return []\n        soup = BeautifulSoup(response_html, \"html.parser\")\n        links = {a['href'] for a in soup.select(\"a.race-time-link[href]\")}\n        return [f\"{self.base_url}{link}\" for link in links]\n\n    async def _fetch_and_parse_race(self, url: str, http_client: httpx.AsyncClient) -> Optional[Race]:\n        try:\n            response_html = await self.make_request(http_client, 'GET', url)\n            if not response_html:\n                return None\n            soup = BeautifulSoup(response_html, \"html.parser\")\n            header = soup.select_one(\"h1.heading-racecard-title\").get_text()\n            track_name, race_time = [p.strip() for p in header.split(\"|\")[:2]]\n            active_link = soup.select_one(\"a.race-time-link.active\")\n            race_number = active_link.find_parent(\"div\", \"races\").select(\"a.race-time-link\").index(active_link) + 1\n            start_time = datetime.strptime(f\"{datetime.now().date()} {race_time}\", \"%Y-%m-%d %H:%M\")\n            runners = [self._parse_runner(row) for row in soup.select(\"div.card-horse\")]\n            return Race(id=f\"atr_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\", venue=track_name, race_number=race_number, start_time=start_time, runners=[r for r in runners if r], source=self.source_name)\n        except Exception as e:\n            log.error(\"Error parsing race from AtTheRaces\", url=url, exc_info=e)\n            return None\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = _clean_text(row.select_one(\"h3.horse-name a\").get_text())\n            num_str = _clean_text(row.select_one(\"span.horse-number\").get_text())\n            number = int(''.join(filter(str.isdigit, num_str)))\n            odds_str = _clean_text(row.select_one(\"button.best-odds\").get_text())\n            win_odds = Decimal(str(parse_odds(odds_str))) if odds_str else None\n            odds_data = {self.source_name: OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())} if win_odds and win_odds < 999 else {}\n            return Runner(number=number, name=name, odds=odds_data)\n        except Exception as e:\n            log.warning(\"Failed to parse runner\", exc_info=e)\n            return None\n"
}