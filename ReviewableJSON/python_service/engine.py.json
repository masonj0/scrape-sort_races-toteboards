{
    "file_path": "python_service/engine.py",
    "content": "# python_service/engine.py\n\nimport asyncio\nimport structlog\nimport httpx\nimport redis.asyncio as redis\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Tuple\n\nfrom .models import Race, AggregatedResponse\nfrom .adapters.base import BaseAdapter\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\n\nlog = structlog.get_logger(__name__)\n\nclass FortunaEngine:\n    def __init__(self, config=None):\n        from .config import get_settings\n        self.config = config or get_settings()\n        self.log = structlog.get_logger(self.__class__.__name__)\n        self.adapters: List[BaseAdapter] = [\n            BetfairAdapter(config=self.config),\n            BetfairGreyhoundAdapter(config=self.config),\n            TVGAdapter(config=self.config),\n            RacingAndSportsAdapter(config=self.config),\n            RacingAndSportsGreyhoundAdapter(config=self.config),\n            AtTheRacesAdapter(config=self.config),\n            SportingLifeAdapter(config=self.config),\n            TimeformAdapter(config=self.config),\n            TheRacingApiAdapter(config=self.config),\n            GbgbApiAdapter(config=self.config),\n            HarnessAdapter(config=self.config)\n        ]\n        self.http_client = httpx.AsyncClient()\n        self.redis_client = redis.from_url(self.config.REDIS_URL, decode_responses=True)\n        self.log.info(\"Redis client initialized\", redis_url=self.config.REDIS_URL)\n\n    async def close(self):\n        await self.http_client.aclose()\n        await self.redis_client.aclose() # Use aclose() for async client\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapter, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"Wraps an adapter's fetch call, catches all exceptions, and returns a consistent payload.\"\"\"\n        start_time = datetime.now()\n        try:\n            result = await adapter.fetch_races(date, self.http_client)\n            duration = (datetime.now() - start_time).total_seconds()\n            return (adapter.source_name, result, duration)\n        except Exception as e:\n            duration = (datetime.now() - start_time).total_seconds()\n            log.error(\"Adapter raised an unhandled exception\", adapter=adapter.source_name, error=str(e), exc_info=True)\n            failed_result = {\n                'races': [],\n                'source_info': {\n                    'name': adapter.source_name,\n                    'status': 'FAILED',\n                    'races_fetched': 0,\n                    'error_message': str(e),\n                    'fetch_duration': duration\n                }\n            }\n            return (adapter.source_name, failed_result, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        runner_map[new_runner.number].odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n        return list(race_map.values())\n\n    async def get_races(self, date: str, background_tasks: set, source_filter: str = None) -> Dict[str, Any]:\n        cache_key = f\"fortuna:races:{date}\"\n        if not source_filter: # Only use cache for 'all sources' requests\n            try:\n                cached_data = await self.redis_client.get(cache_key)\n                if cached_data:\n                    self.log.info(\"CACHE HIT\", key=cache_key)\n                    # Pydantic can validate directly from the JSON string\n                    return AggregatedResponse.model_validate_json(cached_data).model_dump()\n            except Exception as e:\n                self.log.error(\"Redis GET failed\", error=str(e))\n\n        self.log.info(\"CACHE MISS\", key=cache_key)\n        target_adapters = self.adapters\n        if source_filter:\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n        for result in results:\n            if isinstance(result, Exception):\n                self.log.error(\"Adapter fetch failed\", error=result, exc_info=False)\n                continue\n            adapter_name, adapter_result, duration = result\n            source_info = adapter_result.get('source_info', {})\n            source_info['fetch_duration'] = round(duration, 2)\n            source_infos.append(source_info)\n            if source_info.get('status') == 'SUCCESS':\n                all_races.extend(adapter_result.get('races', []))\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, '%Y-%m-%d').date(),\n            races=deduped_races,\n            sources=source_infos,\n            metadata={\n                'fetch_time': datetime.now(),\n                'sources_queried': [a.source_name for a in target_adapters],\n                'sources_successful': len([s for s in source_infos if s['status'] == 'SUCCESS']),\n                'total_races': len(deduped_races)\n            }\n        )\n\n        if not source_filter: # Only use cache for 'all sources' requests\n            try:\n                # Cache the result for 5 minutes (300 seconds)\n                await self.redis_client.set(cache_key, response_obj.model_dump_json(), ex=300)\n                self.log.info(\"CACHE SET\", key=cache_key, expiry=300)\n            except Exception as e:\n                self.log.error(\"Redis SET failed\", error=str(e))\n\n        return response_obj.model_dump()"
}