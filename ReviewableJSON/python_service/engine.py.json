{
    "file_path": "python_service/engine.py",
    "content": "# python_service/engine.py\n\nimport asyncio\nimport inspect\nfrom datetime import datetime\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport asyncio\nimport httpx\nimport structlog\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base import BaseAdapter\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .cache_manager import cache_async_result\nfrom .health import health_monitor\nfrom .models import AggregatedResponse\nfrom .models import OddsData\nfrom .models import Race\nfrom .models import Runner\nfrom .models_v3 import NormalizedRace\n\nlog = structlog.get_logger(__name__)\n\n\nclass FortunaEngine:\n    def __init__(self, config=None):\n        from .config import get_settings\n\n        self.config = config or get_settings()\n        self.logger = structlog.get_logger(__name__)\n        self.adapters: List[BaseAdapter] = [\n            BetfairAdapter(source_name=BetfairAdapter.SOURCE_NAME, base_url=BetfairAdapter.BASE_URL),\n            BetfairGreyhoundAdapter(source_name=BetfairGreyhoundAdapter.SOURCE_NAME, base_url=BetfairGreyhoundAdapter.BASE_URL),\n            RacingAndSportsAdapter(config=self.config),\n            RacingAndSportsGreyhoundAdapter(config=self.config),\n            AtTheRacesAdapter(config=self.config),\n            RacingPostAdapter(config=self.config),\n            HarnessAdapter(config=self.config),\n            EquibaseAdapter(config=self.config),\n            SportingLifeAdapter(config=self.config),\n            TimeformAdapter(config=self.config),\n            TheRacingApiAdapter(config=self.config),\n            GbgbApiAdapter(config=self.config),\n        ]\n        # V3 ADAPTERS\n        self.v3_adapters = [\n            BetfairDataScientistAdapter(\n                model_name=\"ThoroughbredModel\",\n                url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets?id=thoroughbred-model&date=\",\n            ),\n            TVGAdapter(config=self.config),\n        ]\n        self.http_limits = httpx.Limits(\n            max_connections=config.HTTP_POOL_CONNECTIONS, max_keepalive_connections=config.HTTP_MAX_KEEPALIVE\n        )\n        self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapter, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"\n        Wraps an adapter's fetch call for safe, non-blocking execution,\n        and returns a consistent payload with timing information.\n        Handles both modern async adapters and legacy sync adapters.\n        \"\"\"\n        start_time = datetime.now()\n        races = []\n        error_message = None\n        is_success = False\n\n        try:\n            # Check if the adapter's fetch_races method is a modern async function\n            if inspect.iscoroutinefunction(adapter.fetch_races):\n                result = await adapter.fetch_races(date, self.http_client)\n            else:\n                # This is a legacy, synchronous adapter. Run it in a separate thread.\n                self.logger.warning(\n                    \"legacy_sync_adapter_detected\",\n                    adapter=adapter.source_name,\n                    recommendation=\"This adapter should be refactored to be fully asynchronous.\"\n                )\n                result = await asyncio.to_thread(adapter.fetch_races, date, self.http_client)\n\n            # Assuming the result is a dictionary with a 'races' key\n            if result and 'races' in result:\n                races = result.get('races', [])\n                is_success = True\n            else:\n                error_message = \"Adapter returned no data or malformed response\"\n\n        except Exception as e:\n            self.logger.error(\n                \"Critical failure during fetch from adapter.\",\n                adapter=adapter.source_name,\n                error=str(e),\n                exc_info=True\n            )\n            error_message = str(e)\n\n        duration = (datetime.now() - start_time).total_seconds()\n        health_monitor.record_adapter_response(adapter.source_name, success=is_success, duration=duration)\n\n        # Construct a consistent source_info payload regardless of success or failure\n        payload = {\n            \"races\": races,\n            \"source_info\": {\n                \"name\": adapter.source_name,\n                \"status\": \"SUCCESS\" if is_success else \"FAILED\",\n                \"races_fetched\": len(races),\n                \"error_message\": error_message,\n                \"fetch_duration\": duration,\n            },\n        }\n        return (adapter.source_name, payload, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races from multiple sources and reconciles odds.\"\"\"\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            # Use a robust key: venue, date, and race number\n            key = f\"{race.venue.upper()}-{race.start_time.strftime('%Y-%m-%d')}-{race.race_number}\"\n\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                # Merge runners and odds into the existing race object\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        # Runner exists, reconcile odds\n                        existing_runner = runner_map[new_runner.number]\n                        updated_odds = existing_runner.odds.copy()\n                        updated_odds.update(new_runner.odds)\n                        existing_runner.odds = updated_odds\n                    else:\n                        # New runner, add to the existing race\n                        existing_race.runners.append(new_runner)\n\n                # Update source count\n                existing_race.source += f\", {race.source}\"\n\n        return list(race_map.values())\n\n    async def get_races(self, date: str, background_tasks: set, source_filter: str = None) -> Dict[str, Any]:\n        if source_filter:\n            self.log.info(\"Bypassing cache for source-specific request\", source=source_filter)\n            return await self._fetch_races_from_sources(date, source_filter=source_filter)\n\n        return await self._get_all_races_cached(date, background_tasks=background_tasks)\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"fortuna_engine_races\")\n    async def _get_all_races_cached(self, date: str, background_tasks: set) -> Dict[str, Any]:\n        \"\"\"This method fetches races for all sources and its result is cached.\"\"\"\n        self.log.info(\"CACHE MISS: Fetching all races from sources.\", date=date)\n        return await self._fetch_races_from_sources(date)\n\n    def _convert_v3_race_to_v2(self, v3_race: NormalizedRace) -> Race:\n        \"\"\"Converts a V3 NormalizedRace object to a V2 Race object.\"\"\"\n        import re\n\n        race_number = 0\n        match = re.search(r\"\\d+\", v3_race.race_name)\n        if match:\n            race_number = int(match.group())\n\n        runners = []\n        for v3_runner in v3_race.runners:\n            odds_data = OddsData(\n                win=Decimal(str(v3_runner.odds_decimal)),\n                source=v3_race.source_ids[0],\n                last_updated=datetime.now(timezone.utc),\n            )\n            runner = Runner(\n                id=v3_runner.runner_id,\n                name=v3_runner.name,\n                number=int(v3_runner.saddle_cloth)\n                if v3_runner.saddle_cloth and v3_runner.saddle_cloth.isdigit()\n                else 99,\n                odds={v3_race.source_ids[0]: odds_data},\n            )\n            runners.append(runner)\n\n        return Race(\n            id=v3_race.race_key,\n            venue=v3_race.track_key,\n            race_number=race_number,\n            start_time=datetime.fromisoformat(v3_race.start_time_iso),\n            runners=runners,\n            source=v3_race.source_ids[0],\n            race_name=v3_race.race_name,\n        )\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"odds_engine_fetch\")\n    async def _fetch_races_from_sources(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"Helper method to contain the logic for fetching and aggregating races.\"\"\"\n        target_adapters = self.adapters\n        if source_filter:\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n\n        # Run V3 adapters\n        for adapter in self.v3_adapters:\n            if hasattr(adapter, 'fetch_and_normalize'):\n                # Handle synchronous V3 adapters\n                v3_task = asyncio.to_thread(adapter.fetch_and_normalize)\n                tasks.append(v3_task)\n            elif hasattr(adapter, 'get_races'):\n                # Handle asynchronous V3 adapters\n                v3_task = adapter.get_races(date)\n                tasks.append(v3_task)\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n\n        v3_start_time = datetime.now()  # Approximate start time for all V3 adapters\n\n        for result in results:\n            try:\n                if isinstance(result, Exception):\n                    self.log.error(\"Adapter fetch failed\", error=result, exc_info=False)\n                    continue\n\n                # Correctly differentiate between V2 and V3 results\n                if isinstance(result, tuple) and len(result) == 3:  # V2 Adapter Result\n                    adapter_name, adapter_result, duration = result\n                    source_info = adapter_result.get(\"source_info\", {})\n                    source_info[\"fetch_duration\"] = round(duration, 2)\n                    source_infos.append(source_info)\n                    if source_info.get(\"status\") == \"SUCCESS\":\n                        all_races.extend(adapter_result.get(\"races\", []))\n                elif isinstance(result, list) and all(\n                    isinstance(r, NormalizedRace) for r in result\n                ):  # V3 Adapter Result\n                    if result:\n                        v3_races = result\n                        adapter_name = v3_races[0].source_ids[0]\n                        translated_races = [self._translate_v3_race_to_v2(nr) for nr in result]\n                        all_races.extend(translated_races)\n\n                        v3_duration = (datetime.now() - v3_start_time).total_seconds()\n                        source_infos.append(\n                            {\n                                \"name\": adapter_name,\n                                \"status\": \"SUCCESS\",\n                                \"races_fetched\": len(translated_races),\n                                \"error_message\": None,\n                                \"fetch_duration\": round(v3_duration, 2),\n                            }\n                        )\n            except Exception:\n                self.log.error(\"Failed to process result from an adapter.\", exc_info=True)\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            sources=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n            },\n        )\n\n        # --- Add Success Notification ---\n        try:\n            from windows_toasts import Toast, WindowsToaster\n            toaster = WindowsToaster(\"Fortuna Faucet Data Refresh\")\n            new_toast = Toast()\n            new_toast.text_fields = [f\"Successfully fetched {len(deduped_races)} races from {len(source_infos)} sources.\"]\n            toaster.show_toast(new_toast)\n        except (ImportError, RuntimeError):\n            pass\n\n        return response_obj.model_dump()\n\n    def _translate_v3_race_to_v2(self, norm_race: NormalizedRace) -> Race:\n        \"\"\"Translates a V3 NormalizedRace into a V2 Race object.\"\"\"\n        import re\n\n        race_number = 0\n        match = re.search(r\"R(\\d+)\", norm_race.race_name)\n        if match:\n            race_number = int(match.group(1))\n\n        runners = []\n        for norm_runner in norm_race.runners:\n            adapter_name = norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\"\n            odds_data = OddsData(\n                win=Decimal(str(norm_runner.odds_decimal)), source=adapter_name, last_updated=datetime.now(timezone.utc)\n            )\n\n            try:\n                runner_number = int(norm_runner.saddle_cloth)\n            except (ValueError, TypeError):\n                runner_number = None\n\n            runner = Runner(\n                id=norm_runner.runner_id, name=norm_runner.name, number=runner_number, odds={adapter_name: odds_data}\n            )\n            runners.append(runner)\n\n        return Race(\n            id=norm_race.race_key,\n            venue=norm_race.track_key,\n            start_time=datetime.fromisoformat(norm_race.start_time_iso),\n            race_number=race_number,\n            runners=runners,\n            source=norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\",\n            # Store extra V3 data in metadata for future use\n            metadata={\"v3_race_name\": norm_race.race_name},\n        )\n"
}