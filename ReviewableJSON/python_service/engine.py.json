{
    "filepath": "./python_service/engine.py",
    "content": "# engine.py\n# The final, supercharged version of the Python Collection Service engine.\n\n\nimport logging\nimport json\nimport subprocess\nimport concurrent.futures\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional, Union, Dict\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom pydantic_settings import BaseSettings\nfrom cachetools import TTLCache\n\n# --- Finalized Settings Model ---\nclass Settings(BaseSettings):\n    QUALIFICATION_SCORE: float = 75.0\n    FIELD_SIZE_OPTIMAL_MIN: int = 4\n    FIELD_SIZE_OPTIMAL_MAX: int = 6\n    FIELD_SIZE_ACCEPTABLE_MIN: int = 7\n    FIELD_SIZE_ACCEPTABLE_MAX: int = 8\n    FIELD_SIZE_OPTIMAL_POINTS: int = 30\n    FIELD_SIZE_ACCEPTABLE_POINTS: int = 10\n    FIELD_SIZE_PENALTY_POINTS: int = -20\n    FAV_ODDS_POINTS: int = 30\n    MAX_FAV_ODDS: float = 3.5\n    SECOND_FAV_ODDS_POINTS: int = 40\n    MIN_2ND_FAV_ODDS: float = 4.0\n    DATABASE_BATCH_SIZE: int = 100\n    RUST_ENGINE_TIMEOUT: int = 10\n    ODDS_API_KEY: Optional[str] = None\n\n# --- Finalized Data Models ---\n\nclass Runner(BaseModel):\n    name: str\n    odds: Optional[float] = None\n\nclass Race(BaseModel):\n    race_id: str\n    track_name: str\n    race_number: Optional[int] = None\n    post_time: Optional[datetime] = None\n    runners: List[Runner]\n    source: Optional[str] = None\n    checkmate_score: Optional[float] = None\n    is_qualified: Optional[bool] = None\n    trifecta_factors_json: Optional[str] = None\n    analysis_details: Optional[str] = None # For advanced analysis\n\n# --- Resilient Fetcher Stub ---\nclass DefensiveFetcher:\n    def get(self, url: str, headers: Optional[Dict[str, str]] = None) -> Union[dict, str, None]:\n\n        try:\n            command = [\"curl\", \"-s\", \"-L\", \"--tlsv1.2\", \"--http1.1\"]\n            if headers:\n                for key, value in headers.items():\n                    command.extend([\"-H\", f\"{key}: {value}\"])\n            command.append(url)\n\n            result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=15)\n            response_text = result.stdout\n\n            try:\n                return json.loads(response_text)\n            except json.JSONDecodeError:\n                logging.warning(f\"Failed to decode JSON from {url}, returning raw text.\")\n                return response_text # Return text as fallback\n        except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:\n            logging.error(f\"CRITICAL: curl GET failed for {url}. Details: {e}\")\n            return None\n\n\n# --- Enhanced Adapter Stubs ---\nclass BaseAdapterV7(ABC):\n    def __init__(self, fetcher: DefensiveFetcher, settings: Settings):\\\n        self.fetcher = fetcher\n        self.settings = settings\n    @abstractmethod\n    def fetch_races(self) -> List[Race]: raise NotImplementedError\n\nclass EnhancedTVGAdapter(BaseAdapterV7):\n    def fetch_races(self) -> List[Race]:\n        # TODO: Implement full logic with caching as per blueprint\n        return []\n\nclass TheOddsApiAdapter(BaseAdapterV7):\n    def fetch_races(self) -> List[Race]:\n        # TODO: Implement full logic with multi-bookmaker parsing\n        return []\n\nPRODUCTION_ADAPTERS = [EnhancedTVGAdapter, TheOddsApiAdapter] # Add others as they are built\n\n# --- Supercharged Orchestrator ---\nclass SuperchargedOrchestrator:\n    def __init__(self, settings: Settings):\n        self.fetcher = DefensiveFetcher()\n        self.settings = settings\n        self.adapters = [Adapter(self.fetcher, self.settings) for Adapter in PRODUCTION_ADAPTERS]\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    def get_races_parallel(self) -> tuple[list[Race], list[dict]]:\n        # This method will be enhanced with performance monitoring and data validation\n        # For now, it implements the core concurrent fetching logic\n        all_races, statuses = [], []\n        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.adapters)) as executor:\n            future_to_adapter = {executor.submit(adapter.fetch_races): adapter for adapter in self.adapters}\n            for future in concurrent.futures.as_completed(future_to_adapter):\n                adapter = future_to_adapter[future]\n                try:\n                    races = future.result()\n                    all_races.extend(races)\n                    # Create a basic status receipt for now\n                    statuses.append({'adapter_id': adapter.__class__.__name__, 'status': 'OK', 'races_found': len(races)})\n                except Exception as e:\n                    self.logger.error(f\"Adapter {adapter.__class__.__name__} failed: {e}\", exc_info=True)\n                    statuses.append({'adapter_id': adapter.__class__.__name__, 'status': 'ERROR', 'error_message': str(e)})\n        return all_races, statuses\n\n# --- Enhanced Trifecta Analyzer Stub ---\nclass EnhancedTrifectaAnalyzer:\n    def __init__(self, settings: Settings):\n        self.settings = settings\n        # TODO: Load ML model and historical data\n\n    def analyze_race_advanced(self, race: Race) -> Race:\n        # TODO: Implement full analysis with base scoring, ML, and historical factors\n        race.checkmate_score = 50.0 # Placeholder\n        race.is_qualified = race.checkmate_score >= self.settings.QUALIFICATION_SCORE\n        race.analysis_details = json.dumps({'base_score': 50.0})\n\n# --- Adapters ---\ndef _convert_odds_to_float(odds_str: Optional[Union[str, float]]) -> Optional[float]:\n    if isinstance(odds_str, float): return odds_str\n    if not odds_str or not isinstance(odds_str, str): return None\n    odds_str = odds_str.strip().upper()\n    if odds_str in ['SP', 'SCRATCHED']: return None\n    if odds_str in ['EVS', 'EVENS']: return 2.0\n    if '/' in odds_str:\n        try:\n            num, den = map(int, odds_str.split('/'))\n            return (num / den) + 1.0 if den != 0 else None\n        except (ValueError, ZeroDivisionError): return None\n    try: return float(odds_str)\n    except (ValueError, TypeError): return None\n\nclass BaseAdapterV7(ABC):\n    def __init__(self, fetcher: DefensiveFetcher): self.fetcher = fetcher\n    @abstractmethod\n    def fetch_races(self) -> List[Race]: raise NotImplementedError\n\nclass TVGAdapter(BaseAdapterV7):\n    SOURCE_ID = \"tvg\"\n    BASE_URL = \"https://mobile-api.tvg.com/api/mobile/races/today\"\n    def fetch_races(self) -> List[Race]:\n        response_data = self.fetcher.get(self.BASE_URL, response_type='json')\n        if not isinstance(response_data, dict) or 'races' not in response_data:\n            logging.warning(f\"TVGAdapter received invalid or non-dict data: {type(response_data)}\")\n            return []\n        all_races = []\n        for race_info in response_data.get('races', []):\n            try:\n                runners = [Runner(name=r.get('horseName', 'N/A'), odds=self._parse_odds(r.get('odds'))) for r in race_info.get('runners', []) if not r.get('scratched') and self._parse_odds(r.get('odds')) is not None]\n                if not runners: continue\n                all_races.append(Race(race_id=f\"tvg_{race_info.get('raceId')}\", track_name=race_info.get('trackName', 'N/A'), race_number=race_info.get('raceNumber'), post_time=datetime.fromisoformat(race_info.get('postTime').replace('Z', '+00:00')) if race_info.get('postTime') else None, runners=runners, source=self.SOURCE_ID))\n            except Exception: continue\n        return all_races\n\n    def _parse_odds(self, odds_data: Optional[Dict]) -> Optional[float]:\n        if not odds_data or odds_data.get('morningLine') is None: return None\n        try:\n            num, den = map(int, odds_data['morningLine'].split('/'))\n            return (num / den) + 1.0\n        except (ValueError, TypeError, ZeroDivisionError): return None\n\nclass BetfairExchangeAdapter(BaseAdapterV7):\n    SOURCE_ID = \"betfair_exchange\"\n    def fetch_races(self) -> List[Race]:\n        races = []\n        endpoint = \"https://ero.betfair.com/www/sports/exchange/readonly/v1/bymarket?alt=json&filter=canonical&maxResults=25&rollupLimit=2&types=EVENT,MARKET_DESCRIPTION,RUNNER_DESCRIPTION,RUNNER_EXCHANGE_PRICES_BEST,MARKET_STATE&marketProjection=EVENT,MARKET_START_TIME,RUNNER_DESCRIPTION&eventTypeIds=7\"\n        try:\n            data = self.fetcher.get(endpoint, response_type='json', headers={'Accept': 'application/json'})\n            if not isinstance(data, dict):\n                logging.warning(f\"BetfairExchangeAdapter received invalid or non-dict data: {type(data)}\")\n                return []\n            parsed_races = self._parse_betfair_races(data)\n            if parsed_races: races.extend(parsed_races)\n        except Exception as e: logging.warning(f\"Betfair endpoint failed: {e}\")\n        for race in races: race.source = self.SOURCE_ID\n        return races\n\n    def _parse_betfair_races(self, data: dict) -> List[Race]:\n        races = []\n        try:\n            event_nodes = data.get('eventTypes', [{}])[0].get('eventNodes', [])\n            for NotImplementedError _nodes:\n                event = event_node.get('event', {})\n                for market_node in event_node.get('marketNodes', []):\n                    market = market_node.get('market', {})\n                    if market.get('marketType', '') != 'WIN': continue\n                    runners = []\n                    for runner in market_node.get('runners', []):\n                        if runner.get('state', {}).get('status') != 'ACTIVE': continue\n                        odds = None\n                        if 'exchange' in runner:\n                            available_to_back = runner['exchange'].get('availableToBack', [])\n                            if available_to_back: odds = available_to_back[0].get('price')\n                        runners.append(Runner(name=runner.get('description', {}).get('runnerName', 'Unknown'), odds=odds))\n                    if len(runners) >= 3:\n                        start_time = None\n                        if market.get('marketStartTime'):\n                            try: start_time = datetime.fromisoformat(market['marketStartTime'].replace('Z', '+00:00'))\n                            except: pass\n                        race = Race(race_id=f\"betfair_{market.get('marketId', 'unknown')}\", track_name=event.get('venue', 'Betfair Exchange'), post_time=start_time, race_number=int(event.get('eventName', '0').split('R')[-1]) if 'R' in event.get('eventName', '') else None, runners=runners)\n                        races.append(race)\n        except Exception as e: logging.error(f\"Error parsing Betfair data structure: {e}\")\n        return races\n\nclass PointsBetAdapter(BaseAdapterV7):\n    SOURCE_ID = \"pointsbet\"\n    BASE_URL = \"https://api.nj.pointsbet.com/api/v2/sports/horse-racing/events/upcoming?page=1\"\n    def fetch_races(self) -> List[Race]:\n        response_data = self.fetcher.get(self.BASE_URL, response_type='json')\n        if not isinstance(response_data, dict) or not response_data.get('events'):\n            logging.warning(f\"PointsBetAdapter received invalid or non-dict data: {type(response_data)}\")\n            return []\n        races = []\n        for event in response_data['events']:\n            try:\n                if not event.get('winPlaceOddsAvailable'): continue\n                runners = [Runner(name=o.get('name', 'Unknown'), odds=_convert_odds_to_float(o.get('price'))) for o in event.get('fixedPrice', {}).get('outcomes', []) if o.get('outcomeType') == 'Win']\n                if len(runners) < 3: continue\n                start_time = datetime.fromisoformat(event['startsAt'].replace('Z', '+00:00')) if event.get('startsAt') else None\n                race = Race(race_id=f\"pointsbet_{event.get('key', 'unknown')}\", track_name=event.get('competitionName', 'Unknown Track'), race_number=event.get('eventNumber'), post_time=start_time, runners=runners, source=self.SOURCE_ID)\n                races.append(race)\n            except Exception as e:\n                logging.warning(f\"Skipping malformed PointsBet event: {e}\")\n                continue\n        return races\n\nPRODUCTION_ADAPTERS = [TVGAdapter, BetfairExchangeAdapter, PointsBetAdapter]\n\n# --- Orchestrator ---\nclass DataSourceOrchestrator:\n    def __init__(self):\n        self.fetcher = DefensiveFetcher()\n        self.adapters: List[BaseAdapterV7] = [Adapter(self.fetcher) for Adapter in PRODUCTION_ADAPTERS]\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    def _fetch_from_adapter(self, adapter: BaseAdapterV7) -> tuple[list[Race], dict]:\n        adapter_id = adapter.__class__.__name__\n        start_time = time.time()\n        status = {\"adapter_id\": adapter_id, \"timestamp\": datetime.now(), \"status\": \"ERROR\", \"races_found\": 0, \"error_message\": \"Unknown error\", \"response_time\": 0}\n        try:\n            races = adapter.fetch_races()\n            end_time = time.time()\n            status.update({\"status\": \"OK\", \"races_found\": len(races), \"error_message\": None, \"response_time\": end_time - start_time})\n            return races, status\n        except Exception as e:\n            end_time = time.time()\n            self.logger.error(f\"Adapter {adapter_id} failed: {e}\", exc_info=True)\n            status.update({\"error_message\": str(e), \"response_time\": end_time - start_time})\n            return [], status\n\n    def get_races(self) -> tuple[list[Race], list[dict]]:\n        all_races, statuses = [], []\n        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.adapters)) as executor:\n            future_to_adapter = {executor.submit(self._fetch_from_adapter, adapter): adapter for adapter in self.adapters}\n            for future in concurrent.futures.as_completed(future_to_adapter):\n                try:\n                    races, status = future.result()\n                    if races: all_races.extend(races)\n                    statuses.append(status)\n                except Exception as e:\n                    adapter_id = future_to_adapter[future].__class__.__name__\n                    self.logger.critical(f\"A future for {adapter_id} failed unexpectedly: {e}\", exc_info=True)\n                    statuses.append({\"adapter_id\": adapter_id, \"timestamp\": datetime.now(), \"status\": \"ERROR\", \"races_found\": 0, \"error_message\": f\"Future failed: {e}\", \"response_time\": 0})\n        self.logger.info(f\"Orchestrator fetched a total of {len(all_races)} races from {len(self.adapters)} adapters.\")\n        return all_races, statuses\n\n# --- Analyzer ---\nclass TrifectaAnalyzer:\n    def analyze_race(self, race: Race, settings: Settings) -> Race:\n        score = 0\n        trifecta_factors = {}\n        horses_with_odds = sorted([r for r in race.runners if r.odds], key=lambda h: h.odds)\n        num_runners = len(horses_with_odds)\n\n        if settings.FIELD_SIZE_OPTIMAL_MIN <= num_runners <= settings.FIELD_SIZE_OPTIMAL_MAX:\n            points, ok, reason = settings.FIELD_SIZE_OPTIMAL_POINTS, True, f\"Optimal field size ({num_runners})\"\n        elif settings.FIELD_SIZE_ACCEPTABLE_MIN <= num_runners <= settings.FIELD_SIZE_ACCEPTABLE_MAX:\n            points, ok, reason = settings.FIELD_SIZE_ACCEPTABLE_POINTS, True, f\"Acceptable field size ({num_runners})\"\n        else:\n            points, ok, reason = settings.FIELD_SIZE_PENALTY_POINTS, False, f\"Field size not ideal ({num_runners})\"\n        score += points\n        trifecta_factors[\"fieldSize\"] = {\"points\": points, \"ok\": ok, \"reason\": reason}\n\n        if num_runners >= 2:\n            fav, sec_fav = horses_with_odds[0], horses_with_odds[1]\n            if fav.odds <= settings.MAX_FAV_ODDS:\n                points, ok, reason = settings.FAV_ODDS_POINTS, True, f\"Favorite odds OK ({fav.odds:.2f})\"\n            else:\n                points, ok, reason = 0, False, f\"Favorite odds too high ({fav.odds:.2f})\"\n            score += points\n            trifecta_factors[\"favoriteOdds\"] = {\"points\": points, \"ok\": ok, \"reason\": reason}\n\n            if sec_fav.odds >= settings.MIN_2ND_FAV_ODDS:\n                points, ok, reason = settings.SECOND_FAV_ODDS_POINTS, True, f\"2nd Favorite OK ({sec_fav.odds:.2f})\"\n            else:\n                points, ok, reason = 0, False, f\"2nd Favorite odds too low ({sec_fav.odds:.2f})\"\n            score += points\n            trifecta_factors[\"secondFavoriteOdds\"] = {\"points\": points, \"ok\": ok, \"reason\": reason}\n\n        race.checkmate_score = score\n        race.is_qualified = score >= settings.QUALIFICATION_SCORE\n        race.trifecta_factors_json = json.dumps(trifecta_factors)\n\n        return race"
}