{
    "file_path": "convert_to_json.py",
    "content": "# convert_to_json.py\n\nimport json\nimport os\nimport re\nimport sys\nfrom multiprocessing import Process, Queue\n\n# --- Configuration ---\nMANIFEST_FILES = ['MANIFEST2.md', 'MANIFEST3.md']\nOUTPUT_DIR = 'ReviewableJSON'\nFILE_PROCESSING_TIMEOUT = 10  # Seconds to wait before killing a hung file read\n\n# --- Core Functions ---\ndef parse_manifest_for_links(manifest_path):\n    \"\"\"Parses a manifest file to extract raw GitHub file links.\"\"\"\n    if not os.path.exists(manifest_path):\n        return []\n    with open(manifest_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    return re.findall(r'(https://raw\\.githubusercontent\\.com/masonj0/scrape-sort_races-toteboards/refs/heads/main/[-/\\w\\.]+)', content)\n\ndef _sandboxed_file_read(file_path, q):\n    \"\"\"This function runs in a separate process to read a file.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            content = f.read()\n        q.put({\"file_path\": file_path, \"content\": content})\n    except Exception as e:\n        q.put({\"error\": str(e)})\n\ndef convert_file_to_json_sandboxed(file_path):\n    \"\"\"Reads a file in a sandboxed process with a timeout.\"\"\"\n    q = Queue()\n    p = Process(target=_sandboxed_file_read, args=(file_path, q))\n    p.start()\n    p.join(timeout=FILE_PROCESSING_TIMEOUT)\n\n    if p.is_alive():\n        p.terminate()\n        p.join() # Ensure termination is complete\n        return {\"error\": f\"Timeout: File processing took longer than {FILE_PROCESSING_TIMEOUT} seconds.\"}\n\n    if not q.empty():\n        return q.get()\n    return {\"error\": \"Unknown error in sandboxed read process.\"}\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'='*60}\\nStarting IRONCLAD JSON backup process...\\n{'='*60}\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    all_links = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        links = parse_manifest_for_links(manifest)\n        all_links.extend(links)\n        print(f\"    --> Found {len(links)} links.\")\n\n    if not all_links:\n        print(\"\\n[FATAL] No links found in any manifest. Aborting.\")\n        return\n\n    print(f\"\\nFound a total of {len(all_links)} unique files to process.\")\n    processed_count, failed_count = 0, 0\n\n    for link in set(all_links): # Use set to avoid processing duplicate links\n        local_path = '/'.join(link.split('/main/')[1:])\n        print(f\"\\nProcessing: {local_path}\")\n\n        json_data = convert_file_to_json_sandboxed(local_path)\n\n        if json_data and \"error\" not in json_data:\n            output_path = os.path.join(OUTPUT_DIR, local_path + '.json')\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4)\n\n            print(f\"    [SUCCESS] Saved backup to {output_path}\")\n            processed_count += 1\n        else:\n            error_msg = json_data.get(\"error\", \"Unknown error\") if json_data else \"File not found\"\n            print(f\"    [ERROR] Failed to process {local_path}: {error_msg}\")\n            failed_count += 1\n\n    print(f\"\\n{'='*60}\\nBackup process complete.\\nSuccessfully processed: {processed_count}/{len(all_links)}\\nFailed/Skipped: {failed_count}\\n{'='*60}\")\n\n    if failed_count > 0:\n        sys.exit(1) # Exit with an error code if any files failed\n\nif __name__ == \"__main__\":\n    main()"
}