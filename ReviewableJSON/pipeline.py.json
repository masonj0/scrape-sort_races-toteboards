{"content": "import inspect\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\n\nfrom . import adapters\nfrom .scorer import RaceScorer\nfrom .base import BaseAdapter, BaseAdapterV3\nfrom .ui.terminal_ui import TerminalUI\n\ndef load_adapters(specific_source: str = None) -> List[type]:\n    \"\"\"\n    Dynamically loads adapter classes from the adapters module.\n    If a specific_source is provided, only that adapter is loaded.\n    \"\"\"\n    adapter_classes = []\n    for name, obj in inspect.getmembers(adapters, inspect.isclass):\n        if issubclass(obj, (BaseAdapter, BaseAdapterV3)) and obj not in (BaseAdapter, BaseAdapterV3):\n            if specific_source and hasattr(obj, 'SOURCE_ID') and obj.SOURCE_ID != specific_source:\n                continue\n            adapter_classes.append(obj)\n    return adapter_classes\n\n\nasync def run_pipeline(min_runners: int, specific_source: str = None):\n    \"\"\"\n    Orchestrates the end-to-end pipeline for fetching, parsing, and scoring races.\n    \"\"\"\n    ui = TerminalUI()\n    ui.setup_logging()\n\n    logging.info(\"--- Paddock Parser NG Pipeline Start ---\")\n\n    all_races = []\n    adapter_classes = load_adapters(specific_source)\n    \n    if not adapter_classes:\n        logging.warning(f\"No adapters found for source: '{specific_source}'\" if specific_source else \"No adapters found at all.\")\n        return\n\n    ui.start_fetching_progress(len(adapter_classes))\n\n    for adapter_class in adapter_classes:\n        adapter = adapter_class()\n        logging.info(f\"Running adapter: {getattr(adapter, 'SOURCE_ID', 'Unknown')}...\")\n\n        try:\n            normalized_races = []\n            if isinstance(adapter, BaseAdapterV3):\n                races = await adapter.fetch()\n                normalized_races.extend(races)\n            elif isinstance(adapter, BaseAdapter):\n                raw_data = adapter.fetch_data()\n                normalized_races = adapter.parse_data(raw_data)\n\n            if normalized_races:\n                logging.info(f\"Parsed {len(normalized_races)} races from {getattr(adapter, 'SOURCE_ID', 'Unknown')}.\")\n                all_races.extend(normalized_races)\n            else:\n                logging.warning(f\"No races parsed for {getattr(adapter, 'SOURCE_ID', 'Unknown')}.\")\n\n        except NotImplementedError:\n            logging.warning(f\"Adapter {getattr(adapter, 'SOURCE_ID', 'Unknown')} does not support live fetching and was skipped.\")\n        except Exception as e:\n            logging.error(f\"Adapter {getattr(adapter, 'SOURCE_ID', 'Unknown')} failed: {e}\", exc_info=True)\n        finally:\n            ui.update_fetching_progress()\n\n    ui.stop_fetching_progress()\n\n    if not all_races:\n        logging.info(\"No races were successfully parsed from any source.\")\n        logging.info(\"--- Pipeline End ---\")\n        return\n\n    # Filter races based on field size\n    if min_runners > 1:\n        all_races = [r for r in all_races if r.number_of_runners and r.number_of_runners >= min_runners]\n\n    # Score the races\n    scorer = RaceScorer()\n    for race in all_races:\n        race.score = scorer.score(race)\n\n    # Sort races by score (descending)\n    all_races.sort(key=lambda r: r.score or 0, reverse=True)\n\n    # Display the results using the new TerminalUI\n    ui.display_races(all_races)\n\n    logging.info(\"--- Pipeline End ---\")\n"}
