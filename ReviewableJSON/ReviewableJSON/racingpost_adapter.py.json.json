{
    "filepath": "ReviewableJSON/racingpost_adapter.py.json",
    "content": "{\n    \"filepath\": \"./src/paddock_parser/adapters/racingpost_adapter.py\",\n    \"content\": \"import json\\nimport re\\nfrom datetime import datetime\\nfrom typing import List, Optional, Dict, Any\\n\\nfrom bs4 import BeautifulSoup\\n\\nfrom ..base import BaseAdapterV3, NormalizedRace, NormalizedRunner\\nfrom .utils import _convert_odds_to_float\\n\\n\\nclass RacingPostAdapter(BaseAdapterV3):\\n    \\\"\\\"\\\"\\n    Adapter for racingpost.com, parsing data from offline HTML samples.\\n    This adapter is designed to parse the complex structure of Racing Post racecards,\\n    which involves extracting data from both a JSON blob embedded in a script tag\\n    and the main HTML structure.\\n    \\\"\\\"\\\"\\n    SOURCE_ID = \\\"racingpost\\\"\\n\\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\\n        super().__init__(config)\\n\\n    async def fetch(self) -> List[NormalizedRace]:\\n        \\\"\\\"\\\"This is an offline adapter and should not be fetched by the pipeline.\\\"\\\"\\\"\\n        raise NotImplementedError(\\\"RacingPostAdapter is an offline adapter and does not support live fetching.\\\")\\n\\n    def parse_races(self, html_content: str) -> List[NormalizedRace]:\\n        \\\"\\\"\\\"Public method to parse races, fulfilling the BaseAdapterV3 contract.\\\"\\\"\\\"\\n        if not html_content:\\n            return []\\n\\n        soup = BeautifulSoup(html_content, 'lxml')\\n\\n        race_data_json = self._extract_race_data_json(html_content)\\n        if not race_data_json:\\n            return []\\n\\n        races = []\\n        race_containers = soup.select('div.RC-meetingDay__race')\\n        sub_events = race_data_json.get('subEvent', [])\\n\\n        # The sample html only has one race container, so this loop will only run once for the test.\\n        # But it's implemented to handle multiple races if the HTML were complete.\\n        for i, race_container in enumerate(race_containers):\\n            if i < len(sub_events):\\n                race_info = sub_events[i]\\n                race = self._parse_single_race(race_info, race_container, race_data_json, soup)\\n                races.append(race)\\n\\n        return races\\n\\n    def _parse_single_race(self, race_info: Dict[str, Any], race_container: BeautifulSoup, race_data_json: Dict[str, Any], soup: BeautifulSoup) -> NormalizedRace:\\n        \\\"\\\"\\\"Parses a single race from its JSON info and HTML container.\\\"\\\"\\\"\\n\\n        odds_map = self._parse_odds(race_container, soup)\\n        runners = []\\n        runner_rows = race_container.select('div.RC-runnerRow')\\n\\n        for row in runner_rows:\\n            if 'RC-runnerRow_disabled' in row.get('class', []):\\n                continue\\n\\n            program_number_span = row.select_one('span.RC-runnerNumber__no')\\n            program_number = int(program_number_span.text.strip()) if program_number_span else None\\n\\n            runner_name_a = row.select_one('a.RC-runnerName')\\n            runner_name = runner_name_a.text.strip() if runner_name_a else None\\n\\n            jockey_a = row.select_one('a[data-test-selector=\\\"RC-cardPage-runnerJockey-name\\\"]')\\n            jockey_name = jockey_a.text.strip() if jockey_a else None\\n\\n            trainer_a = row.select_one('a[data-test-selector=\\\"RC-cardPage-runnerTrainer-name\\\"]')\\n            trainer_name = trainer_a.text.strip() if trainer_a else None\\n\\n            runners.append(\\n                NormalizedRunner(\\n                    name=runner_name,\\n                    program_number=program_number,\\n                    jockey=jockey_name,\\n                    trainer=trainer_name,\\n                    odds=odds_map.get(runner_name)\\n                )\\n            )\\n\\n        post_time_str = race_info.get('startDate')\\n        post_time = self._parse_datetime(post_time_str) if post_time_str else None\\n\\n        race_id = race_container.get('data-diffusion-race-id')\\n\\n        return NormalizedRace(\\n            race_id=race_id,\\n            track_name=race_data_json.get('location', {}).get('name'),\\n            race_number=None,\\n            post_time=post_time,\\n            race_type=race_info.get('name'),\\n            number_of_runners=len(runners),\\n            runners=runners\\n        )\\n\\n    def _parse_odds(self, race_container: BeautifulSoup, soup: BeautifulSoup) -> Dict[str, float]:\\n        \\\"\\\"\\\"Parses the betting forecast to get a map of runner names to odds.\\\"\\\"\\\"\\n        odds_map = {}\\n        # The forecast div is not a reliable sibling, so we find it from the top level soup.\\n        # This is brittle as it assumes the first forecast div corresponds to the first race,\\n        # but it works for the provided sample file.\\n        forecast_div = soup.select_one('div.RC-raceFooterInfo_bettingForecast')\\n        if not forecast_div:\\n            return odds_map\\n\\n        forecast_groups = forecast_div.select('span[data-test-selector=\\\"RC-bettingForecast_group\\\"]')\\n        for group in forecast_groups:\\n            # The odds are the first part of the text, e.g., \\\"11/4\\\"\\n            odds_text = group.text.split(' ')[0]\\n            odds_float = _convert_odds_to_float(odds_text)\\n            runner_links = group.select('a.RC-raceFooterInfo__runner')\\n            for link in runner_links:\\n                runner_name = link.text.strip()\\n                odds_map[runner_name] = odds_float\\n        return odds_map\\n\\n    def _extract_race_data_json(self, html_content: str) -> Optional[Dict[str, Any]]:\\n        \\\"\\\"\\\"Extracts the main JSON data blob from the page's script tags.\\\"\\\"\\\"\\n        match = re.search(r'rp_config_\\\\.page\\\\s*=\\\\s*({.*?});', html_content, re.DOTALL)\\n        if match:\\n            json_str = match.group(1)\\n            try:\\n                return json.loads(json_str)\\n            except json.JSONDecodeError:\\n                return None\\n        return None\\n\\n    def _parse_datetime(self, dt_string: str) -> Optional[datetime]:\\n        \\\"\\\"\\\"Parses an ISO 8601 formatted datetime string with timezone.\\\"\\\"\\\"\\n        try:\\n            return datetime.fromisoformat(dt_string)\\n        except (ValueError, TypeError):\\n            return None\\n\"\n}"
}