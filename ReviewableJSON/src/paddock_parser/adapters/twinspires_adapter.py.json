{
    "filepath": "src/paddock_parser/adapters/twinspires_adapter.py",
    "content": "#!/usr/bin/env python3\n\"\"\"\nA V3-compliant adapter for TwinSpires, based on direct reconnaissance.\nThis adapter uses a two-stage scraping process to acquire race data.\n\"\"\"\nimport anyio\nimport logging\nfrom datetime import datetime\nfrom typing import List, Optional, Dict, Any\n\nfrom ..base import BaseAdapterV3, NormalizedRace, NormalizedRunner\nfrom ..fetcher import get_page_content\nfrom bs4 import BeautifulSoup\n\nclass TwinSpiresAdapter(BaseAdapterV3):\n    \"\"\" An adapter for the TwinSpires website. \"\"\"\n\n    SOURCE_ID = \"twinspires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        super().__init__(config)\n\n    async def fetch(self) -> List[NormalizedRace]:\n        \"\"\" Fetches all race data using a two-stage process. \"\"\"\n        index_url = f\"{self.BASE_URL}/adw/todays-tracks?sortOrder=nextUp\"\n        logging.info(f\"Fetching race index from: {index_url}\")\n        index_html = await get_page_content(index_url)\n\n        if not index_html:\n            logging.warning(\"Failed to fetch TwinSpires race index.\")\n            return []\n\n        soup = BeautifulSoup(index_html, 'html.parser')\n        race_links = []\n        for link in soup.find_all('a', href=lambda h: h and '/race/' in h and '/results/' not in h):\n            race_links.append(self.BASE_URL + link['href'])\n\n        if not race_links:\n            logging.warning(\"No race links found on TwinSpires index page.\")\n            return []\n\n        logging.info(f\"Found {len(race_links)} races. Fetching details concurrently...\")\n\n        results_map = {}\n        async with anyio.create_task_group() as tg:\n            for link in race_links:\n                async def work(url):\n                    results_map[url] = await get_page_content(url)\n\n                tg.start_soon(work, link)\n\n        successful_htmls = [html for html in results_map.values() if isinstance(html, str)]\n        return self._parse_race_detail_pages(successful_htmls)\n\n    def _parse_race_detail_pages(self, race_htmls: list[str]) -> list[NormalizedRace]:\n        \"\"\" Parses a list of race detail HTML snippets. \"\"\"\n        all_races = []\n        for html in race_htmls:\n            try:\n                race = self._parse_single_race_detail(html)\n                if race:\n                    all_races.append(race)\n            except Exception as e:\n                logging.warning(f\"Skipping a malformed race detail page in TwinSpires parse: {e}\", exc_info=True)\n                continue\n        return all_races\n\n    def _parse_single_race_detail(self, html: str) -> Optional[NormalizedRace]:\n        \"\"\" Parses a single race detail HTML page. \"\"\"\n        soup = BeautifulSoup(html, 'html.parser')\n\n        header = soup.find('div', class_='race-title')\n        if not header: return None\n\n        track_name = header.find('a').text.strip()\n        race_number_text = header.find('strong').text\n        race_number = int(''.join(filter(str.isdigit, race_number_text)))\n\n        # A full implementation would parse post time, distance, etc.\n        # Using a placeholder for now.\n        post_time = datetime.now()\n\n        runners = []\n        program = soup.find('div', id='program')\n        if not program: return None\n\n        for i, runner_row in enumerate(program.find_all('div', class_='runner-wrapper')):\n            name_tag = runner_row.find('div', class_='runner-name')\n            if not name_tag: continue\n            name = name_tag.text.strip()\n\n            odds_span = runner_row.find('span', class_='odds')\n            odds_str = odds_span.text.strip() if odds_span else None\n\n            if not odds_str: continue\n\n            try:\n                if '/' in odds_str:\n                    num, den = map(int, odds_str.split('/'))\n                    if den == 0: continue\n                    odds = (num / den) + 1.0\n                else:\n                    odds = float(odds_str) + 1.0\n\n                runners.append(NormalizedRunner(\n                    name=name,\n                    odds=odds,\n                    program_number=i + 1 # Program number is based on order\n                ))\n            except (ValueError, TypeError):\n                continue\n\n        if not runners:\n            return None\n\n        race_id = f\"{track_name.replace(' ', '')}-{race_number}\"\n\n        return NormalizedRace(\n            race_id=race_id,\n            track_name=track_name,\n            race_number=race_number,\n            post_time=post_time,\n            number_of_runners=len(runners),\n            runners=runners\n        )\n\n    def parse_races(self, html_content: str) -> List[NormalizedRace]:\n        \"\"\"\n        Parses a single page containing multiple race summaries.\n        Not used by this adapter's two-stage fetch process.\n        \"\"\"\n        return []\n"
}