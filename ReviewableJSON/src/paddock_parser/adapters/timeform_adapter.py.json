{
    "filepath": "src/paddock_parser/adapters/timeform_adapter.py",
    "content": "import asyncio\nimport logging\nimport re\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom urllib.parse import urljoin\n\nfrom ..base import BaseAdapterV3, NormalizedRace, NormalizedRunner\nfrom ..http_client import ForagerClient\nfrom .utils import _convert_odds_to_float\n\nclass TimeformAdapter(BaseAdapterV3):\n    \"\"\"\n    Adapter for timeform.com.\n    This adapter implements a three-stage \"drill-down\" to fetch all race data.\n    1. Fetch the main racecards page.\n    2. Extract links to all of today's meetings.\n    3. Concurrently fetch all race detail pages.\n    4. Parse each detail page to extract runner information.\n    \"\"\"\n    SOURCE_ID = \"timeform\"\n    BASE_URL = \"https://www.timeform.com\"\n\n    def __init__(self, config=None):\n        super().__init__(config)\n        self.forager = ForagerClient()\n\n    async def fetch(self) -> List[NormalizedRace]:\n        \"\"\"\n        Fetches all race data by first getting the summary page to find\n        race links, then fetching each of those pages concurrently.\n        \"\"\"\n        index_url = f\"{self.BASE_URL}/horse-racing/racecards\"\n        index_html = await self.forager.fetch(index_url)\n        if not index_html:\n            logging.warning(\"Failed to fetch the Timeform racecards index page.\")\n            return []\n\n        race_links = self._extract_race_links(index_html)\n        if not race_links:\n            logging.warning(\"No race links found on the Timeform index page.\")\n            return []\n\n        tasks = [self.forager.fetch(url) for url in race_links]\n        race_html_pages = await asyncio.gather(*tasks)\n\n        all_races = []\n        for html, url in zip(race_html_pages, race_links):\n            if html:\n                race = self.parse_race_details(html, url)\n                if race:\n                    all_races.append(race)\n        return all_races\n\n    def _extract_race_links(self, html_content: str) -> List[str]:\n        \"\"\"Extracts all individual race links from the main racecards page.\"\"\"\n        soup = BeautifulSoup(html_content, 'lxml')\n        links = []\n        for link in soup.select(\"ul.w-racecard-grid-meeting-races-compact li a\"):\n            href = link.get(\"href\")\n            if href:\n                links.append(urljoin(self.BASE_URL, href))\n        return links\n\n    def parse_race_details(self, html_content: str, url: str) -> Optional[NormalizedRace]:\n        \"\"\"Parses the race detail page to extract all available data.\"\"\"\n        soup = BeautifulSoup(html_content, 'lxml')\n\n        header = soup.select_one(\"div.rp-race-card-header h1\")\n        if not header:\n            return None\n\n        header_text = header.get_text(strip=True)\n        time_match = re.search(r\"(\\d{2}:\\d{2})\", header_text)\n        if not time_match:\n            return None\n\n        race_time_str = time_match.group(1)\n        track_name = header_text.replace(race_time_str, \"\").strip()\n\n        runners = []\n        for runner_item in soup.select(\"li.rp-race-card__runner\"):\n            saddle_cloth_tag = runner_item.select_one(\".rp-race-card__runner__saddle-cloth\")\n            name_tag = runner_item.select_one(\".rp-race-card__runner__name\")\n            odds_tag = runner_item.select_one(\".rp-race-card__runner__odds\")\n\n            if not all([saddle_cloth_tag, name_tag, odds_tag]):\n                continue\n\n            runners.append(\n                NormalizedRunner(\n                    program_number=int(saddle_cloth_tag.get_text(strip=True)),\n                    name=name_tag.get_text(strip=True),\n                    odds=_convert_odds_to_float(odds_tag.get_text(strip=True)),\n                )\n            )\n\n        if not runners:\n            return None\n\n        try:\n            post_time = datetime.strptime(f\"{datetime.now().date()} {race_time_str}\", \"%Y-%m-%d %H:%M\")\n        except ValueError:\n            post_time = None\n\n        return NormalizedRace(\n            race_id=url,\n            track_name=track_name,\n            race_number=0, # Not available on detail page in this format\n            post_time=post_time,\n            runners=runners,\n            number_of_runners=len(runners),\n        )\n\n    def parse_races(self, html_content: str) -> List[NormalizedRace]:\n        \"\"\"\n        This method is required by the BaseAdapterV3 interface, but is not\n        used by the new drill-down fetch logic.\n        \"\"\"\n        pass\n"
}