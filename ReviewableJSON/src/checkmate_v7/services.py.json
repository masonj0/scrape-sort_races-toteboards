{
    "filepath": "./src/checkmate_v7/services.py",
    "content": "\"\"\"\nCheckmate V7: `services.py` - THE GATEWAY\n\"\"\"\nimport logging\nimport asyncio\nimport random\nimport time\nimport anyio\nimport json\nimport re\nfrom abc import ABC, abstractmethod\nfrom datetime import date, datetime, timezone\nfrom typing import List, Optional\n\nimport aiohttp\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom celery import Celery\nfrom io import StringIO\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom . import config, logic\nfrom .models import AdapterStatusORM, PredictionORM, ResultORM, JoinORM, Base, Race, Runner\n\n# --- Celery App Configuration ---\ncelery_app = Celery('tasks', broker=config.REDIS_URL)\n\n# --- Celery Logging Integration ---\nfrom celery.signals import after_setup_logger, after_setup_task_logger\n\n@after_setup_logger.connect\n@after_setup_task_logger.connect\ndef setup_celery_logging(logger, **kwargs):\n    \"\"\"\n    This function is connected to Celery's logging signals and\n    reconfigures the logger to use our structured JSON format.\n    \"\"\"\n    # Use the same handlers from the root logger\n    for handler in logging.getLogger().handlers:\n        logger.addHandler(handler)\n    \n    logger.propagate = False\n\nclass CircuitBreaker:\n    def __init__(self, failure_threshold=5, recovery_timeout=60):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.failure_count = 0\n        self.state = \"closed\"\n        self.last_failure_time = None\n\n    async def __aenter__(self):\n        if self.state == \"open\":\n            if time.time() - self.last_failure_time > self.recovery_timeout:\n                self.state = \"half-open\"\n            else:\n                raise Exception(\"Circuit is open\")\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if exc_type:\n            self.failure_count += 1\n            if self.failure_count >= self.failure_threshold:\n                self.state = \"open\"\n                self.last_failure_time = time.time()\n        elif self.state == \"half-open\":\n            self.failure_count = 0\n            self.state = \"closed\"\n\nclass DefensiveFetcher:\n    def __init__(self):\n        self.circuit_breakers = {}\n\n    async def fetch(self, url, headers=None, response_type='text'):\n        return await self._request('GET', url, headers=headers, response_type=response_type)\n\n    async def post(self, url, headers=None, json_data=None, response_type='json'):\n        return await self._request('POST', url, headers=headers, json_data=json_data, response_type=response_type)\n\n    async def _request(self, method, url, headers=None, json_data=None, response_type='text'):\n        domain = url.split('/')[2]\n        if domain not in self.circuit_breakers:\n            self.circuit_breakers[domain] = CircuitBreaker()\n\n        cb = self.circuit_breakers[domain]\n\n        retries = 5\n        for i in range(retries):\n            try:\n                async with cb:\n                    async with aiohttp.ClientSession(headers=headers) as session:\n                        await asyncio.sleep(random.uniform(0.5, 1.5))\n\n                        async with session.request(method, url, json=json_data, timeout=15) as response:\n                            response.raise_for_status()\n                            if response_type == 'json':\n                                return await response.json()\n                            else:\n                                return await response.text()\n            except Exception as e:\n                logging.warning(f\"Attempt {i+1}/{retries} failed for {method} {url}: {e}\")\n                if i < retries - 1:\n                    wait_time = (2 ** i) + random.uniform(0, 1)\n                    logging.info(f\"Retrying in {wait_time:.2f} seconds...\")\n                    await asyncio.sleep(wait_time)\n                else:\n                    logging.error(f\"All retries failed for {method} {url}\")\n                    raise\n\nclass DataSourceOrchestrator:\n    def __init__(self, session):\n        self.fetcher = DefensiveFetcher()\n        self.db_session = session\n        # Gold, Silver, Bronze tiering\n        self.adapters: List[BaseAdapterV7] = [\n            # --- NEW TIER 1 API ADAPTERS ---\n            FanDuelGraphQLAdapterV7(self.fetcher),\n            BetfairDataScientistAdapterV7(self.fetcher),\n            TwinspiresAdapterV7(self.fetcher),\n            # --- EXISTING FOUNDATION ---\n            RacingPostAdapterV7(self.fetcher),\n            PointsBetAdapterV7(self.fetcher),\n            EquibaseAdapterV7(self.fetcher),\n        ]\n\n    async def get_races(self) -> List[Race]:\n        \"\"\"\n        Iterates through adapters by priority, attempting to fetch races.\n        Returns the first successful, non-empty list of races.\n        \"\"\"\n        all_races = []\n        for adapter in self.adapters:\n            adapter_name = adapter.SOURCE_ID\n            logging.info(f\"Attempting to fetch races from {adapter_name}\")\n            try:\n                # In a real-world scenario, the URL would be dynamic or configured\n                # For now, we call fetch_races without arguments where possible\n                # or with a default/mock URL.\n                if isinstance(adapter, RacingPostAdapterV7):\n                    # This adapter needs a specific URL, which we don't have in this context.\n                    # We will rely on other adapters for now.\n                    logging.warning(f\"Skipping {adapter_name} as it requires a specific URL.\")\n                    continue\n\n                races = await adapter.fetch_races()\n\n                if races:\n                    logging.info(f\"Successfully fetched {len(races)} races from {adapter_name}\")\n                    # In a real system, we might aggregate or just return the first success\n                    all_races.extend(races)\n                    # For now, let's return after the first successful fetch to avoid duplicates\n                    return all_races\n                else:\n                    logging.info(f\"No races found from {adapter_name}\")\n\n            except Exception as e:\n                logging.error(f\"Failed to fetch from {adapter_name}: {e}\")\n                # Here we would update the adapter status in the DB\n                # e.g., await self.update_adapter_status(adapter_name, \"error\")\n                continue\n\n        logging.info(f\"Harvested a total of {len(all_races)} races from all sources.\")\n        return all_races\n\ndef get_db_session():\n    engine = create_engine(config.DATABASE_URL)\n    Base.metadata.create_all(bind=engine)\n    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n    return SessionLocal()\n\n@celery_app.task(bind=True, max_retries=3)\ndef run_prediction_cycle(self):\n    \"\"\"The main entry point task for the Prediction Engine.\"\"\"\n    logging.info(\"Starting prediction cycle...\")\n    session = None\n    try:\n        session = get_db_session()\n        orchestrator = DataSourceOrchestrator(session)\n        races = asyncio.run(orchestrator.get_races())\n\n        logging.info(f\"Found {len(races)} races to process.\")\n        for race in races:\n            # Convert dataclass to dict for Celery serialization\n            process_race_for_prediction.delay(race.__dict__)\n\n    except SQLAlchemyError as e:\n        logging.error(\"Database error during prediction cycle\", extra={\"error\": str(e)})\n        self.retry(exc=e, countdown=60)\n    except Exception as e:\n        logging.error(\"An unexpected error occurred during prediction cycle\", extra={\"error\": str(e)})\n        self.retry(exc=e, countdown=60)\n    finally:\n        if session:\n            session.close()\n\n@celery_app.task\ndef process_race_for_prediction(race_data: dict):\n    \"\"\"Analyzes a single race and saves a prediction if it qualifies.\"\"\"\n    # This task is now designed to be called by the main cycle\n    session = get_db_session()\n    try:\n        # The race data would be a dict here, needs to be reconstructed\n        # For now, we'll just log it.\n        logging.info(f\"Processing race: {race_data.get('track')} R{race_data.get('race_number')}\")\n        # analyzer = logic.AdvancedPlaceBetAnalyzer() # When implemented\n        # analysis = analyzer.analyze(race_data)\n        # if analysis.should_bet:\n        #     # db_manager.save_prediction(analysis)\n        #     pass\n    finally:\n        session.close()\n\n\n@celery_app.task(bind=True, max_retries=3)\ndef run_audit_cycle(self):\n    \"\"\"The main entry point task for the Historian Engine.\"\"\"\n    logging.info(\"Starting audit cycle...\")\n    session = None\n    try:\n        session = get_db_session()\n        pending_predictions = session.query(PredictionORM).filter_by(status='pending').all()\n        logging.info(f\"Found {len(pending_predictions)} pending predictions to audit.\")\n        for pred in pending_predictions:\n            # Check if race time has passed\n            # For now, we just queue all pending\n            process_race_for_results.delay(pred.race_key)\n    except SQLAlchemyError as e:\n        logging.error(\"Database error during audit cycle\", extra={\"error\": str(e)})\n        self.retry(exc=e, countdown=300)\n    except Exception as e:\n        logging.error(\"An unexpected error occurred during audit cycle\", extra={\"error\": str(e)})\n        self.retry(exc=e, countdown=300)\n    finally:\n        if session:\n            session.close()\n\n\n@celery_app.task\ndef process_race_for_results(race_key: str):\n    \"\"\"Fetches results for a single race and creates an audit record.\"\"\"\n    logging.info(f\"Fetching results for {race_key}...\")\n    # In a real implementation, this would fetch real results and\n    # create a real audit record. For now, it's a placeholder.\n    pass\n\n# --- V7 Adapters ---\n\nclass BaseAdapterV7(ABC):\n    def __init__(self, defensive_fetcher: DefensiveFetcher):\n        self.fetcher = defensive_fetcher\n\n    @abstractmethod\n    async def fetch_races(self) -> List[Race]:\n        raise NotImplementedError\n\nclass RacingPostAdapterV7(BaseAdapterV7):\n    SOURCE_ID = \"racingpost\"\n    BASE_URL = \"https://www.racingpost.com\" # Example, not used in offline parse\n\n    async def fetch_races(self, url: str) -> List[Race]:\n        \"\"\"Fetches and parses races from a given Racing Post URL.\"\"\"\n        html_content = await self.fetcher.fetch(url)\n        if not html_content:\n            return []\n        return self._parse_races(html_content)\n\n    def _parse_races(self, html_content: str) -> List[Race]:\n        \"\"\"Parses races from the HTML content.\"\"\"\n        if not html_content:\n            return []\n\n        soup = BeautifulSoup(html_content, 'lxml')\n        race_data_json = self._extract_race_data_json(html_content)\n        if not race_data_json:\n            return []\n\n        races = []\n        # This logic is transplanted from the legacy adapter and adapted for V7 models\n        # It is simplified for this example and would need to be more robust in production.\n        race_containers = soup.select('div.RC-meetingDay__race')\n        for race_container in race_containers:\n            race_id = race_container.get('data-diffusion-race-id')\n            track_name = race_data_json.get('location', {}).get('name')\n\n            # --- Full Runner Parsing Logic ---\n            runners = []\n            runner_rows = race_container.select('div.RC-runnerRow')\n            for row in runner_rows:\n                if 'RC-runnerRow_disabled' in row.get('class', []):\n                    continue\n\n                program_number_span = row.select_one('span.RC-runnerNumber__no')\n                program_number = int(program_number_span.text.strip()) if program_number_span else None\n\n                runner_name_a = row.select_one('a.RC-runnerName')\n                runner_name = runner_name_a.text.strip() if runner_name_a else \"Unknown Runner\"\n\n                # V7 Runner model does not include jockey, trainer, or odds.\n                # This data is available in the source but will be omitted here.\n\n                runners.append(Runner(\n                    name=runner_name,\n                    program_number=program_number\n                ))\n\n            races.append(Race(\n                race_id=race_id,\n                track_name=track_name,\n                race_number=None, # Not easily available in top-level data\n                runners=runners\n            ))\n        return races\n\n    def _extract_race_data_json(self, html_content: str) -> dict:\n        \"\"\"Extracts the main JSON data blob from the page's script tags.\"\"\"\n        # Corrected the regex to match 'rp_config_.page' instead of 'rp_config_\\.page'\n        match = re.search(r'rp_config_\\.page\\s*=\\s*({.*?});', html_content, re.DOTALL)\n        if match:\n            json_str = match.group(1)\n            try:\n                return json.loads(json_str)\n            except json.JSONDecodeError:\n                return {}\n        return {}\n\nclass PointsBetAdapterV7(BaseAdapterV7):\n    SOURCE_ID = \"pointsbet\"\n    API_URL = \"https://api.au.pointsbet.com/api/v2/racing/races/today\"\n\n    async def fetch_races(self) -> List[Race]:\n        \"\"\"Fetches and parses races from the PointsBet API.\"\"\"\n        raw_content = await self.fetcher.fetch(self.API_URL)\n        if not raw_content:\n            return []\n        try:\n            json_content = json.loads(raw_content)\n        except json.JSONDecodeError:\n            logging.error(f\"{self.SOURCE_ID}: Failed to decode JSON from response.\")\n            return []\n        return self._parse_races(json_content.get('events', []))\n\n    def _parse_races(self, events: list) -> List[Race]:\n        \"\"\"Parses the JSON response from the PointsBet API.\"\"\"\n        all_races = []\n        for event in events:\n            try:\n                if not event.get('runners') or not event.get('meetingName'):\n                    continue\n\n                runners = []\n                for runner_data in event.get('runners', []):\n                    program_number = runner_data.get('runnerNumber')\n                    if not program_number:\n                        continue\n\n                    odds_obj = runner_data.get('fixedWinOdds')\n                    odds = None\n                    if odds_obj:\n                        odds = odds_obj.get('price')\n\n                    runners.append(Runner(\n                        name=runner_data.get('name', 'Unknown Runner'),\n                        program_number=int(program_number),\n                        odds=float(odds) if odds else None\n                    ))\n\n                if runners:\n                    all_races.append(Race(\n                        race_id=event.get('key'),\n                        track_name=event.get('meetingName'),\n                        race_number=event.get('raceNumber'),\n                        runners=runners\n                    ))\n            except (KeyError, TypeError, ValueError) as e:\n                logging.warning(f\"PointsBetV7: Skipping a malformed event in parse: {e}\")\n                continue\n        return all_races\n\nclass EquibaseAdapterV7(BaseAdapterV7):\n    SOURCE_ID = \"equibase\"\n    BASE_URL = \"http://www.equibase.com\"\n\n    async def fetch_races(self) -> List[Race]:\n        \"\"\"\n        Fetches all race data for today via a two-step process:\n        1. Fetch the main entries page to get the race schedule.\n        2. Fetch the detail page for each race to get the runners.\n        \"\"\"\n        date_str = date.today().strftime('%m%d%y')\n        entries_url = f\"{self.BASE_URL}/entries/ENT_{date_str}.html?COUNTRY=USA\"\n\n        # Step 1: Fetch and parse the main schedule page\n        schedule_html = await self.fetcher.fetch(entries_url)\n        if not schedule_html:\n            return []\n\n        partial_races, detail_urls = self._parse_race_schedule(schedule_html)\n        \n        # Create a mapping from URL to race object to recombine later\n        url_to_race_map = {url: race for url, race in zip(detail_urls, partial_races)}\n        \n        # Step 2: Fetch all detail pages concurrently using anyio\n        detail_pages_html = {}\n\n        async def fetch_and_store(url):\n            html = await self.fetcher.fetch(f\"{self.BASE_URL}{url}\")\n            if html:\n                detail_pages_html[url] = html\n\n        async with anyio.create_task_group() as tg:\n            for url in detail_urls:\n                tg.start_soon(fetch_and_store, url)\n\n        # Step 3: Parse runners from detail pages and combine\n        final_races = []\n        for url, race in url_to_race_map.items():\n            detail_html = detail_pages_html.get(url)\n            if detail_html:\n                runners = self._parse_runners_from_detail_page(detail_html)\n                race.runners = runners\n            final_races.append(race)\n\n        return final_races\n\n    def _parse_race_schedule(self, html_content: str) -> (List[Race], List[str]):\n        \"\"\"Parses the main entries page to get race info and detail page URLs.\"\"\"\n        if not html_content:\n            return [], []\n\n        soup = BeautifulSoup(html_content, 'html.parser')\n        partial_races = []\n        detail_urls = []\n\n        track_tables = soup.find_all('table', summary=lambda s: s and s.startswith('Track Abbr:'))\n        for track_table in track_tables:\n            try:\n                track_name = track_table.find('tr').find('strong').text.strip()\n                for race_row in track_table.find_all('tr', class_='entry'):\n                    links = race_row.find_all('a')\n                    if len(links) < 2:\n                        continue\n\n                    race_number = int(''.join(filter(str.isdigit, links[0].text.strip())))\n                    race_id = f\"{self.SOURCE_ID}_{track_name.replace(' ', '')}_{date.today().strftime('%Y%m%d')}_R{race_number}\"\n                    detail_url = links[1]['href']\n\n                    partial_races.append(Race(\n                        race_id=race_id, track_name=track_name, race_number=race_number, runners=[]\n                    ))\n                    detail_urls.append(detail_url)\n            except Exception as e:\n                logging.warning(f\"EquibaseV7: Skipping a malformed schedule table/row: {e}\")\n                continue\n        return partial_races, detail_urls\n\n    def _parse_runners_from_detail_page(self, html_content: str) -> List[Runner]:\n        \"\"\"Parses the race detail page to get a list of runners.\"\"\"\n        if not html_content:\n            return []\n\n        soup = BeautifulSoup(html_content, 'html.parser')\n        runners = []\n        # Assumption: Runners are in a table with class 'entries'\n        runner_rows = soup.select('table.entries tr.entry')\n        for row in runner_rows:\n            try:\n                # Assumption: Program number is in a <strong> tag\n                program_number_tag = row.find('strong')\n                program_number = int(program_number_tag.text.strip()) if program_number_tag else None\n\n                # Assumption: Runner name is in a <td> with class 'horse'\n                runner_name_tag = row.find('td', class_='horse')\n                runner_name = runner_name_tag.text.strip() if runner_name_tag else 'Unknown Runner'\n\n                if program_number and runner_name:\n                    runners.append(Runner(name=runner_name, program_number=program_number))\n            except Exception as e:\n                logging.warning(f\"EquibaseV7: Skipping a malformed runner row: {e}\")\n                continue\n        return runners\n\n\nclass FanDuelGraphQLAdapterV7(BaseAdapterV7):\n    \"\"\"Adapter for the FanDuel GraphQL API.\"\"\"\n    SOURCE_ID = \"fanduel\"\n    API_URL = \"https://sb-prod-df.sportsbook.fanduel.com/api/v2/horse-racing/races\"\n\n    async def fetch_races(self) -> List[Race]:\n        \"\"\"Fetches data from the FanDuel GraphQL API.\"\"\"\n        graphql_query = {\n            \"query\": \"\"\"\n                query AllRaces($first: Int!, $next: String) {\n                    allRaces(first: $first, after: $next) {\n                        edges { node { trackName raceNumber postTime runners { runnerName odds scratched } } }\n                    }\n                }\n            \"\"\",\n            \"variables\": {\"first\": 100}\n        }\n        try:\n            raw_data = await self.fetcher.post(self.API_URL, json_data=graphql_query)\n            if not raw_data:\n                return []\n            return self._parse_races(raw_data)\n        except Exception as e:\n            logging.error(f\"{self.SOURCE_ID}: Failed to fetch or parse races: {e}\")\n            return []\n\n    def _parse_races(self, raw_data: dict) -> List[Race]:\n        \"\"\"Parses the JSON response from the GraphQL API.\"\"\"\n        races = []\n        race_edges = raw_data.get(\"data\", {}).get(\"allRaces\", {}).get(\"edges\", [])\n        if not race_edges:\n            return []\n\n        for edge in race_edges:\n            node = edge.get(\"node\", {})\n            if not node:\n                continue\n\n            runners = []\n            for runner_data in node.get(\"runners\", []):\n                if runner_data.get('scratched'):\n                    continue\n                runners.append(Runner(\n                    name=runner_data.get(\"runnerName\"),\n                    odds=self._to_float_odds(runner_data.get(\"odds\")),\n                    program_number=None  # Not provided in API\n                ))\n\n            post_time = self._to_datetime(node.get(\"postTime\"))\n            track_name = node.get('trackName')\n            race_number = node.get('raceNumber')\n\n            races.append(Race(\n                race_id=f\"{self.SOURCE_ID}_{track_name}_{race_number}\",\n                track_name=track_name,\n                race_number=race_number,\n                post_time=post_time,\n                runners=runners\n            ))\n        return races\n\n    def _to_float_odds(self, odds_str: Optional[str]) -> Optional[float]:\n        if not odds_str or \"/\" not in odds_str:\n            return None\n        try:\n            num, den = map(int, odds_str.split('/'))\n            return (num / den) + 1.0\n        except (ValueError, ZeroDivisionError):\n            return None\n\n    def _to_datetime(self, timestamp_str: Optional[str]) -> Optional[datetime]:\n        if not timestamp_str:\n            return None\n        try:\n            return datetime.fromisoformat(timestamp_str.replace(\"Z\", \"+00:00\"))\n        except (ValueError, TypeError):\n            return None\n\n\nclass BetfairDataScientistAdapterV7(BaseAdapterV7):\n    \"\"\"Adapter for the Betfair Data Scientist API (CSV format).\"\"\"\n    SOURCE_ID = \"betfair_data_scientist\"\n    BASE_URL = \"https://betfair-data-supplier.herokuapp.com/api/widgets/iggy-joey/datasets\"\n\n    async def fetch_races(self) -> List[Race]:\n        \"\"\"Fetches and parses data from the Betfair Data Scientist API.\"\"\"\n        today = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n        url = f\"{self.BASE_URL}/?date={today}&presenter=RatingsPresenter&csv=true\"\n\n        csv_data = await self.fetcher.fetch(url, response_type='text')\n        if not csv_data:\n            return []\n        return self._parse_races(csv_data)\n\n    def _parse_races(self, csv_content: str) -> List[Race]:\n        \"\"\"Parses the CSV content from the API into a list of Race objects.\"\"\"\n        if not csv_content:\n            return []\n\n        try:\n            data = StringIO(csv_content)\n            df = pd.read_csv(data, dtype={\"selection_id\": str})\n            df.rename(columns={\"meetings.races.runners.ratedPrice\": \"rating\"}, inplace=True)\n            df = df[[\"market_id\", \"selection_id\", \"rating\"]]\n\n            races = {}\n            for _, row in df.iterrows():\n                race_id = str(row[\"market_id\"])\n                if race_id not in races:\n                    races[race_id] = Race(\n                        race_id=race_id,\n                        track_name=\"Unknown\", # Not in this API response\n                        race_number=None,\n                        runners=[],\n                    )\n\n                runner = Runner(\n                    name=str(row[\"selection_id\"]), # Using selection_id as name\n                    program_number=None, # Not in this API response\n                    odds=row[\"rating\"],\n                )\n                races[race_id].runners.append(runner)\n\n            return list(races.values())\n        except Exception as e:\n            logging.error(f\"{self.SOURCE_ID}: Failed to parse CSV data: {e}\")\n            return []\n\n\nclass TwinspiresAdapterV7(BaseAdapterV7):\n    \"\"\"Adapter for the TwinSpires website (two-stage scrape).\"\"\"\n    SOURCE_ID = \"twinspires\"\n    BASE_URL = \"https://www.twinspires.com\"\n\n    async def fetch_races(self) -> List[Race]:\n        \"\"\"Fetches all race data using a two-stage process.\"\"\"\n        index_url = f\"{self.BASE_URL}/adw/todays-tracks?sortOrder=nextUp\"\n        index_html = await self.fetcher.fetch(index_url, response_type='text')\n        if not index_html:\n            return []\n\n        detail_links = self._parse_race_links(index_html)\n        if not detail_links:\n            return []\n\n        detail_pages_html = []\n        async def fetch_and_store(link):\n            html = await self.fetcher.fetch(link, response_type='text')\n            if html:\n                detail_pages_html.append(html)\n\n        async with anyio.create_task_group() as tg:\n            for link in detail_links:\n                tg.start_soon(fetch_and_store, link)\n\n        all_races = []\n        for html in detail_pages_html:\n            if html:\n                try:\n                    race = self._parse_single_race_detail(html)\n                    if race:\n                        all_races.append(race)\n                except Exception as e:\n                    logging.warning(f\"{self.SOURCE_ID}: Skipping a malformed race detail page: {e}\")\n                    continue\n        return all_races\n\n    def _parse_race_links(self, html_content: str) -> List[str]:\n        \"\"\"Parses the index page to find links to all race detail pages.\"\"\"\n        soup = BeautifulSoup(html_content, 'html.parser')\n        race_links = []\n        for link in soup.find_all('a', href=lambda h: h and '/races/' in h and '/results/' not in h):\n            race_links.append(self.BASE_URL + link['href'])\n        return list(set(race_links)) # Use set to remove duplicate links\n\n    def _parse_single_race_detail(self, html: str) -> Optional[Race]:\n        \"\"\"Parses a single race detail HTML page.\"\"\"\n        soup = BeautifulSoup(html, 'html.parser')\n\n        header = soup.find('div', class_='race-title')\n        if not header: return None\n\n        track_name = header.find('a').text.strip()\n        race_number_text = header.find('strong').text\n        race_number = int(''.join(filter(str.isdigit, race_number_text)))\n\n        runners = []\n        program = soup.find('div', id='program')\n        if not program: return None\n\n        for i, runner_row in enumerate(program.find_all('div', class_='runner-wrapper')):\n            name_tag = runner_row.find('div', class_='runner-name')\n            if not name_tag: continue\n            name = name_tag.text.strip()\n\n            odds_span = runner_row.find('span', class_='odds')\n            odds_str = odds_span.text.strip() if odds_span else None\n\n            if not odds_str: continue\n\n            try:\n                if '/' in odds_str:\n                    num, den = map(int, odds_str.split('/'))\n                    odds = (num / den) + 1.0 if den != 0 else None\n                else:\n                    odds = float(odds_str) + 1.0\n\n                if odds is not None:\n                    runners.append(Runner(\n                        name=name,\n                        odds=odds,\n                        program_number=i + 1\n                    ))\n            except (ValueError, TypeError):\n                continue\n\n        if not runners:\n            return None\n\n        return Race(\n            race_id=f\"{self.SOURCE_ID}_{track_name.replace(' ', '')}_{race_number}\",\n            track_name=track_name,\n            race_number=race_number,\n            runners=runners\n        )\n"
}