{
    "filepath": "./src/checkmate_v7/api.py",
    "content": "\"\"\"\nCheckmate V7: `api.py` - THE CONDUCTOR\n\"\"\"\nfrom fastapi import FastAPI, BackgroundTasks, HTTPException\nfrom . import services\nimport numpy as np\nfrom scipy.stats import wilcoxon\n\ndef percentile_bootstrap_ci(data, n_bootstrap=1000, ci_level=0.95):\n    \"\"\"Calculate percentile bootstrap confidence interval for the mean.\"\"\"\n    if len(data) < 2:\n        return (np.nan, np.nan)\n    bootstrap_means = []\n    for _ in range(n_bootstrap):\n        bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n        bootstrap_means.append(np.mean(bootstrap_sample))\n\n    lower_bound = np.percentile(bootstrap_means, (1 - ci_level) / 2 * 100)\n    upper_bound = np.percentile(bootstrap_means, (1 + ci_level) / 2 * 100)\n    return (lower_bound, upper_bound)\n\ndef wilcoxon_p_value(data):\n    \"\"\"Perform a one-sample Wilcoxon signed-rank test.\"\"\"\n    if len(data) < 10: # Not enough data for a meaningful test\n        return np.nan\n    # Test if the mean of the data is significantly different from zero\n    statistic, p_value = wilcoxon(data)\n    return p_value\n\nfrom . import logging_config\n\napp = FastAPI()\n\n@app.on_event(\"startup\")\ndef on_startup():\n    \"\"\"\n    Configures logging and overrides Uvicorn's default loggers\n    to use the new structured JSON format on application startup.\n    \"\"\"\n    logging_config.setup_logging()\n\n    # Reconfigure Uvicorn's loggers to use our new handler\n    # This ensures that access logs and server errors are also in JSON format\n    loggers_to_override = [\"uvicorn\", \"uvicorn.error\", \"uvicorn.access\"]\n    root_handlers = logging.getLogger().handlers\n    \n    for logger_name in loggers_to_override:\n        uvicorn_logger = logging.getLogger(logger_name)\n        uvicorn_logger.handlers = root_handlers\n        uvicorn_logger.propagate = False\n\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"Checkmate V7 API is running.\"}\n\n@app.post(\"/races/process\", status_code=202)\ndef process_race(race_url: str, background_tasks: BackgroundTasks):\n    \"\"\"Dispatches a background job to process a race.\"\"\"\n    background_tasks.add_task(services.process_race_for_prediction, race_url)\n    return {\"message\": \"Race processing job accepted.\"}\n\nfrom .models import PerformanceMetricsSchema, JoinORM, PredictionORM, PredictionSchema\nfrom .services import get_db_session\n\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom sqlalchemy import text\nimport logging\nfrom typing import List\nfrom datetime import datetime, timezone\n\n@app.get(\"/predictions/active\", response_model=List[PredictionSchema])\ndef get_active_predictions():\n    \"\"\"Returns all predictions with a 'pending' status.\"\"\"\n    session = get_db_session()\n    try:\n        pending_preds = session.query(PredictionORM).filter(PredictionORM.status == 'pending').all()\n        \n        results = []\n        for pred in pending_preds:\n            pred_schema = PredictionSchema.from_orm(pred)\n            if pred.race_local_datetime:\n                # Assume race_local_datetime is a naive datetime representing UTC\n                post_time_utc = pred.race_local_datetime.replace(tzinfo=timezone.utc)\n                now_utc = datetime.now(timezone.utc)\n                minutes_to_post = (post_time_utc - now_utc).total_seconds() / 60\n                pred_schema.minutes_to_post = minutes_to_post\n            results.append(pred_schema)\n            \n        return results\n    except SQLAlchemyError as e:\n        logging.error(\"Database error while fetching active predictions\", extra={\"error\": str(e)})\n        raise HTTPException(status_code=500, detail=\"Database error\")\n    except Exception as e:\n        logging.error(\"An unexpected error occurred while fetching active predictions\", extra={\"error\": str(e)})\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n    finally:\n        if session:\n            session.close()\n\n@app.get(\"/performance\", response_model=PerformanceMetricsSchema)\ndef get_performance():\n    \"\"\"Returns a statistically rigorous performance report.\"\"\"\n    session = get_db_session()\n    try:\n        joins = session.query(JoinORM).filter_by(audit_status='completed').all()\n\n        if not joins or len(joins) < 2:\n            return PerformanceMetricsSchema(\n                total_bets=len(joins) if joins else 0, win_rate=0, roi_percent=0, net_profit=0,\n                confidence_interval=[0, 0], p_value=1.0, sample_size=len(joins) if joins else 0\n            )\n\n        pnl_data = [j.pnl_native for j in joins]\n        roi_data = [j.roi for j in joins]\n\n        total_bets = len(joins)\n        total_wins = sum(1 for pnl in pnl_data if pnl > 0)\n        win_rate = (total_wins / total_bets) * 100\n        net_profit = sum(pnl_data)\n        total_staked = sum([j.stake_used for j in joins])\n        roi_percent = (net_profit / total_staked) * 100 if total_staked > 0 else 0\n\n        ci = percentile_bootstrap_ci(roi_data)\n        p_val = wilcoxon_p_value(pnl_data)\n\n        return PerformanceMetricsSchema(\n            total_bets=total_bets,\n            win_rate=win_rate,\n            roi_percent=roi_percent,\n            net_profit=net_profit,\n            confidence_interval=[ci[0], ci[1]],\n            p_value=p_val if not np.isnan(p_val) else None,\n            sample_size=total_bets\n        )\n    except SQLAlchemyError as e:\n        logging.error(\"Database error while fetching performance metrics\", extra={\"error\": str(e)})\n        raise HTTPException(status_code=500, detail=\"Database error\")\n    except Exception as e:\n        logging.error(\"An unexpected error occurred while fetching performance metrics\", extra={\"error\": str(e)})\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n    finally:\n        if session:\n            session.close()\n\nfrom .models import HealthCheckResponse\nimport redis\n\nfrom . import config\n\n@app.get(\"/health\", response_model=HealthCheckResponse)\ndef get_health():\n    \"\"\"Returns the health status of the application's critical services.\"\"\"\n\n    # Check database health\n    db_status = \"ok\"\n    try:\n        session = get_db_session()\n        session.execute(text(\"SELECT 1\"))\n        session.close()\n    except Exception as e:\n        db_status = f\"error: {e}\"\n\n    # Check Celery (Redis) health\n    celery_status = \"ok\"\n    try:\n        r = redis.Redis.from_url(config.REDIS_URL, socket_connect_timeout=1)\n        r.ping()\n    except Exception as e:\n        celery_status = f\"error: {e}\"\n\n    return HealthCheckResponse(\n        status=\"ok\" if db_status == \"ok\" and celery_status == \"ok\" else \"error\",\n        database=db_status,\n        celery=celery_status\n    )\n"
}