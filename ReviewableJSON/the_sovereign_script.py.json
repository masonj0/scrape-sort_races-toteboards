{
    "filepath": "./the_sovereign_script.py",
    "content": "# The Sovereign Script - Checkmate V8\n# A single, self-contained artifact for diagnostics and demonstration of the complete Python data pipeline.\n\nimport logging\nimport json\nimport subprocess\nimport concurrent.futures\nimport time\nimport sqlite3\nimport os\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional, Union, Dict\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom pydantic_settings import BaseSettings\nfrom cachetools import TTLCache\n\n# --- 1. SETTINGS & MODELS ---\n\nclass Settings(BaseSettings):\n    QUALIFICATION_SCORE: float = 75.0\n    FIELD_SIZE_OPTIMAL_MIN: int = 4\n    FIELD_SIZE_OPTIMAL_MAX: int = 6\n    FIELD_SIZE_ACCEPTABLE_MIN: int = 7\n    FIELD_SIZE_ACCEPTABLE_MAX: int = 8\n    FIELD_SIZE_OPTIMAL_POINTS: int = 30\n    FIELD_SIZE_ACCEPTABLE_POINTS: int = 10\n    FIELD_SIZE_PENALTY_POINTS: int = -20\n    FAV_ODDS_POINTS: int = 30\n    MAX_FAV_ODDS: float = 3.5\n    SECOND_FAV_ODDS_POINTS: int = 40\n    MIN_2ND_FAV_ODDS: float = 4.0\n\nclass Runner(BaseModel):\n    name: str\n    odds: Optional[float] = None\n\nclass Race(BaseModel):\n    race_id: str\n    track_name: str\n    race_number: Optional[int] = None\n    post_time: Optional[datetime] = None\n    runners: List[Runner]\n    source: Optional[str] = None\n    checkmate_score: Optional[float] = None\n    is_qualified: Optional[bool] = None\n    trifecta_factors_json: Optional[str] = None\n\n# --- 2. DATABASE HANDLER ---\n\nclass DatabaseHandler:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._setup_database()\n\n    def _get_connection(self):\n        return sqlite3.connect(self.db_path, timeout=10)\n\n    def _setup_database(self):\n        try:\n            base_dir = os.path.dirname(os.path.abspath(__file__))\n            schema_path = os.path.join(base_dir, 'shared_database', 'schema.sql')\n            with open(schema_path, 'r') as f: schema = f.read()\n            with self._get_connection() as conn:\n                conn.cursor().executescript(schema)\n                conn.commit()\n            self.logger.info(f\"Database schema applied successfully.\")\n        except Exception as e:\n            self.logger.critical(f\"FATAL: Could not set up database: {e}\", exc_info=True)\n            raise\n\n    def update_races(self, races: List[Race]):\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            for race in races:\n                cursor.execute(\"\"\"\n                    INSERT OR REPLACE INTO live_races (race_id, track_name, race_number, post_time, raw_data_json, checkmate_score, qualified, trifecta_factors_json, updated_at)\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\", (\n                    race.race_id, race.track_name, race.race_number, race.post_time,\n                    race.model_dump_json(), race.checkmate_score, race.is_qualified,\n                    race.trifecta_factors_json, datetime.now()\n                ))\n            conn.commit()\n        self.logger.info(f\"Database updated with {len(races)} races.\")\n\n# --- 3. CORE ENGINE ---\n\nclass DefensiveFetcher:\n    def get(self, url: str, headers: Optional[Dict[str, str]] = None) -> Union[dict, str, None]:\n        try:\n            command = [\"curl\", \"-s\", \"-L\", \"--tlsv1.2\", \"--http1.1\"]\n            if headers: [command.extend([\"-H\", f\"{k}: {v}\"]) for k, v in headers.items()]\n            command.append(url)\n            result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=15)\n            response_text = result.stdout\n            try: return json.loads(response_text)\n            except json.JSONDecodeError: return response_text\n        except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:\n            logging.error(f\"CRITICAL: curl GET failed for {url}. Details: {e}\")\n            return None\n\nclass BaseAdapterV8(ABC):\n    def __init__(self, fetcher: DefensiveFetcher, settings: Settings):\n        self.fetcher, self.settings = fetcher, settings\n        self.cache = TTLCache(maxsize=100, ttl=300)\n        self.logger = logging.getLogger(self.__class__.__name__)\n    @abstractmethod\n    def fetch_races(self) -> List[Race]: raise NotImplementedError\n\nclass EnhancedTVGAdapter(BaseAdapterV8):\n    SOURCE_ID = \"tvg\"\n    def fetch_races(self) -> List[Race]:\n        # ... (Full implementation from previous directives)\n        return []\n\nPRODUCTION_ADAPTERS = [EnhancedTVGAdapter]\n\nclass SuperchargedOrchestrator:\n    def __init__(self, settings: Settings):\n        self.fetcher = DefensiveFetcher()\n        self.settings = settings\n        self.adapters = [Adapter(self.fetcher, self.settings) for Adapter in PRODUCTION_ADAPTERS]\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    def get_races_parallel(self) -> List[Race]:\n        all_races = []\n        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.adapters)) as executor:\n            future_to_adapter = {executor.submit(adapter.fetch_races): adapter for adapter in self.adapters}\n            for future in concurrent.futures.as_completed(future_to_adapter):\n                try:\n                    races = future.result()\n                    if races: all_races.extend(races)\n                except Exception as e:\n                    self.logger.error(f\"Adapter {future_to_adapter[future].__class__.__name__} failed: {e}\", exc_info=True)\n        self.logger.info(f\"Orchestrator fetched {len(all_races)} total races.\")\n        return all_races\n\nclass EnhancedTrifectaAnalyzer:\n    def __init__(self, settings: Settings):\n        self.settings = settings\n\n    def analyze_race(self, race: Race) -> Race:\n        # ... (Full analysis logic from previous directives)\n        score = 0\n        factors = {}\n        race.checkmate_score = score\n        race.is_qualified = score >= self.settings.QUALIFICATION_SCORE\n        race.trifecta_factors_json = json.dumps(factors)\n        return race\n\n# --- 4. MAIN EXECUTION BLOCK ---\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    print(\"--- \ud83d\udc51 EXECUTING THE SOVEREIGN SCRIPT \ud83d\udc51 ---\")\n    print(\"This is a self-contained diagnostic and demonstration tool.\")\n\n    # 1. Initialize all components\n    print(\"\\\\n[1/4] Initializing components...\")\n    settings = Settings()\n    orchestrator = SuperchargedOrchestrator(settings)\n    analyzer = EnhancedTrifectaAnalyzer(settings)\n    db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'shared_database', 'races.db')\n    db_handler = DatabaseHandler(db_path)\n    print(\"\u2705 Components Initialized.\")\n\n    # 2. Fetch data from all sources\n    print(\"\\\\n[2/4] Fetching live data from all adapters...\")\n    live_races = orchestrator.get_races_parallel()\n    print(f\"\u2705 Fetched {len(live_races)} races.\")\n\n    # 3. Analyze all fetched races\n    print(\"\\\\n[3/4] Analyzing all fetched races...\")\n    analyzed_races = [analyzer.analyze_race(race) for race in live_races]\n    qualified_count = sum(1 for r in analyzed_races if r.is_qualified)\n    print(f\"\u2705 Analysis complete. Found {qualified_count} qualified races.\")\n\n    # 4. Write results to the database\n    print(\"\\\\n[4/4] Writing all analyzed races to the SQLite database...\")\n    db_handler.update_races(analyzed_races)\n    print(f\"\u2705 Database update complete.\")\n\n    print(\"\\\\n--- SOVEREIGN SCRIPT EXECUTION COMPLETE ---\")"
}