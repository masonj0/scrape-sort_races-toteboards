{
    "file_path": "create_fortuna_json.py",
    "content": "# create_fortuna_json.py\n# This script now dynamically reads the manifests and creates three separate, categorized JSON packages.\n\nimport json\nimport os\nimport re\nimport sys\n\n# --- Configuration ---\nMANIFEST_FILES = ['MANIFEST.md', 'MANIFEST2.md', 'MANIFEST3.md']\nOUTPUT_FILE_PART1 = 'FORTUNA_ALL_PART1.JSON' # Core Backend\nOUTPUT_FILE_PART2 = 'FORTUNA_ALL_PART2.JSON' # Adapter Fleet\nOUTPUT_FILE_PART3 = 'FORTUNA_ALL_PART3.JSON' # Frontend, Docs, Tests & Tooling\n\n# --- ENLIGHTENED PARSING LOGIC ---\ndef extract_and_normalize_path(line: str) -> str | None:\n    \"\"\"\n    Extracts a file path from a line, handling multiple formats, and normalizes it.\n    Handles:\n    - Markdown links: `* [display](path)`\n    - Plain paths in backticks: ``- `path.py` - description``\n    - Plain paths with list markers: `- path/to/file.py`\n    \"\"\"\n    line = line.strip()\n    if not line or line.startswith('#'):\n        return None\n\n    # 1. Check for Markdown link format\n    md_match = re.search(r'\\[.*\\]\\((https?://[^\\)]+)\\)', line)\n    if not md_match:\n        md_match = re.search(r'\\[.*\\]\\(([^)]+)\\)', line)\n\n    if md_match:\n        path = md_match.group(1)\n    else:\n        # 2. Check for paths in backticks\n        bt_match = re.search(r'`([^`]+)`', line)\n        if bt_match:\n            path = bt_match.group(1)\n        else:\n            # 3. Assume plain path, stripping list markers\n            path = re.sub(r'^[*-]\\s*', '', line).split(' ')[0]\n\n    # --- Path Standardization ---\n    if not path or not ('.' in path or '/' in path):\n        if not path.endswith('.md'):\n             return None # Not a valid path\n\n    # If it's a full raw GitHub URL, extract the local path\n    if path.startswith('https://raw.githubusercontent.com/'):\n        path = '/'.join(path.split('/main/')[1:])\n\n    # Final check to avoid capturing descriptions\n    if ' ' in path and not path.startswith('`'):\n        return None\n\n    return path.strip()\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'='*60}\\nStarting FORTUNA Triumvirate Dossier creation...\\n{'='*60}\")\n\n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        if not os.path.exists(manifest):\n            print(f\"    [WARNING] Manifest not found: {manifest}\")\n            continue\n\n        with open(manifest, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n\n        paths_found = 0\n        for line in lines:\n            path = extract_and_normalize_path(line)\n            if path:\n                all_local_paths.append(path)\n                paths_found += 1\n        print(f\"    --> Found {paths_found} valid file paths.\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    part1_data = {} # Core Backend\n    part2_data = {} # Adapter Fleet\n    part3_data = {} # Frontend, Docs, Tests, Tooling\n    failed_count = 0\n    unique_local_paths = sorted(list(set(all_local_paths)))\n\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to categorize and process.\")\n\n    for local_path in unique_local_paths:\n        try:\n            print(f\"--> Processing: {local_path}\")\n\n            if not os.path.exists(local_path):\n                print(f\"    [ERROR] File not found on disk: {local_path}\")\n                failed_count += 1\n                continue\n\n            with open(local_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()\n\n            # --- Categorization Logic (Triumvirate) ---\n            if local_path.startswith('python_service/adapters/'):\n                part2_data[local_path] = content\n            elif local_path.startswith('python_service/'):\n                part1_data[local_path] = content\n            else:\n                part3_data[local_path] = content\n\n        except Exception as e:\n            print(f\"    [ERROR] Failed to read {local_path}: {e}\")\n            failed_count += 1\n\n    # --- Write Part 1 ---\n    print(f\"\\nWriting {len(part1_data)} files to {OUTPUT_FILE_PART1}...\")\n    with open(OUTPUT_FILE_PART1, 'w', encoding='utf-8') as f:\n        json.dump(part1_data, f, indent=4)\n    print(f\"    [SUCCESS] {OUTPUT_FILE_PART1} created.\")\n\n    # --- Write Part 2 ---\n    print(f\"Writing {len(part2_data)} files to {OUTPUT_FILE_PART2}...\")\n    with open(OUTPUT_FILE_PART2, 'w', encoding='utf-8') as f:\n        json.dump(part2_data, f, indent=4)\n    print(f\"    [SUCCESS] {OUTPUT_FILE_PART2} created.\")\n\n    # --- Write Part 3 ---\n    print(f\"Writing {len(part3_data)} files to {OUTPUT_FILE_PART3}...\")\n    with open(OUTPUT_FILE_PART3, 'w', encoding='utf-8') as f:\n        json.dump(part3_data, f, indent=4)\n    print(f\"    [SUCCESS] {OUTPUT_FILE_PART3} created.\")\n\n    total_processed = len(part1_data) + len(part2_data) + len(part3_data)\n    print(f\"\\n{'='*60}\\nPackaging process complete.\\nSuccessfully processed: {total_processed}/{len(unique_local_paths)}\\nFailed/Skipped: {failed_count}\\n{'='*60}\")\n\n    if failed_count > 0:\n        print(\"\\n[WARNING] Some files failed to process. The output may be incomplete.\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()"
}