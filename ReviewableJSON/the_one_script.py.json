{
    "filepath": "./the_one_script.py",
    "content": "#!/usr/bin/env python3\n\"\"\"\n# The One Script V8 (Perfected): The Complete Checkmate Racing Analysis System\n# A single, self-contained application incorporating all features from the tri-hybrid architecture.\n\n# VERSION 8.1 - THE 10/10 MANDATE\n# - Corrected TrifectaAnalyzer logic.\n# - Implemented non-blocking Streamlit refresh.\n# - Hardened DefensiveFetcher with persistent client and non-JSON handling.\n# - Implemented robust database safety with explicit columns and date handling.\n# - Corrected concurrency hygiene for the background service.\n# - Implemented robust data deduplication logic.\n# - Added production-grade logging and settings validation.\n\nSetup:\n1. pip install -r requirements.txt\n2. Create .env with: RACING_API_KEY=\"your_key\" (optional)\n3. Run: streamlit run the_one_script.py\n\"\"\"\n\nimport asyncio\nimport streamlit as st\nimport time\nimport os\nimport json\nimport sqlite3\nimport logging\nimport threading\nfrom datetime import datetime, date, timedelta\nfrom typing import List, Optional, Dict, Any\nfrom abc import ABC, abstractmethod\nimport httpx\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom pydantic_settings import BaseSettings\nimport pandas as pd\nimport tempfile\nimport subprocess\nfrom tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n\n\n# --- 1. LOGGING & SETTINGS ---\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)s | %(threadName)s | %(name)s | %(message)s\",\n)\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\nlogging.info(\"Checkmate V8 (Perfected) starting up\")\n\nclass Settings(BaseSettings):\n    QUALIFICATION_SCORE: float = Field(default=75.0)\n    FIELD_SIZE_OPTIMAL_MIN: int = Field(default=4)\n    FIELD_SIZE_OPTIMAL_MAX: int = Field(default=6)\n    FIELD_SIZE_ACCEPTABLE_MIN: int = Field(default=7)\n    FIELD_SIZE_ACCEPTABLE_MAX: int = Field(default=8)\n    FIELD_SIZE_OPTIMAL_POINTS: int = Field(default=30)\n    FIELD_SIZE_ACCEPTABLE_POINTS: int = Field(default=10)\n    FIELD_SIZE_PENALTY_POINTS: int = Field(default=-20)\n    FAV_ODDS_POINTS: int = Field(default=30)\n    MAX_FAV_ODDS: float = Field(default=3.5)\n    SECOND_FAV_ODDS_POINTS: int = Field(default=40)\n    MIN_2ND_FAV_ODDS: float = Field(default=4.0)\n    RACING_API_KEY: str = Field(default=os.getenv(\"RACING_API_KEY\", \"\"))\n\n# --- 2. DATA MODELS ---\nclass Runner(BaseModel):\n    name: str\n    odds: Optional[float] = None\n\nclass Race(BaseModel):\n    race_id: str\n    track_name: str\n    race_number: Optional[int] = None\n    post_time: Optional[datetime] = None\n    runners: List[Runner]\n    source: Optional[str] = None\n    checkmate_score: Optional[float] = None\n    is_qualified: Optional[bool] = None\n    trifecta_factors_json: Optional[str] = None\n\n# --- 3. DEFENSIVE HTTP CLIENT ---\nclass DefensiveFetcher:\n    def __init__(self):\n        self.failure_counts: Dict[str, int] = {}\n        self.last_failure_time: Dict[str, float] = {}\n        self.circuit_breaker_threshold = 3\n        self.circuit_breaker_timeout = 300\n        self._client = httpx.AsyncClient(timeout=httpx.Timeout(15.0, connect=5.0), headers={\"User-Agent\": \"CheckmateV8/1.0\"})\n\n    async def aclose(self):\n        try: await self._client.aclose()\n        except Exception: pass\n\n    def is_circuit_breaker_open(self, domain: str) -> bool:\n        if self.failure_counts.get(domain, 0) < self.circuit_breaker_threshold: return False\n        if time.time() - self.last_failure_time.get(domain, 0) < self.circuit_breaker_timeout:\n            return True\n        self.failure_counts[domain] = 0\n        return False\n\n    def record_success(self, domain: str):\n        self.failure_counts[domain] = 0\n\n    def record_failure(self, domain: str):\n        self.failure_counts[domain] = self.failure_counts.get(domain, 0) + 1\n        self.last_failure_time[domain] = time.time()\n\n    @staticmethod\n    def get_domain(url: str) -> str:\n        from urllib.parse import urlparse\n        return urlparse(url).netloc\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=2, max=10),\n        reraise=True\n    )\n    async def _make_request(self, url: str, headers: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n        \"\"\"A single, decorated attempt to make an async network request.\"\"\"\n        logging.info(f\"Attempting request for {url}...\")\n        response = await self._client.get(url, headers=headers or {})\n        response.raise_for_status()\n        return response.json()\n\n    async def get(self, url: str, headers: Optional[Dict[str, str]] = None) -> Optional[Dict[str, Any]]:\n        domain = self.get_domain(url)\n        if self.is_circuit_breaker_open(domain):\n            logging.warning(f\"Circuit breaker is open for {domain}. Skipping request.\")\n            return None\n\n        try:\n            response = await self._make_request(url, headers=headers)\n            self.record_success(domain)\n            return response\n        except RetryError as e:\n            logging.critical(f\"All retry attempts failed for {url}. Last error: {e}\")\n            self.record_failure(domain)\n            return None\n        except Exception as e:\n            logging.critical(f\"A non-retryable error occurred for {url}: {e}\")\n            self.record_failure(domain)\n            return None\n\n# --- 4. DATA SOURCE ADAPTERS ---\nclass BaseAdapter(ABC):\n    def __init__(self, fetcher: DefensiveFetcher): self.fetcher = fetcher\n    @abstractmethod\n    async def fetch_races(self) -> List[Race]: raise NotImplementedError\n\nclass TVGAdapter(BaseAdapter):\n    SOURCE_ID, BASE_URL = \"tvg\", \"https://mobile-api.tvg.com/api/mobile/races/today\"\n\n    @staticmethod\n    def _parse_decimal_odds(ml: Optional[str]) -> Optional[float]:\n        if not ml: return None\n        try: n, d = map(int, ml.strip().split(\"/\")); return (n / d) + 1.0\n        except: return None\n\n    async def fetch_races(self) -> List[Race]:\n        data = await self.fetcher.get(self.BASE_URL)\n        if not data or 'races' not in data: return []\n        races: List[Race] = []\n        for r_info in data.get('races', []):\n            try:\n                runners: List[Runner] = []\n                for r in r_info.get('runners', []):\n                    if r.get('scratched'): continue\n                    odds = self._parse_decimal_odds((r.get('odds') or {}).get('morningLine'))\n                    if odds: runners.append(Runner(name=r.get('horseName') or \"N/A\", odds=odds))\n                if len(runners) < 3: continue\n                post_dt = datetime.fromisoformat(r_info['postTime'].replace(\"Z\", \"+00:00\")) if r_info.get('postTime') else None\n                races.append(Race(race_id=f\"tvg_{r_info.get('raceId')}\", track_name=r_info.get('trackName') or 'N/A', race_number=r_info.get('raceNumber'), post_time=post_dt, runners=runners, source=self.SOURCE_ID))\n            except Exception as e: logging.warning(f\"Skipping malformed TVG race: {e}\")\n        return races\n\n# --- 5. DATA ORCHESTRATOR ---\nclass DataSourceOrchestrator:\n    def __init__(self, settings: Settings):\n        self.fetcher = DefensiveFetcher()\n        self.adapters = [TVGAdapter(self.fetcher)]\n\n    async def get_races(self) -> List[Race]:\n        tasks = [adapter.fetch_races() for adapter in self.adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        all_races: List[Race] = []\n        for i, res in enumerate(results):\n            if isinstance(res, Exception): logging.error(f\"Adapter {self.adapters[i].__class__.__name__} failed: {res}\")\n            elif isinstance(res, list): all_races.extend(res)\n        dedup: Dict[str, Race] = {}\n        for r in all_races: dedup[r.race_id] = r\n        return list(dedup.values())\n\n# --- 6. TRIFECTA ANALYSIS ENGINE ---\nclass TrifectaAnalyzer:\n    def analyze_race(self, race: Race, settings: Settings) -> Race:\n        score, factors = 0.0, {}\n        horses = sorted([r for r in race.runners if r.odds], key=lambda h: h.odds)\n        num_runners = len(horses)\n        if settings.FIELD_SIZE_OPTIMAL_MIN <= num_runners <= settings.FIELD_SIZE_OPTIMAL_MAX: p, ok, r = settings.FIELD_SIZE_OPTIMAL_POINTS, True, f\"Optimal field size ({num_runners})\"\n        elif settings.FIELD_SIZE_ACCEPTABLE_MIN <= num_runners <= settings.FIELD_SIZE_ACCEPTABLE_MAX: p, ok, r = settings.FIELD_SIZE_ACCEPTABLE_POINTS, True, f\"Acceptable field size ({num_runners})\"\n        else: p, ok, r = settings.FIELD_SIZE_PENALTY_POINTS, False, f\"Field size not ideal ({num_runners})\"\n        score += p; factors[\"fieldSize\"] = {\"points\": p, \"ok\": ok, \"reason\": r}\n        if num_runners >= 2:\n            fav, sec_fav = horses[0], horses[1]\n            if fav.odds <= settings.MAX_FAV_ODDS: p, ok, r = settings.FAV_ODDS_POINTS, True, f\"Favorite odds OK ({fav.odds:.2f})\"\n            else: p, ok, r = 0, False, f\"Favorite odds too high ({fav.odds:.2f})\"\n            score += p; factors[\"favoriteOdds\"] = {\"points\": p, \"ok\": ok, \"reason\": r}\n            if sec_fav.odds >= settings.MIN_2ND_FAV_ODDS: p, ok, r = settings.SECOND_FAV_ODDS_POINTS, True, f\"2nd Favorite OK ({sec_fav.odds:.2f})\"\n            else: p, ok, r = 0, False, f\"2nd Favorite odds too low ({sec_fav.odds:.2f})\"\n            score += p; factors[\"secondFavoriteOdds\"] = {\"points\": p, \"ok\": ok, \"reason\": r}\n        race.checkmate_score = score\n        race.is_qualified = score >= settings.QUALIFICATION_SCORE\n        race.trifecta_factors_json = json.dumps(factors)\n        return race\n\n# --- 7. DATABASE LAYER ---\nclass DatabaseManager:\n    def __init__(self, db_path: str = \"checkmate_races.db\"):\n        self.db_path = db_path\n        self._setup_database()\n\n    def _setup_database(self):\n        schema = \"\"\"PRAGMA journal_mode=WAL; CREATE TABLE IF NOT EXISTS live_races (race_id TEXT PRIMARY KEY, track_name TEXT, race_number INT, post_time TEXT, raw_data_json TEXT, checkmate_score REAL, qualified INT, trifecta_factors_json TEXT, updated_at TEXT); CREATE INDEX IF NOT EXISTS idx_races_qualified_score ON live_races(qualified, checkmate_score DESC, post_time);\"\"\"\n        with sqlite3.connect(self.db_path) as conn: conn.executescript(schema)\n\n    def _to_iso(self, dt: Optional[datetime]) -> Optional[str]: return dt.isoformat() if isinstance(dt, datetime) else None\n\n    def save_races(self, races: List[Race]):\n        with sqlite3.connect(self.db_path) as conn:\n            for race in races:\n                conn.execute(\"INSERT OR REPLACE INTO live_races (race_id, track_name, race_number, post_time, raw_data_json, checkmate_score, qualified, trifecta_factors_json, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\", (race.race_id, race.track_name, race.race_number, self._to_iso(race.post_time), race.model_dump_json(), race.checkmate_score, 1 if race.is_qualified else 0, race.trifecta_factors_json, datetime.utcnow().isoformat()))\n\n    def get_races(self, qualified_only: bool) -> List[Dict[str, Any]]:\n        query = f\"SELECT * FROM live_races {'WHERE qualified = 1' if qualified_only else ''} ORDER BY post_time IS NULL, checkmate_score DESC\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.row_factory = sqlite3.Row\n            return [dict(row) for row in conn.execute(query)]\n\n    def cleanup_old(self, days: int = 7):\n        cutoff_iso = (datetime.utcnow() - timedelta(days=days)).isoformat()\n        with sqlite3.connect(self.db_path) as conn: conn.execute(\"DELETE FROM live_races WHERE updated_at < ?\\\", (cutoff_iso,))\n\n# --- 8. BACKGROUND SERVICE ---\nclass BackgroundService:\n    def __init__(self, settings: Settings, db: DatabaseManager):\n        self.settings, self.db = settings, db\n        self.orchestrator, self.analyzer = DataSourceOrchestrator(settings), TrifectaAnalyzer()\n        self.postgres_etl = PostgresETL()\n        self.running, self.thread = False, None\n\n    def start(self): \n        if not self.running: self.running, self.thread = True, threading.Thread(target=self._run, daemon=True); self.thread.start()\n\n    def stop(self):\n        self.running = False\n        if self.thread: self.thread.join(5)\n        if self.orchestrator.fetcher: asyncio.run(self.orchestrator.fetcher.aclose())\n\n    def _run(self):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        last_cleanup = 0\n        try:\n            while self.running:\n                try:\n                    races = loop.run_until_complete(self.orchestrator.get_races())\n                    analyzed_races = [self.analyzer.analyze_race(r, self.settings) for r in races]\n                    \n                    # Save to local SQLite for UI\n                    self.db.save_races(analyzed_races)\n                    \n                    # Load into PostgreSQL Data Warehouse\n                    self.postgres_etl.process_and_load(analyzed_races)\n\n                    if time.time() - last_cleanup > 86400: self.db.cleanup_old(); last_cleanup = time.time()\n                except Exception as e: logging.exception(f\"Background error: {e}\")\n                for _ in range(60): # Check stop signal every second\n                    if not self.running: break\n                    time.sleep(1)\n        finally:\n            loop.run_until_complete(self.orchestrator.fetcher.aclose())\n            loop.close()\n\nclass R_Analytics_Bridge:\n    \"\"\"\n    Manages the execution of R scripts for advanced analytics, serving as the\n    bridge between the Python core and the R statistical engine.\n    \"\"\"\n    def __init__(self, r_script_path=\"r_scripts/predictive_model.R\"):\n        \"\"\"\n        Initializes the bridge with the path to the target R script.\n\n        Args:\n            r_script_path (str): The local file path to the .R script to be executed.\n        \"\"\"\n        self.r_script_path = r_script_path\n\n    def analyze_historical_data(self, historical_data: pd.DataFrame) -> dict | None:\n        \"\"\"\n        Executes the R script on a pandas DataFrame of historical race data.\n\n        Args:\n            historical_data (pd.DataFrame): A DataFrame containing the necessary columns\n                                            (e.g., 'odds', 'win', etc.) for the R model.\n\n        Returns:\n            dict | None: A dictionary containing the analysis results from the R script,\n                         or None if the analysis fails.\n        \"\"\"\n        # The Verbatim File Protocol in action: Create a temporary, complete CSV file.\n        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', newline='') as temp_csv:\n            temp_csv_path = temp_csv.name\n            historical_data.to_csv(temp_csv, index=False)\n\n        try:\n            # The Ironclad Protocol adapted for R: Use a controlled subprocess.\n            command = [\"Rscript\", self.r_script_path, temp_csv_path]\n\n            logging.info(f\"Executing R analytics engine: {' '.join(command)}\")\n\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                text=True,\n                check=True,  # Will raise CalledProcessError on non-zero exit codes\n                timeout=60  # Protect against hung R scripts\n            )\n\n            # The Receipts Protocol: Prove the result by parsing the JSON output.\n            logging.info(\"R analytics engine completed successfully.\")\n            analysis_output = json.loads(result.stdout)\n            return analysis_output\n\n        except FileNotFoundError:\n            logging.critical(\"'Rscript' command not found. Is R installed and in the system's PATH?\")\n            return None\n        except subprocess.CalledProcessError as e:\n            logging.critical(f\"R script execution failed with exit code {e.returncode}.\\n[R_STDERR]:\\n{e.stderr}\")\n            return None\n        except subprocess.TimeoutExpired:\n            logging.critical(\"R script execution timed out.\")\n            return None\n        except json.JSONDecodeError:\n            logging.critical(\"Failed to decode JSON output from R script.\")\n            return None\n        finally:\n            # Archive, Don't Annihilate (adapted): Clean up our temporary artifacts.\n            if os.path.exists(temp_csv_path):\n                os.remove(temp_csv_path)\n                logging.info(f\"Cleaned up temporary file: {temp_csv_path}\")\n\nfrom sqlalchemy import create_engine, text\n\nclass PostgresETL:\n    def __init__(self):\n        db_url = os.getenv(\"POSTGRES_URL\", \"postgresql://user:password@localhost:5432/checkmate_db\")\n        self.engine = create_engine(db_url)\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._setup_database()\n\n    def _setup_database(self):\n        try:\n            with self.engine.connect() as conn:\n                with open(\"pg_schemas/historical_races.sql\", \"r\") as f:\n                    conn.execute(text(f.read()))\n                with open(\"pg_schemas/quarantine_races.sql\", \"r\") as f:\n                    conn.execute(text(f.read()))\n                conn.commit()\n            self.logger.info(\"PostgreSQL schemas verified/created successfully.\")\n        except Exception as e:\n            self.logger.critical(f\"FATAL: Could not set up PostgreSQL database: {e}\", exc_info=True)\n            raise\n\n    def process_and_load(self, analyzed_races: List[Race]):\n        valid_for_historical = []\n        quarantined = []\n\n        for race in analyzed_races:\n            errors = []\n            if not race.track_name: errors.append(\"Missing track_name\")\n            if race.race_number is None: errors.append(\"Missing race_number\")\n            if race.post_time is None: errors.append(\"Missing post_time\")\n\n            if not errors:\n                valid_for_historical.append({\n                    \"race_id\": race.race_id,\n                    \"track_name\": race.track_name,\n                    \"race_number\": race.race_number,\n                    \"post_time\": race.post_time,\n                    \"source\": race.source,\n                    \"runners_data\": json.dumps([r.model_dump() for r in race.runners]),\n                    \"checkmate_score\": race.checkmate_score,\n                    \"is_qualified\": race.is_qualified,\n                })\n            else:\n                quarantined.append({\n                    \"race_id\": race.race_id,\n                    \"track_name\": race.track_name,\n                    \"source\": race.source,\n                    \"raw_data_json\": race.model_dump_json(),\n                    \"quarantine_reason\": \", \".join(errors),\n                })\n        \n        if valid_for_historical:\n            try:\n                df = pd.DataFrame(valid_for_historical)\n                df.to_sql('historical_races', self.engine, if_exists='append', index=False)\n                self.logger.info(f\"Successfully loaded {len(df)} races into historical_races.\")\n            except Exception as e:\n                self.logger.error(f\"Failed to load data into historical_races: {e}\", exc_info=True)\n\n        if quarantined:\n            try:\n                df_q = pd.DataFrame(quarantined)\n                df_q.to_sql('quarantine_races', self.engine, if_exists='append', index=False)\n                self.logger.warning(f\"Quarantined {len(df_q)} races for manual review.\")\n            except Exception as e:\n                self.logger.error(f\"Failed to load data into quarantine_races: {e}\", exc_info=True)\n\n# --- 9. STREAMLIT DASHBOARD ---\n\n@st.cache_resource\ndef get_db_manager(): return DatabaseManager()\n\n@st.cache_resource\ndef get_settings():\n    s = Settings()\n    logging.info(f\"Effective settings: {s.model_dump()}\")\n    return s\n\n@st.cache_resource\ndef get_background_service(_settings, _db): return BackgroundService(_settings, _db)\n\ndef _format_post_time(pt: Optional[str]) -> str:\n    if not pt: return \"N/A\"\n    try: return datetime.fromisoformat(pt.replace(\"Z\", \"+00:00\")).strftime(\"%I:%M %p\")\n    except: return \"Invalid Date\"\n\ndef display_race(race: Dict):\n    with st.container():\n        c1, c2 = st.columns()\n        c1.subheader(f\"{race.get('track_name', 'Unknown')} - R{race.get('race_number', '?')}\")\n        c1.caption(f\"Post Time: {_format_post_time(race.get('post_time'))} | Source: {race.get('source', 'N/A')}\")\n        c2.metric(\"Score\", f\"{race.get('checkmate_score', 0) or 0:.1f}\")\n        if st.toggle('Show Analysis', key=f\"toggle_{race.get('race_id')}\"):\n            try: factors = json.loads(race.get('trifecta_factors_json') or \"{}\")\n            except: factors = {}\n            for f in factors.values(): st.text(f\"{'\u2705' if f.get('ok') else '\u274c'} {f.get('reason', '')} ({f.get('points', 0):+.0f} pts)\")\n\ndef main():\n    st.set_page_config(page_title=\"Checkmate V8\", layout=\"wide\")\n    st.title(\"\ud83c\udfc7 Checkmate V8: Sovereign Racing Analysis\")\n    settings, db = get_settings(), get_db_manager()\n    service = get_background_service(settings, db)\n\n    with st.sidebar:\n        st.header(\"\u2699\ufe0f Control Panel\")\n        col1, col2 = st.columns(2)\n        if col1.button(\"\u25b6\ufe0f Start\", use_container_width=True, disabled=st.session_state.get('monitoring', False)):\n            st.session_state.monitoring = True\n            service.start()\n            st.rerun()\n        if col2.button(\"\u23f8\ufe0f Stop\", use_container_width=True, disabled=not st.session_state.get('monitoring', False)):\n            st.session_state.monitoring = False\n            service.stop()\n            st.rerun()\n        st.info(f\"Status: {'\ud83d\udfe2 Active' if st.session_state.get('monitoring', False) else '\ud83d\udd34 Stopped'}\")\n        refresh_sec = st.slider(\"Auto-refresh (sec)\", 10, 120, 60, 10)\n\n    tab1, tab2, tab3 = st.tabs([\"\ud83c\udfc6 Qualified Races\", \"\ud83d\udcca All Races\", \"\ud83d\udd2c Advanced Analytics\"])\n    with tab1:\n        for race in db.get_races(qualified_only=True): display_race(race)\n    with tab2:\n        for race in db.get_races(qualified_only=False): display_race(race)\n    with tab3:\n        st.header(\"R Analytics Bridge\")\n        st.write(\"Execute the R predictive model on the placeholder historical data.\")\n        if st.button(\"Run Historical Analysis\"):\n            with st.spinner(\"Loading data and executing R model... This may take a moment.\"):\n                try:\n                    df = pd.read_csv(\"historical_data.csv\")\n                    bridge = R_Analytics_Bridge()\n                    results = bridge.analyze_historical_data(df)\n                    if results:\n                        st.success(\"Analysis Complete!\")\n                        st.json(results)\n                    else:\n                        st.error(\"R analytics bridge failed. Check server logs for details.\")\n                except FileNotFoundError:\n                    st.error(\"`historical_data.csv` not found. Please ensure the file exists in the root directory.\")\n                except Exception as e:\n                    st.error(f\"An unexpected error occurred: {e}\")\n\n    if st.session_state.get(\"monitoring\", False):\n        from streamlit_autorefresh import st_autorefresh\n        st_autorefresh(interval=refresh_sec * 1000, key=\"autorefresh\")\n\nif __name__ == \"__main__\":\n    main()"
}