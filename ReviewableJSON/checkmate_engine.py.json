{
    "filepath": "./checkmate_engine.py",
    "content": "# checkmate_engine.py\n# This is a self-contained, portable script combining the best of Checkmate V7.\n\nimport logging\nimport json\nimport subprocess\nfrom abc import ABC, abstractmethod\nfrom typing import List, Union, Optional, Dict\nfrom datetime import date, datetime, timezone\nfrom pydantic_settings import BaseSettings\nfrom pydantic import BaseModel, Field\nfrom io import StringIO\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport argparse\nfrom tabulate import tabulate\nfrom colorama import Fore, Style, init as colorama_init\n\n# --- Part 1: Settings ---\nclass Settings(BaseSettings):\n    DATABASE_URL: str = Field(default=\"sqlite:///./checkmate_v7.db\")\n    REDIS_URL: str = Field(default=\"redis://localhost:6379/0\")\n    LOG_LEVEL: str = Field(default=\"INFO\")\n    ODDS_API_KEY: Optional[str] = Field(default=None, description=\"The API key for The Odds API.\")\n    QUALIFICATION_SCORE: float = Field(default=75.0, description=\"Minimum score to qualify.\")\n    FIELD_SIZE_OPTIMAL_MIN: int = Field(default=4)\n    FIELD_SIZE_OPTIMAL_MAX: int = Field(default=6)\n    FIELD_SIZE_ACCEPTABLE_MIN: int = Field(default=7)\n    FIELD_SIZE_ACCEPTABLE_MAX: int = Field(default=8)\n    FIELD_SIZE_OPTIMAL_POINTS: int = Field(default=30)\n    FIELD_SIZE_ACCEPTABLE_POINTS: int = Field(default=10)\n    FIELD_SIZE_PENALTY_POINTS: int = Field(default=-20)\n    FAV_ODDS_POINTS: int = Field(default=30)\n    MAX_FAV_ODDS: float = Field(default=3.5)\n    SECOND_FAV_ODDS_POINTS: int = Field(default=40)\n    MIN_2ND_FAV_ODDS: float = Field(default=4.0)\n\n    class Config:\n        env_file = \".env\"\n        case_sensitive = False\nsettings = Settings()\n\n# --- Part 2: Models ---\nclass Runner(BaseModel):\n    name: str\n    odds: Optional[float] = None\n    program_number: Optional[int] = None\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\nclass Race(BaseModel):\n    race_id: str\n    track_name: str\n    race_number: Optional[int] = None\n    post_time: Optional[datetime] = None\n    race_type: Optional[str] = None\n    number_of_runners: Optional[int] = None\n    runners: List[Runner]\n    source: Optional[str] = None\n\nclass HorseSchema(BaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = None\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n    odds: Optional[float] = None\n    morningLine: Optional[float] = None\n    speed: Optional[int] = None\n    class_rating: Optional[int] = None\n    form: Optional[str] = None\n    lastRaced: Optional[str] = None\n\nclass RaceDataSchema(BaseModel):\n    id: Optional[str] = None\n    track: Optional[str] = None\n    raceNumber: Optional[int] = None\n    postTime: Optional[str] = None\n    horses: List[HorseSchema]\n    conditions: Optional[str] = None\n    distance: Optional[str] = None\n    surface: Optional[str] = None\n    checkmateScore: Optional[float] = None\n    qualified: Optional[bool] = None\n    trifectaFactors: Optional[Dict] = None\n\n# --- Part 3: Base Fetcher & Adapter ---\nclass DefensiveFetcher:\n    \"\"\"The battle-tested, subprocess curl-based fetcher.\"\"\"\n    def get(self, url: str, headers: dict = None, response_type: str = 'auto') -> Union[dict, str, None]:\n        try:\n            command = [\"curl\", \"-s\", \"-L\", \"--tlsv1.2\", \"--http1.1\"]\n            if headers:\n                for key, value in headers.items():\n                    command.extend([\"-H\", f\"{key}: {value}\"])\n            command.append(url)\n            result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=30)\n            response_text = result.stdout\n            if response_type == 'text': return response_text\n            if response_type == 'json': return json.loads(response_text)\n            try: return json.loads(response_text)\n            except json.JSONDecodeError: return response_text\n        except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError) as e:\n            logging.error(f\"CRITICAL: curl GET failed for {url}. Details: {e}\")\n            return None\n\n    def post(self, url: str, json_data: dict, headers: dict = None, response_type: str = 'json') -> Union[dict, str, None]:\n        try:\n            command = [\"curl\", \"-s\", \"-L\", \"-X\", \"POST\", \"--tlsv1.2\", \"--http1.1\"]\n            final_headers = headers.copy() if headers else {}\n            final_headers['Content-Type'] = 'application/json'\n            for key, value in final_headers.items():\n                command.extend([\"-H\", f\"{key}: {value}\"])\n            command.extend([\"-d\", json.dumps(json_data)])\n            command.append(url)\n            result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=30)\n            response_text = result.stdout\n            return json.loads(response_text) if response_type == 'json' else response_text\n        except (subprocess.CalledProcessError, json.JSONDecodeError, subprocess.TimeoutExpired, FileNotFoundError) as e:\n            logging.error(f\"CRITICAL: curl POST failed for {url}. Details: {e}\")\n            return None\n\nclass BaseAdapterV7(ABC):\n    def __init__(self, defensive_fetcher: DefensiveFetcher):\n        self.fetcher = defensive_fetcher\n    @abstractmethod\n    def fetch_races(self) -> List[Race]:\n        raise NotImplementedError\n\n# --- Part 4: Production Adapters ---\ndef _convert_odds_to_float(odds_str: Optional[str]) -> Optional[float]:\n    if not odds_str or not isinstance(odds_str, str): return None\n    odds_str = odds_str.strip().upper()\n    if odds_str in ['SP', 'SCRATCHED']: return None\n    if odds_str in ['EVS', 'EVENS']: return 2.0\n    if '/' in odds_str:\n        try:\n            num, den = map(int, odds_str.split('/'))\n            return (num / den) + 1.0 if den != 0 else None\n        except (ValueError, ZeroDivisionError): return None\n    try: return float(odds_str)\n    except (ValueError, TypeError): return None\n\nclass SkySportsAdapter(BaseAdapterV7):\n    SOURCE_ID = \"skysports\"\n    BASE_URL = \"https://www.skysports.com\"\n    def fetch_races(self) -> List[Race]:\n        index_url = f\"{self.BASE_URL}/racing/racecards\"\n        index_html = self.fetcher.get(index_url, response_type='text')\n        if not index_html: return []\n        soup = BeautifulSoup(index_html, \"lxml\")\n        race_urls = [f\"{self.BASE_URL}{link['href']}\" for link in soup.select(\"a.sdc-site-racing-meetings__event-link[href]\")]\n        all_races = []\n        for url in race_urls[:5]:\n            detail_html = self.fetcher.get(url, response_type='text')\n            if detail_html:\n                race = self._parse_race_details(detail_html, url)\n                if race: all_races.append(race)\n        return all_races\n    def _parse_race_details(self, html: str, url: str) -> Optional[Race]:\n        soup = BeautifulSoup(html, \"lxml\")\n        track_name = soup.select_one(\"h1.sdc-site-racing-header__title\").text.strip()\n        post_time_str = soup.select_one(\"span.sdc-site-racing-header__time\").text.strip()\n        post_time = datetime.combine(date.today(), datetime.strptime(post_time_str, \"%H:%M\").time())\n        runners = [Runner(name=item.select_one(\"h4 a\").text.strip(), program_number=int(item.select_one(\"div strong\").text.strip()), odds=_convert_odds_to_float(item.select_one(\".sdc-site-racing-card__betting-odds\").text.strip())) for item in soup.select(\"div.sdc-site-racing-card__item\") if item.select_one(\"h4 a\")]\n        if not runners: return None\n        return Race(race_id=f\"{self.SOURCE_ID}_{url.split('/')[-1]}\", track_name=track_name, post_time=post_time, runners=[r for r in runners if r.odds], source=self.SOURCE_ID)\n\nclass AtTheRacesAdapter(BaseAdapterV7):\n    SOURCE_ID = \"attheraces\"\n    BASE_URL = \"https://www.attheraces.com\"\n    def fetch_races(self) -> List[Race]:\n        index_html = self.fetcher.get(f\"{self.BASE_URL}/racecards\", response_type='text')\n        if not index_html: return []\n        soup = BeautifulSoup(index_html, 'html.parser')\n        links = [{\"url\": self.BASE_URL + a['href'], \"track\": a.find_previous(\"h2\").text.strip().replace(\" Racecards\",\"\"), \"time\": a.select_one(\"span.h7\").text.strip()[:5]} for a in soup.select(\"a.a--plain[href*='/racecard/']\")]\n        all_races = []\n        for link in links[:5]:\n            html = self.fetcher.get(link[\"url\"], response_type='text')\n            if html:\n                race = self._parse_single_race(html, link)\n                if race: all_races.append(race)\n        return all_races\n    def _parse_single_race(self, html: str, details: Dict) -> Optional[Race]:\n        soup = BeautifulSoup(html, 'html.parser')\n        runners = [Runner(name=card.select_one(\".horse-name a\").text.strip(), program_number=int(card.select_one(\".runner-number\").text.strip()), odds=_convert_odds_to_float(card.select_one(\".odds\").text.strip())) for card in soup.select(\"div.runner-card\") if card.select_one(\".horse-name a\")]\n        if not runners: return None\n        return Race(race_id=f'{self.SOURCE_ID}_{details[\"url\"].split(\"/\")[-2]}_{details[\"url\"].split(\"/\")[-1]}', track_name=details['track'], runners=[r for r in runners if r.odds], source=self.SOURCE_ID)\n\nclass TVGAdapter(BaseAdapterV7):\n    SOURCE_ID = \"tvg\"\n    BASE_URL = \"https://mobile-api.tvg.com/api/mobile/races/today\"\n    def fetch_races(self) -> List[Race]:\n        logging.info(f\"Fetching races from {self.SOURCE_ID}\")\n        response_data = self.fetcher.get(self.BASE_URL, response_type='json')\n        if not response_data or 'races' not in response_data:\n            logging.warning(f\"{self.SOURCE_ID}: No 'races' key found in API response.\")\n            return []\n        return self._parse_races(response_data['races'])\n    def _parse_races(self, races_data: List[Dict]) -> List[Race]:\n        all_races = []\n        for race_info in races_data:\n            try:\n                runners = []\n                for runner_info in race_info.get('runners', []):\n                    if runner_info.get('scratched'): continue\n                    odds_val = self._parse_odds(runner_info.get('odds'))\n                    if odds_val is None: continue\n                    runners.append(Runner(name=runner_info.get('horseName', 'Unknown Horse'), program_number=runner_info.get('programNumber'), odds=odds_val))\n                if not runners: continue\n                post_time = self._parse_time(race_info.get('postTime'))\n                race = Race(race_id=f\"tvg_{race_info.get('raceId')}\", track_name=race_info.get('trackName', 'Unknown Track'), race_number=race_info.get('raceNumber'), post_time=post_time, runners=runners, number_of_runners=len(runners), source=self.SOURCE_ID)\n                all_races.append(race)\n            except Exception as e:\n                logging.warning(f\"Skipping malformed TVG race due to error: {e}\")\n                continue\n        return all_races\n    def _parse_odds(self, odds_data: Optional[Dict]) -> Optional[float]:\n        if not odds_data or odds_data.get('morningLine') is None: return None\n        try:\n            odds_str = odds_data['morningLine']\n            if '/' in odds_str:\n                num, den = map(int, odds_str.split('/'))\n                return (num / den) + 1.0\n            return float(odds_str)\n        except (ValueError, TypeError, ZeroDivisionError): return None\n    def _parse_time(self, time_str: Optional[str]) -> Optional[datetime]:\n        if not time_str: return None\n        try: return datetime.fromisoformat(time_str.replace('Z', '+00:00'))\n        except ValueError: return None\n\n# --- Part 5: The Brain ---\nclass TrifectaAnalyzer:\n    \"\"\"The corrected analyzer, fully aligned with the settings module.\"\"\"\n    def analyze_race(self, race: RaceDataSchema) -> dict:\n        score = 0\n        reasons = []\n        trifecta_factors = {}\n        if not race.horses:\n            return {\"qualified\": False, \"checkmateScore\": 0, \"reasons\": [\"No horses data.\"], \"trifectaFactors\": {}}\n        horses_with_odds = sorted([h for h in race.horses if h.odds], key=lambda h: h.odds)\n        num_runners = len(horses_with_odds)\n        if settings.FIELD_SIZE_OPTIMAL_MIN <= num_runners <= settings.FIELD_SIZE_OPTIMAL_MAX:\n            points, ok, reason = settings.FIELD_SIZE_OPTIMAL_POINTS, True, f\"Optimal field size ({num_runners} runners)\"\n        elif settings.FIELD_SIZE_ACCEPTABLE_MIN <= num_runners <= settings.FIELD_SIZE_ACCEPTABLE_MAX:\n            points, ok, reason = settings.FIELD_SIZE_ACCEPTABLE_POINTS, True, f\"Acceptable field size ({num_runners} runners)\"\n        else:\n            points, ok, reason = settings.FIELD_SIZE_PENALTY_POINTS, False, f\"Field size not ideal ({num_runners} runners)\"\n        score += points\n        reasons.append(reason)\n        trifecta_factors[\"fieldSize\"] = {\"points\": points, \"ok\": ok, \"reason\": reason}\n        if num_runners >= 2:\n            favorite, second_favorite = horses_with_odds[0], horses_with_odds[1]\n            if favorite.odds <= settings.MAX_FAV_ODDS:\n                points, ok, reason = settings.FAV_ODDS_POINTS, True, f\"Favorite odds OK ({favorite.odds})\"\n            else:\n                points, ok, reason = 0, False, f\"Favorite odds too high ({favorite.odds})\"\n            score += points\n            reasons.append(reason)\n            trifecta_factors[\"favoriteOdds\"] = {\"points\": points, \"ok\": ok, \"reason\": reason}\n            if second_favorite.odds >= settings.MIN_2ND_FAV_ODDS:\n                points, ok, reason = settings.SECOND_FAV_ODDS_POINTS, True, f\"2nd Favorite odds OK ({second_favorite.odds})\"\n            else:\n                points, ok, reason = 0, False, f\"2nd Favorite odds too low ({second_favorite.odds})\"\n            score += points\n            reasons.append(reason)\n            trifecta_factors[\"secondFavoriteOdds\"] = {\"points\": points, \"ok\": ok, \"reason\": reason}\n        else:\n            reasons.append(\"Not enough runners with odds for full analysis.\")\n        return {\"qualified\": score >= settings.QUALIFICATION_SCORE, \"checkmateScore\": score, \"reasons\": reasons, \"trifectaFactors\": trifecta_factors}\n\n# --- Part 6: The Conductor ---\nPRODUCTION_ADAPTERS = [SkySportsAdapter, AtTheRacesAdapter, TVGAdapter]\n\nclass DataSourceOrchestrator:\n    def __init__(self):\n        self.fetcher = DefensiveFetcher()\n        adapters_to_use = PRODUCTION_ADAPTERS\n        logging.info(f\"Initializing orchestrator with {len(adapters_to_use)} PRODUCTION adapters.\")\n        self.adapters: List[BaseAdapterV7] = [Adapter(self.fetcher) for Adapter in adapters_to_use]\n\n    def get_races(self) -> tuple[list[Race], list[dict]]:\n        all_races, statuses = [], []\n        for adapter in self.adapters:\n            adapter_id = adapter.__class__.__name__\n            races, error_message, status, notes = [], None, \"OK\", \"\"\n            try:\n                races = adapter.fetch_races()\n                if races: notes = f\"Successfully parsed {len(races)} races.\"\n                else: notes = \"No upcoming races found on source.\"\n                statuses.append({\"adapter_id\": adapter_id, \"status\": status, \"races_found\": len(races), \"notes\": notes, \"error_message\": None})\n                if races: all_races.extend(races)\n            except Exception as e:\n                logging.error(f\"Adapter {adapter_id} failed: {e}\", exc_info=True)\n                status, error_message = \"ERROR\", str(e)\n                notes = f\"API Error: {error_message}\"\n                statuses.append({\"adapter_id\": adapter_id, \"status\": status, \"error_message\": error_message, \"notes\": notes, \"races_found\": 0})\n        return all_races, statuses\n\n# --- Part 7: The Face (CLI) ---\ndef setup_logging():\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - [%(levelname)s] - %(message)s\")\n\ndef convert_race_to_schema(race: Race) -> RaceDataSchema:\n    horses = [HorseSchema(name=r.name, number=r.program_number, jockey=r.jockey, trainer=r.trainer, odds=r.odds) for r in race.runners if r.odds is not None]\n    return RaceDataSchema(id=race.race_id, track=race.track_name, raceNumber=race.race_number, postTime=race.post_time.isoformat() if race.post_time else None, horses=horses)\n\ndef display_results(tipsheet, statuses):\n    colorama_init()\n    print(\"\\n--- \" + Fore.CYAN + \"Adapter Status\" + Style.RESET_ALL + \" ---\")\n    status_headers = {\"adapter_id\": \"Adapter\", \"status\": \"Status\", \"races_found\": \"Races Found\", \"notes\": \"Notes\"}\n    status_data = []\n    for s in statuses:\n        status_color = Fore.GREEN if s['status'] == 'OK' else Fore.RED\n        status_data.append([s['adapter_id'], status_color + s['status'] + Style.RESET_ALL, s['races_found'], s['notes']])\n    print(tabulate(status_data, headers=status_headers, tablefmt=\"grid\"))\n\n    if not tipsheet:\n        print(\"\\n\" + Fore.YELLOW + \"No qualified races found for the tipsheet.\" + Style.RESET_ALL)\n        return\n\n    print(\"\\n--- \" + Fore.CYAN + \"Checkmate Qualified Races\" + Style.RESET_ALL + \" ---\")\n    for race in tipsheet:\n        print(f\"\\n\" + Fore.GREEN + f\"Track: {race['trackName']} - Race: {race['raceNumber']} - Post Time: {race['postTime']} - Score: {race['checkmateScore']}\" + Style.RESET_ALL)\n        runner_headers = {\"name\": \"Horse\", \"odds\": \"Odds\", \"program_number\": \"Program\"}\n        runner_data = [[r['name'], r['odds'], r.get('number')] for r in race['runners']]\n        print(tabulate(runner_data, headers=runner_headers, tablefmt=\"psql\"))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Checkmate V7 - The Ultimate Engine\")\n    parser.add_argument(\"--output\", choices=[\"json\", \"table\"], default=\"table\", help=\"Specify the output format.\")\n    args = parser.parse_args()\n    setup_logging()\n    logging.info(\"--- Starting Checkmate V7: The Ultimate Engine ---\")\n    orchestrator = DataSourceOrchestrator()\n    analyzer = TrifectaAnalyzer()\n    logging.info(\"Fetching live race data...\")\n    races, statuses = orchestrator.get_races()\n    logging.info(f\"Orchestrator status: {statuses}\")\n    tipsheet = []\n    if not races:\n        logging.warning(\"No races were found by the orchestrator.\")\n    else:\n        logging.info(f\"Found {len(races)} races. Analyzing for Checkmate opportunities...\")\n        for race in races:\n            race_schema = convert_race_to_schema(race)\n            analysis = analyzer.analyze_race(race_schema)\n            if analysis[\"qualified\"]:\n                logging.info(f\"Checkmate QUALIFIED: {race.track_name} - Race {race.race_number} (Score: {analysis['checkmateScore']})\")\n                tipsheet.append({\"trackName\": race.track_name, \"raceNumber\": race.race_number, \"postTime\": race.post_time.isoformat() if race.post_time else None, \"checkmateScore\": analysis[\"checkmateScore\"], \"analysis\": analysis, \"runners\": [r.model_dump() for r in race_schema.horses]})\n            else:\n                logging.info(f\"Checkmate SKIPPED: {race.track_name} - Race {race.race_number} (Score: {analysis['checkmateScore']})\")\n\n    if args.output == \"json\":\n        timestamp = datetime.now().strftime(\"%m%d_%Hh%M\")\n        output_filename = f\"tipsheet_{timestamp}.json\"\n        logging.info(f\"Writing {len(tipsheet)} qualified races to {output_filename}...\")\n        with open(output_filename, \"w\") as f:\n            json.dump(tipsheet, f, indent=2)\n        logging.info(\"Successfully generated tipsheet.\")\n    else:\n        display_results(tipsheet, statuses)\n\n    logging.info(\"--- Checkmate V7 Showcase Run Finished ---\")\n\nif __name__ == \"__main__\":\n    main()"
}