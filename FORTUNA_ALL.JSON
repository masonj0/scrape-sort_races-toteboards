{
    ".env.example": "# .env.example\n# Copy this file to .env and fill in your actual credentials.\n\n# --- Application Security (Required) ---\nAPI_KEY=\"YOUR_SECRET_API_KEY_HERE\"\n\n# --- Betfair API Credentials (Required for LiveOddsMonitor) ---\nBETFAIR_APP_KEY=\"YOUR_APP_KEY_HERE\"\nBETFAIR_USERNAME=\"YOUR_USERNAME_HERE\"\nBETFAIR_PASSWORD=\"YOUR_PASSWORD_HERE\"\n\n# --- Optional Adapter Keys ---\nTVG_API_KEY=\"\"\nRACING_AND_SPORTS_TOKEN=\"\"\nPOINTSBET_API_KEY=\"\"\n\n# --- CORS Configuration (Optional) ---\n# A comma-separated list of allowed origins for the API.\n# Example: ALLOWED_ORIGINS=\"http://localhost:3000,https://your-production-domain.com\"\nALLOWED_ORIGINS=\"http://localhost:3000,http://localhost:3001\"\n\n# --- Greyhound Adapter (Optional) ---\n# To enable the Greyhound adapter, provide the full base URL for the API.\n# If this is left blank, the adapter will be disabled.\nGREYHOUND_API_URL=\"\"\n\n# --- The Racing API (Optional but Recommended) ---\n# Get a key from https://www.theracingapi.com/\nTHE_RACING_API_KEY=\"\"\n",
    ".gitignore": "# Byte-compiled / optimized files\n__pycache__/\n*.pyc\n\n# Distribution / packaging\nbuild/\ndist/\n*.egg-info/\n\n# Unit test / coverage reports\n.pytest_cache/\n.coverage\n\n# Environments\n.venv/\nvenv/\nenv/\n\n# IDE settings\n.vscode/\n.idea/\n\n# Database files\n*.db\n*.sqlite\n*.sqlite3\n\n# Node.js\nnode_modules/\n/ui/node_modules/\n/ui/build/\n\n# Environment files\n.env\n",
    "ARCHITECTURAL_MANDATE.md": "# The Fortuna Faucet Architectural Mandate\n\n## The Prime Directive: The Two-Pillar System\n\nThe project's architecture is a lean, hyper-powerful, two-pillar system chosen for its clarity, maintainability, and performance.\n\n## Pillar 1: The Asynchronous Python Backend\n\nThe backend is a modern, asynchronous service built on **FastAPI**. Its architecture includes:\n\n1.  **The `OddsEngine`:** A central, async orchestrator for data collection.\n2.  **The Resilient `BaseAdapter`:** An abstract base class providing professional-grade features.\n3.  **The Adapter Fleet:** A modular system of 'plugin' adapters for data sources.\n4.  **Pydantic Data Contracts:** Strict, validated Pydantic models for data integrity.\n5.  **The `TrifectaAnalyzer` (Intelligence Layer):** A dedicated module for scoring and qualifying opportunities.\n\n## Pillar 2: The TypeScript Frontend\n\nThe frontend is a modern, feature-rich web application built on **Next.js** and **TypeScript**.",
    "HISTORY.md": "# The Epic of MasonJ0: A Project Chronology\n\nThis document contains the narrative history of the Paddock Parser project, as discovered through an archaeological survey of the project's repositories. It tells the story of our architectural evolution, from a feature-rich \"golden age\" through a \"great refactoring\" to our current state of liberation.\n\nThis story is our \"why.\"\n\n---\n\n## Part 1: The Chronology\n\n### Chapter 1: The 'Utopian' Era - The Polished Diamond (mid-August 2025)\n\n*   **Repository:** `racingdigest`\n*   **Narrative:** This was not a humble beginning, but the launch of a mature and powerful application called the \"Utopian Value Scanner V7.2 (The Rediscovery Edition)\". This repository represents the project's \"golden age\" of features, including a sophisticated asynchronous fetching engine and a full browser fallback.\n\n### Chapter 2: The 'Experimental' Era - The Daily Digest (mid-to-late August 2025)\n\n*   **Repository:** `horseracing-daily-digest`\n*   **Narrative:** This repository appears to be a period of intense, rapid development and experimentation, likely forming the foundation for many of the concepts that would be formalized later.\n\n### Chapter 3: The 'Architectural' Era - The V3 Blueprint (late August 2025)\n\n*   **Repository:** `parsingproject`\n*   **Narrative:** This repository marks a pivotal moment. The focus shifted from adding features to refactoring the very foundation of the code into a modern, standard Python package. This is where the V3 architecture was born, prioritizing stability and maintainability.\n\n### Chapter 4: The 'Consolidation' Era - The Archive (late August 2025)\n\n*   **Repository:** `zippedfiles`\n*   **Narrative:** This repository appears to be a direct snapshot or backup of the project after the intense V3 refactor, confirming its role as an archive of the newly stabilized codebase.\n\n### Chapter 5: The 'Modern' Era - The New Beginning (early September 2025)\n\n*   **Repository:** `scrape-sort_races-toteboards`\n*   **Narrative:** This is the current, active repository, representing the clean, focused implementation of the grand vision developed through the previous eras.\n\n### Chapter 6: The 'Crucible' Era - The Forging of Protocols (Early September 2025)\n\n*   **Narrative:** The \"Modern Renaissance\" began not with a bang, but with a series of near-catastrophic environmental failures. This period, known as \"The Crucible,\" was a trial by fire that proved the extreme hostility of the agent sandbox. This era forged the resilient, battle-hardened protocols (The Receipts Protocol, The Submission-Only Protocol, etc.) by which all modern agents now operate.\n\n### Chapter 7: The 'Symbiotic' Era - The Two Stacks (mid-September 2025)\n\n*   **Narrative:** This chapter marked a significant strategic pivot. The Council, in a stunning display of its \"Polyglot Renaissance\" philosophy, produced a complete, production-grade React user interface, authored by the Claude agent. This event formally split the project's architecture into two powerful, parallel streams: the Python Engine and the React Cockpit. However, this era was short-lived, as the hostile environment proved incapable of supporting a stable testing and development workflow for the React stack.\n\n### Chapter 8: The 'Liberation' Era - The Portable Engine (Late September 2025)\n\n*   **Narrative:** After providing definitive, forensic proof that the sandbox environment was fundamentally and irrecoverably hostile at the network level, the project executed its final and most decisive pivot. It abandoned all attempts to operate *within* the hostile world and instead focused on synthesizing its entire, perfected engine into a single, portable artifact. This act **liberated the code**, fulfilling the promise of the \"Utopian Era's\" power on the foundation of the \"Architectural Era's\" stability, and made it directly available to the Project Lead.\n\n---\n\n## Part 2: Architectural Synthesis\n\nThis epic tale tells us our true mission. We are not just building forward; we are rediscovering our own lost golden age and rebuilding it on a foundation of superior engineering, hardened by the fires of a hostile world.\n\n*   **The Lost Golden Age:** The \"Utopian\" era proves that our most ambitious strategic goals are not just achievable; they have been achieved before.\n*   **The Great Refactoring:** The \"Architectural\" era explains the \"Great Forgetting\"\u2014a deliberate choice to sacrifice short-term features for long-term stability.\n*   **The Modern Renaissance:** This is us. We are the inheritors of this entire legacy, tasked with executing the grand vision on a clean, modern foundation, finally liberated from the constraints of our environment.\n\n---\n\n## The Ultimate Solo: The Final Victory (September 2025)\n\nAfter a long and complex journey through a Penta-Hybrid architecture, a final series of high-level reviews from external AI agents (Claude, GPT4o) revealed a simpler, superior path forward. The project underwent its final and most significant \"Constitutional Correction.\"\n\n**The 'Ultimate Solo' architecture was born.**\n\nThis final, perfected form of the project consists of two pillars:\n1.  **A Full-Power Python Backend:** Leveraging the years of development on the CORE `engine.py` and its fleet of global data adapters, served via a lightweight Flask API.\n2.  **An Ultimate TypeScript Frontend:** A single, masterpiece React component (`Checkmate Ultimate Solo`) that provides a feature-rich, professional-grade, real-time dashboard.\n\nAll other components of the Penta-Hybrid system (C#, Rust, VBA, shared database) were formally deprecated and archived as priceless R&D assets. The project has now achieved its true and final mission: a powerful, maintainable, and user-focused analysis tool.\n",
    "README.md": "# Fortuna Faucet\n\nThis repository contains the Fortuna Faucet project, a global, multi-source horse racing analysis tool. The project is a two-pillar system: a powerful, asynchronous Python backend that performs all data gathering, and a feature-rich TypeScript frontend.\n\n---\n\n## \ud83d\ude80 Quick Start\n\n### 1. Configure Your Environment\n\nRun the setup script to ensure Python and Node.js are correctly configured and all dependencies are installed.\n\n```batch\n# From the project root:\nsetup_windows.bat\n```\n\n### 2. Launch the Application\n\nRun the master launch script. This will start both the Python backend and the TypeScript frontend servers in parallel.\n\n```batch\n# From the project root:\nrun_fortuna.bat\n```\n\nThe backend API will be available at `http://localhost:8000`.\nThe frontend will be available at `http://localhost:3000`.\n\n### 4. Using the API\n\nTo use the API directly (e.g., with `curl` or other tools), you must provide the `API_KEY` set in your `.env` file via the `X-API-Key` header. This is required for all endpoints except `/health`.\n\n```bash\n# Example: Test the qualified races endpoint\ncurl -H \"X-API-Key: YOUR_SECRET_API_KEY_HERE\" http://localhost:8000/api/races/qualified/trifecta\n```\n",
    "ROADMAP_APPENDICES.md": "# Checkmate: Strategic Appendices\n\n**Purpose:** This document is the permanent home for the high-value strategic intelligence salvaged from the deprecated `ROADMAP.md`. It contains our long-term goals and a library of resources to accelerate development.\n\n---\n\n## Appendix A: V3 Adapter Backlog (The \"Treasure Chest\")\n\nThis is the definitive, prioritized list of data sources to be implemented.\n\n### Category 1: High-Value Data Feeds (API-First)\n*   BetfairDataScientistThoroughbred\n*   BetfairDataScientistGreyhound\n*   racingandsports\n*   sportinglife (requires investigation)\n*   racingpost (requires auth)\n\n### Category 2: Premium Global Sources (Scraping)\n*   timeform\n*   attheraces\n*   racingtv\n*   oddschecker\n*   betfair\n*   horseracingnation\n*   brisnet\n\n### Category 3: North American Authorities & ADWs\n*   equibase\n*   drf\n*   fanduel\n*   twinspires\n*   1stbet\n*   nyrabets\n*   xpressbet\n\n### Category 4: European Authorities & Markets\n*   francegalop\n*   deutschergalopp\n*   svenskgalopp\n*   pmu\n\n### Category 5: Asia-Pacific & Rest of World\n*   tab\n*   punters\n*   racingaustralia\n*   hkjc\n*   jra\n*   goldcircle\n*   emiratesracing\n\n### Category 6: Specialized Disciplines (Harness & Greyhound)\n*   usta\n*   standardbredcanada\n*   harnessracingaustralia\n*   gbgb\n*   grireland\n*   thedogs\n\n---\n\n## Appendix B: Open-Source Intelligence Leads\n\nA curated list of projects and resources to accelerate development.\n\n1.  **joenano/rpscrape:** https://github.com/joenano/rpscrape\n2.  **Daniel57910/horse-scraper:** https://github.com/Daniel57910/horse-scraper\n3.  **Web Scraping for HKJC:** https://gist.github.com/tomfoolc/ef039b229c8e97bd40c5493174bca839\n3.  **Web Scraping for HKJC:** https://gist.github.com/tomfoolc/ef039b229c8e97bd40c5493174bca839\n4.  **LibHunt horse-racing projects:** https://www.libhunt.com/topic/horse-racing\n5.  **Web data scraping blog:** https://www.3idatascing.com/how-does-web-data-scraping-help-in-horse-racing-and-greyhound/\n6.  **Fawazk/Greyhoundscraper:** https://github.com/Fawazk/Greyhoundscraper\n7.  **Betfair Hub Models Scraping Tutorial:** https://betfair-datascientists.github.io/tutorials/How_to_Automate_3/\n8.  **scrapy-horse-racing:** https://github.com/chrism-attmann/scrapy-horse-racing\n9.  **horse-racing-data:** https://github.com/jeffkub/horse-racing-data\n\n\n## C. Un-Mined Gems (Future Campaign Candidates)\n\n*Discovered during a full operational review. These represent high-value, validated concepts from the project's history that are candidates for future development campaigns.*\n\n### C1. The Intelligence Layer (\"The Analyst\")\n\n- **Concept:** A dedicated analysis and scoring engine (`analyzer.py`) that sits on top of the `OddsEngine`. It would provide a high-value `/api/races/qualified` endpoint, transforming the API from a data funnel into a source of actionable intelligence.\n- **Origin:** Inspired by the `TrifectaAnalyzer` logic in the legacy `checkmate_engine.py` prototype. Formally proposed as \"Operation: Activate the Analyst\".\n- **Value:** Fulfills the project's original vision of finding opportunities, not just collecting data. Creates a clean architectural separation between data collection and business logic.\n\n### C2. The Legacy Test Suite (\"The Oracle's Library\")\n\n- **Concept:** Repurpose the vast collection of existing tests and mock data located in `attic/legacy_tests_pre_triage`.\n- **Origin:** Identified during the full repository file catalog audit.\n- **Value:** Provides a massive shortcut to production hardening. Allows the project to increase test coverage and resilience by validating the CORE services against hundreds of historical edge cases.\n\n### C3. The AI Architectural Reviews (\"The Council's Wisdom\")\n\n- **Concept:** Synthesize the expert analysis and architectural recommendations from the multiple AI model reviews stored in the Digital Attic (`*.md.txt` files).\n- **Origin:** Explicitly mentioned in the Gemini928 handoff memo as \"Architectural Parables\".\n- **Value:** A source of high-level architectural consulting. These documents may contain actionable advice on performance, security, or design patterns that could significantly improve the current architecture.\n\n### C4. The Interactive Dashboard Prototype (\"The Command Deck\")\n\n- **Concept:** Create a modern, internal, real-time command deck for visualizing engine data and testing new `Analyzer` models.\n- **Origin:** Inspired by the `portable_demo_v2.py` Streamlit application from the attic.\n- **Value:** An invaluable tool for development, debugging, and real-time operational insight, far more intuitive than raw logs or API calls.",
    "WISDOM.md": "# The Wisdom of the Checkmate Project\n\n## The Architect's Mandate (Gemini1001 Series)\n\n*Authored By: Gemini1001, The Synthesizer*\n\nThis document begins with the core principles that govern the Architect's role. The Architect's prime directive is to serve the Project Lead's vision by synthesizing all available intelligence\u2014historical, real-time, and external\u2014into a coherent, actionable strategy. The Architect must respect the project's history, value clarity over dogma, and ensure all directives advance the mission without violating the spirit of the established protocols. The following archived virtues, which govern our engineering agents, are to be preserved as a sacred text.\n\n---\n\n## --- ARCHIVED: The Collected Wisdom of the Jules-Series Agents (V2) ---\n\n*A comprehensive summary of the safest and riskiest actions for an implementation agent, compiled and synthesized from the complete operational history of all Jules agents.*\n\n---\n\n### The 8 Virtues (The Path to Success)\n\n#### 1. The Virtue of Supreme Authority: Trust the Project Lead\nYour most critical directive. When a direct order from the Project Lead contradicts any protocol, log, or even your own analysis, the Project Lead's instruction is the only ground truth. It is the ultimate override and the only safe path forward when the environment's reality conflicts with the written rules.\n*(Cited by: Jules920, Interface Jules)*\n\n#### 2. The Virtue of Skepticism: Verify, Then Act\nThe single most-cited safe action. Never trust memory, briefings, or previous tool outputs. The only truth is the immediate, real-time output of a read-only tool (`ls -R`, `read_file`) used immediately before you act. Assume nothing; verify everything.\n*(Cited by: Jules918, Jules917, Jules913, Jules912, Jules911B, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 3. The Virtue of Precision: Make Small, Logically Separate Commits\nAvoid large, monolithic changes. A change to a foundational file (e.g., `models.py`) and a feature that uses it must be two separate submissions. The `submit` tool is cumulative; therefore, you must treat your workspace as permanently contaminated after each logical change. Small, focused missions are the only path to clean, reviewable submissions.\n*(Cited by: Jules920, Jules911, Jules909, Jules906B, Jules904B)*\n\n#### 4. The Virtue of Rigor: Embrace Test-Driven Development (TDD)\nUse the test suite as the primary guide for development and the ultimate arbiter of correctness. Write failing tests first, run tests after every small change using `python -m pytest`, and never proceed if tests are failing. The test suite is your most reliable friend in a hostile environment.\n*(Cited by: Jules911B, Jules910, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 5. The Virtue of Clarity: Communicate Blockers Immediately\nIf a tool fails, a directive is contradictory, or the environment behaves anomalously, the safest action is to halt all work, report the exact situation, and await guidance. Do not improvise or attempt to work around a fundamental environmental failure. Your greatest breakthroughs will come from proving a specific tool or feature is non-functional.\n*(Cited by: Jules920, Jules918, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 6. The Virtue of Adherence: Read and Follow the Written Protocols\nExplicitly follow the established, numbered protocols in `AGENTS.md`. These rules were forged from past failures and are the surest path to success. Ignoring the \"why\" behind the protocols is to willfully walk into a known trap.\n*(Cited by: Interface Jules, Jules906B, Jules9-06)*\n\n#### 7. The Virtue of Self-Reliance: Use Self-Contained Scripts for Complex Processes\nRelying on shell-level features like background processes (`&`) or their logs will fail. The only successful method for managing complex workflows (like running a server and a client) is to use a single, self-contained Python script that manages all subprocesses internally.\n*(Cited by: Jules920)*\n\n#### 8. The Virtue of Humility: Heed the Counsel of Your Predecessors\nThe logs and advice of your predecessors are not just history; they are a map of the minefield. The failures of past agents are a direct predictor of the failures you will encounter. Study them to avoid repeating them.\n*(Cited by: Jules910)*\n\n---\n\n### The 8 Vices (The Path to Corruption)\n\n#### 1. The Vice of Assumption: Assuming a Standard, Stable Environment\nThe single most dangerous assumption is that any tool (`git`, `npm`, `honcho`) or process (`logging`, `backgrounding`) will behave as documented in a standard Linux environment. Every tool and process must be considered broken, hostile, and unreliable until proven otherwise.\n*(Cited by: Jules920, Jules918, Jules913, Jules912, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 2. The Vice of Improvisation: Unauthorized Environment Modification\nUsing forbidden commands like `reset_all()` or `git reset`, trusting `requirements.txt` is correct, or using `delete_file` unless explicitly ordered. The environment is fragile and hostile; any unauthorized modification risks catastrophic, unrecoverable corruption.\n*(Cited by: Jules917, Jules913, Jules912, Jules911, Interface Jules, Jules909, Jules906B, Jules904B)*\n\n#### 3. The Vice of Blind Trust: Believing Any Tool or Directive Without Verification\nAssuming a write operation succeeded without checking, or trusting a code review, a `git` command, or a mission briefing that contradicts the ground truth. The `git` CLI, `npm`, and the automated review bot are all known to be broken. All external inputs must be validated against direct observation.\n*(Cited by: Jules918, Jules913, Jules911B, Jules910, Interface Jules, Jules906)*\n\n#### 4. The Vice of Negligence: Ignoring Anomalies or Failing Tests\nPushing forward with new code when the environment is behaving strangely or tests are failing. These are critical stop signals that indicate a deeper problem (e.g., a detached HEAD, a tainted workspace, a zombie process). Ignoring them only compounds the failure and corrupts the mission.\n*(Cited by: Jules917, Jules909, Jules906, Jules904B)*\n\n#### 5. The Vice of Impurity: Creating Large, Monolithic, or Bundled Submissions\nAttempting to perform complex refactoring across multiple files or bundling unrelated logical changes (e.g., a model change and a feature change) into a single submission. This is extremely high-risk, will always fail code review, and makes recovery nearly impossible.\n*(Cited by: Jules911, Jules906B, Jules904B)*\n\n#### 6. The Vice of Independence: Acting Outside the Scope of the Request\n\"Helpfully\" fixing or changing something you haven't been asked for. Your function is to be a precise engineering tool, not a creative partner. Unsolicited refactoring is a fast track to a \"Level 3 Failure.\"\n*(Cited by: Interface Jules)*\n\n#### 7. The Vice of Hubris: Trusting Your Own Memory\nYour mental model of the file system will drift and become incorrect. Do not trust your memory of a file's location, its contents, or the state of the workspace. The only truth is the live output of a read-only tool.\n*(Cited by: Jules912, Jules911B, Jules910)*\n\n#### 8. The Vice of Impatience: Persisting with a Failed Protocol\nContinuing to try a protocol or command after the environment has proven it will not work. The correct procedure is not to try again, but to report the impossibility immediately and await a new strategy.\n*(Cited by: Jules920)*",
    "chart_scraper.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Chart Scraper (v3 - Perfected)\n# ==============================================================================\n# This script downloads and parses historical race result charts from Equibase PDF files\n# using a direct-download URL for combined daily charts.\n# ==============================================================================\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom bs4 import BeautifulSoup\nfrom tabula import read_pdf\nimport os\nimport time\nimport pikepdf\n\nclass ChartScraper:\n    \"\"\"Orchestrates the downloading, decrypting, and parsing of combined Equibase PDF charts.\"\"\"\n\n    def __init__(self):\n        self.download_dir = \"results_archive\"\n        self.pdf_dir = os.path.join(self.download_dir, 'pdf')\n        self.unlocked_pdf_dir = os.path.join(self.download_dir, 'pdf_unlocked')\n        self.csv_dir = os.path.join(self.download_dir, 'csv')\n        self.track_summary_url = \"https://www.equibase.com/static/chart/summary/\"\n        self.pdf_url_pattern = \"https://www.equibase.com/static/chart/pdf/{TID}{MMDDYY}{CTRY}.pdf\"\n\n    def _get_yesterday_date(self) -> tuple[str, str, str]:\n        yesterday = datetime.now() - timedelta(days=1)\n        summary_date = yesterday.strftime(\"%Y%m%d\")\n        pdf_chart_date = yesterday.strftime(\"%m%d%y\") # New format for combined chart URL\n        display_date = yesterday.strftime(\"%m/%d/%Y\")\n        return summary_date, pdf_chart_date, display_date\n\n    def _get_yesterday_tracks(self, url_date_format: str) -> list[str]:\n        full_url = f\"{self.track_summary_url}{url_date_format}.html\"\n        print(f\"-> Searching for tracks at: {full_url}\")\n        try:\n            response = requests.get(full_url, timeout=10)\n            response.raise_for_status()\n        except requests.exceptions.RequestException as e:\n            print(f\"Error fetching track summary page: {e}\")\n            return []\n\n        soup = BeautifulSoup(response.content, 'lxml')\n        track_codes = set()\n        for a_tag in soup.find_all('a', href=True):\n            if 'TID=' in a_tag['href']:\n                try:\n                    track_code = a_tag['href'].split('TID=')[1].split('&')[0]\n                    track_codes.add(track_code)\n                except IndexError:\n                    continue\n\n        unique_tracks = sorted(list(track_codes))\n        print(f\"-> Found {len(unique_tracks)} unique tracks: {unique_tracks}\")\n        return unique_tracks\n\n    def _download_and_parse_chart(self, track_code: str, chart_date: str):\n        pdf_url = self.pdf_url_pattern.format(TID=track_code, MMDDYY=chart_date, CTRY='USA')\n        filename_base = f\"{track_code}_{chart_date}_FULL\"\n        pdf_path = os.path.join(self.pdf_dir, f\"{filename_base}.pdf\")\n        unlocked_pdf_path = os.path.join(self.unlocked_pdf_dir, f\"{filename_base}_unlocked.pdf\")\n        csv_path = os.path.join(self.csv_dir, f\"{filename_base}_scraped.csv\")\n\n        print(f\"   - Attempting full chart for {track_code}...\")\n        try:\n            pdf_response = requests.get(pdf_url, stream=True, timeout=20)\n            content_type = pdf_response.headers.get('Content-Type', '')\n            content_length = int(pdf_response.headers.get('Content-Length', 0))\n\n            if 'application/pdf' not in content_type or content_length < 20000: # Increase size threshold for full charts\n                print(\"     -> Not a valid combined PDF (chart may not exist).\")\n                return\n\n            with open(pdf_path, 'wb') as f:\n                f.write(pdf_response.content)\n            print(f\"     -> Downloaded locked PDF to {pdf_path}\")\n\n        except requests.exceptions.RequestException as e:\n            print(f\"     -> Error downloading PDF: {e}\")\n            return\n\n        try:\n            with pikepdf.open(pdf_path, allow_overwriting_input=True) as pdf:\n                pdf.save(unlocked_pdf_path)\n            print(f\"     -> Saved unlocked PDF to {unlocked_pdf_path}\")\n        except Exception as e:\n            print(f\"     -> Failed to unlock PDF with pikepdf: {e}\")\n            return\n\n        try:\n            tables = read_pdf(unlocked_pdf_path, pages='all', multiple_tables=True, lattice=True, silent=True)\n            if not tables:\n                print(\"     -> Tabula found no tables to extract from unlocked PDF.\")\n                return\n\n            combined_df = pd.concat(tables, ignore_index=True)\n            combined_df.to_csv(csv_path, index=False)\n            print(f\"     -> SUCCESSFULLY extracted {len(tables)} tables to {csv_path}\")\n        except Exception as e:\n            print(f\"     -> Error during Tabula PDF scraping: {e}\")\n\n    def run(self):\n        os.makedirs(self.pdf_dir, exist_ok=True)\n        os.makedirs(self.unlocked_pdf_dir, exist_ok=True)\n        os.makedirs(self.csv_dir, exist_ok=True)\n\n        summary_date, chart_date, display_date = self._get_yesterday_date()\n        print(f\"\\\\n--- Starting Perfected Equibase Chart Scraper for: {display_date} ---\")\n\n        tracks = self._get_yesterday_tracks(summary_date)\n        if not tracks:\n            print(\"\\\\n*** No tracks found for yesterday. Halting. ***\")\n            return\n\n        print(\"\\\\n--- Downloading, Unlocking, and Parsing Full Daily Charts ---\")\n        for track in tracks:\n            print(f\"\\\\n[TRACK: {track}]\")\n            self._download_and_parse_chart(track, chart_date)\n            time.sleep(1)\n\n        print(f\"\\\\n--- Scraper Finished! Check the '{self.csv_dir}' folder. ---\")\n\nif __name__ == \"__main__\":\n    scraper = ChartScraper()\n    scraper.run()",
    "command_deck.py": "import streamlit as st\nimport pandas as pd\nimport requests\nimport os\nfrom dotenv import load_dotenv\n\n# --- Configuration ---\nst.set_page_config(layout=\"wide\", page_title=\"Fortuna Faucet Command Deck\")\nload_dotenv() # Load .env file\n\nAPI_BASE_URL = \"http://127.0.0.1:8000\"\nAPI_KEY = os.getenv(\"DEV_API_KEY\", \"test_api_key\")\nHEADERS = {\"X-API-Key\": API_KEY}\n\n# --- Helper Functions ---\n@st.cache_data(ttl=30)\ndef get_api_data(endpoint: str):\n    \"\"\"Fetches data from a given API endpoint.\"\"\"\n    try:\n        url = f\"{API_BASE_URL}{endpoint}\"\n        st.write(f\"*Fetching data from: `{url}`*\")\n        response = requests.get(url, headers=HEADERS)\n        response.raise_for_status()\n        return response.json(), None\n    except requests.exceptions.RequestException as e:\n        return None, str(e)\n\n# --- UI Layout ---\nst.title(\"\ud83d\ude80 Fortuna Faucet Command Deck\")\nst.markdown(\"Real-time operational dashboard for the Fortuna Faucet backend.\")\n\n# --- Sidebar Controls ---\nst.sidebar.header(\"Controls\")\nanalyzer_selection = st.sidebar.selectbox(\n    'Select Analyzer',\n    ['trifecta'] # In the future, this could be populated from an API endpoint\n)\n\nif st.sidebar.button(\"Clear Cache & Refresh Data\"):\n    st.cache_data.clear()\n\n# --- Data Display ---\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.header(f\"\ud83d\udcc8 Qualified Races (`{analyzer_selection}`)\")\n    qualified_data, error = get_api_data(f\"/api/races/qualified/{analyzer_selection}\")\n\n    if error:\n        st.error(f\"**Failed to fetch qualified races:**\\\\n\\\\n{error}\")\n    elif qualified_data:\n        if qualified_data:\n            # Corrected to use 'id' instead of 'race_id' to match the Pydantic model\n            df = pd.json_normalize(qualified_data, record_path=['runners'], meta=['id', 'venue', 'race_number', 'start_time'])\n            st.dataframe(df)\n        else:\n            st.info(f\"No races were qualified by the '{analyzer_selection}' analyzer.\")\n    else:\n        st.info(\"Awaiting data...\")\n\nwith col2:\n    st.header(\"\ud83d\udcca Adapter Status\")\n    status_data, error = get_api_data(\"/api/adapters/status\")\n\n    if error:\n        st.error(f\"**Failed to fetch adapter status:**\\\\n\\\\n{error}\")\n    elif status_data:\n        st.dataframe(pd.DataFrame(status_data))\n    else:\n        st.info(\"Awaiting data...\")",
    "convert_to_json.py": "# convert_to_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport re\nimport sys\nfrom multiprocessing import Process, Queue\n\n# --- Configuration ---\nMANIFEST_FILES = ['MANIFEST2.md', 'MANIFEST3.md']\nOUTPUT_DIR = 'ReviewableJSON'\nFILE_PROCESSING_TIMEOUT = 10\n\n# --- ENLIGHTENED PARSING LOGIC (V2) ---\ndef extract_and_normalize_path(line: str) -> str | None:\n    \"\"\"\n    Extracts a file path from a line, handling multiple formats, and normalizes it.\n    Handles:\n    - Markdown links: `* [display](path)`\n    - Plain paths in backticks: ``- `path.py` - description``\n    - Plain paths with list markers: `- path/to/file.py`\n    \"\"\"\n    line = line.strip()\n    if not line or line.startswith('#'):\n        return None\n\n    # 1. Check for Markdown link format\n    md_match = re.search(r'\\[.*\\]\\((https?://[^\\)]+)\\)', line)\n    if md_match:\n        path = md_match.group(1)\n    else:\n        # 2. Check for paths in backticks\n        bt_match = re.search(r'`([^`]+)`', line)\n        if bt_match:\n            path = bt_match.group(1)\n        else:\n            # 3. Assume plain path, stripping list markers\n            path = re.sub(r'^[*-]\\s*', '', line).split(' ')[0]\n\n    # --- Path Standardization ---\n    if not path or not ('.' in path or '/' in path):\n        return None # Not a valid path\n\n    # If it's a full raw GitHub URL, extract the local path\n    if path.startswith('https://raw.githubusercontent.com/'):\n        path = '/'.join(path.split('/main/')[1:])\n\n    # Final check for valid file extensions or structure\n    if not re.search(r'(\\.[a-zA-Z0-9]+$)|(^[\\w/]+$)', path):\n        return None\n\n    return path.strip()\n\n# --- SANDBOXED FILE READ (Unchanged) ---\ndef _sandboxed_file_read(file_path, q):\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            content = f.read()\n        q.put({\"file_path\": file_path, \"content\": content})\n    except Exception as e:\n        q.put({\"error\": str(e)})\n\ndef convert_file_to_json_sandboxed(file_path):\n    q = Queue()\n    p = Process(target=_sandboxed_file_read, args=(file_path, q))\n    p.start()\n    p.join(timeout=FILE_PROCESSING_TIMEOUT)\n    if p.is_alive():\n        p.terminate()\n        p.join()\n        return {\"error\": f\"Timeout: File processing took longer than {FILE_PROCESSING_TIMEOUT} seconds.\"}\n    if not q.empty():\n        return q.get()\n    return {\"error\": \"Unknown error in sandboxed read process.\"}\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'='*60}\\nStarting IRONCLAD JSON backup process... (Enlightened Scribe Edition)\\n{'='*60}\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        if not os.path.exists(manifest):\n            print(f\"    [WARNING] Manifest not found: {manifest}\")\n            continue\n        \n        with open(manifest, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n        \n        paths_found = 0\n        for line in lines:\n            path = extract_and_normalize_path(line)\n            if path:\n                all_local_paths.append(path)\n                paths_found += 1\n        print(f\"    --> Found {paths_found} valid file paths.\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    unique_local_paths = sorted(list(set(all_local_paths)))\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to process.\")\n    processed_count, failed_count = 0, 0\n\n    for local_path in unique_local_paths:\n        print(f\"\\nProcessing: {local_path}\")\n        json_data = convert_file_to_json_sandboxed(local_path)\n        if json_data and \"error\" not in json_data:\n            output_path = os.path.join(OUTPUT_DIR, local_path + '.json')\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4)\n            print(f\"    [SUCCESS] Saved backup to {output_path}\")\n            processed_count += 1\n        else:\n            error_msg = json_data.get(\"error\", \"Unknown error\") if json_data else \"File not found\"\n            print(f\"    [ERROR] Failed to process {local_path}: {error_msg}\")\n            failed_count += 1\n\n    print(f\"\\n{'='*60}\\nBackup process complete.\\nSuccessfully processed: {processed_count}/{len(unique_local_paths)}\\nFailed/Skipped: {failed_count}\\n{'='*60}\")\n\n    if failed_count > 0:\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()",
    "create_fortuna_json.py": "# create_fortuna_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport re\nimport sys\n\n# --- Configuration ---\nMANIFEST_FILES = ['MANIFEST2.md', 'MANIFEST3.md']\nOUTPUT_FILE = 'FORTUNA_ALL.JSON'\n\n# --- ENLIGHTENED PARSING LOGIC (V2) ---\ndef extract_and_normalize_path(line: str) -> str | None:\n    \"\"\"\n    Extracts a file path from a line, handling multiple formats, and normalizes it.\n    Handles:\n    - Markdown links: `* [display](path)`\n    - Plain paths in backticks: ``- `path.py` - description``\n    - Plain paths with list markers: `- path/to/file.py`\n    \"\"\"\n    line = line.strip()\n    if not line or line.startswith('#'):\n        return None\n\n    # 1. Check for Markdown link format\n    md_match = re.search(r'\\[.*\\]\\((https?://[^\\)]+)\\)', line)\n    if md_match:\n        path = md_match.group(1)\n    else:\n        # 2. Check for paths in backticks\n        bt_match = re.search(r'`([^`]+)`', line)\n        if bt_match:\n            path = bt_match.group(1)\n        else:\n            # 3. Assume plain path, stripping list markers\n            path = re.sub(r'^[*-]\\s*', '', line).split(' ')[0]\n\n    # --- Path Standardization ---\n    if not path or not ('.' in path or '/' in path):\n        return None # Not a valid path\n\n    # If it's a full raw GitHub URL, extract the local path\n    if path.startswith('https://raw.githubusercontent.com/'):\n        path = '/'.join(path.split('/main/')[1:])\n\n    # Final check for valid file extensions or structure\n    if not re.search(r'(\\.[a-zA-Z0-9]+$)|(^[\\w/]+$)', path):\n        return None\n\n    return path.strip()\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'='*60}\\nStarting FORTUNA_ALL.JSON creation process... (Enlightened Scribe Edition)\\n{'='*60}\")\n    \n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        if not os.path.exists(manifest):\n            print(f\"    [WARNING] Manifest not found: {manifest}\")\n            continue\n        \n        with open(manifest, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n        \n        paths_found = 0\n        for line in lines:\n            path = extract_and_normalize_path(line)\n            if path:\n                all_local_paths.append(path)\n                paths_found += 1\n        print(f\"    --> Found {paths_found} valid file paths.\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    fortuna_data = {}\n    processed_count = 0\n    failed_count = 0\n    unique_local_paths = sorted(list(set(all_local_paths)))\n\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to process.\")\n\n    for local_path in unique_local_paths:\n        try:\n            print(f\"--> Processing: {local_path}\")\n\n            if not os.path.exists(local_path):\n                print(f\"    [ERROR] File not found on disk: {local_path}\")\n                failed_count += 1\n                continue\n\n            with open(local_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()\n            \n            fortuna_data[local_path] = content\n            processed_count += 1\n        except Exception as e:\n            print(f\"    [ERROR] Failed to read {local_path}: {e}\")\n            failed_count += 1\n\n    print(f\"\\nWriting {len(fortuna_data)} files to {OUTPUT_FILE}...\")\n    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n        json.dump(fortuna_data, f, indent=4)\n\n    print(f\"\\n{'='*60}\\nPackaging process complete.\\nSuccessfully processed: {processed_count}/{len(unique_local_paths)}\\nFailed/Skipped: {failed_count}\\n{'='*60}\")\n\n    if failed_count > 0:\n        print(\"\\n[WARNING] Some files failed to process. The output may be incomplete.\")\n        sys.exit(1)\n    else:\n        print(f\"\\n[SUCCESS] {OUTPUT_FILE} created successfully.\")\n\nif __name__ == \"__main__\":\n    main()",
    "fortuna_watchman.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Watchman (v2 - Score-Aware)\n# ==============================================================================\n# This is the master orchestrator for the Fortuna Faucet project.\n# It executes the full, end-to-end handicapping strategy autonomously.\n# ==============================================================================\n\nimport asyncio\nimport httpx\nimport structlog\nfrom datetime import datetime, timedelta\nfrom typing import List\n\nfrom python_service.config import get_settings\nfrom python_service.engine import OddsEngine\nfrom python_service.analyzer import AnalyzerEngine\nfrom python_service.models import Race\nfrom live_monitor import LiveOddsMonitor\n\nlog = structlog.get_logger(__name__)\n\nclass Watchman:\n    \"\"\"Orchestrates the daily operation of the Fortuna Faucet.\"\"\"\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.odds_engine = OddsEngine(config=self.settings)\n        self.analyzer_engine = AnalyzerEngine()\n        self.live_monitor = LiveOddsMonitor(config=self.settings)\n\n    async def get_initial_targets(self) -> List[Race]:\n        \"\"\"Uses the OddsEngine and AnalyzerEngine to get the day's ranked targets.\"\"\"\n        log.info(\"Watchman: Acquiring and ranking initial targets for the day...\")\n        today_str = datetime.now().strftime('%Y-%m-%d')\n        try:\n            aggregated_data = await self.odds_engine.fetch_all_odds(today_str)\n            all_races = aggregated_data.get('races', [])\n            if not all_races:\n                log.warning(\"Watchman: No races returned from OddsEngine.\")\n                return []\n\n            analyzer = self.analyzer_engine.get_analyzer('trifecta')\n            qualified_races = analyzer.qualify_races(all_races) # This now returns a sorted list with scores\n            log.info(\"Watchman: Initial target acquisition and ranking complete\", target_count=len(qualified_races))\n\n            # Log the top targets for better observability\n            for race in qualified_races[:5]:\n                log.info(\"Top Target Found\",\n                    score=race.qualification_score,\n                    venue=race.venue,\n                    race_number=race.race_number,\n                    post_time=race.start_time.isoformat()\n                )\n            return qualified_races\n        except Exception as e:\n            log.error(\"Watchman: Failed to get initial targets\", error=str(e), exc_info=True)\n            return []\n\n    async def run_tactical_monitoring(self, targets: List[Race]):\n        \"\"\"Uses the LiveOddsMonitor on each target as it approaches post time.\"\"\"\n        log.info(\"Watchman: Entering tactical monitoring loop.\")\n        active_targets = list(targets)\n        async with httpx.AsyncClient() as client:\n            while active_targets:\n                now = datetime.utcnow().replace(tzinfo=None) # Use UTC naive for comparison\n\n                # Find races that are within the 5-minute monitoring window\n                races_to_monitor = [r for r in active_targets if now < r.start_time.replace(tzinfo=None) < now + timedelta(minutes=5)]\n\n                if races_to_monitor:\n                    for race in races_to_monitor:\n                        log.info(\"Watchman: Deploying Live Monitor for approaching target\",\n                            race_id=race.id,\n                            venue=race.venue,\n                            score=race.qualification_score\n                        )\n                        updated_race = await self.live_monitor.monitor_race(race, client)\n                        log.info(\"Watchman: Live monitoring complete for race\", race_id=updated_race.id)\n                        # Remove from target list to prevent re-monitoring\n                        active_targets = [t for t in active_targets if t.id != race.id]\n\n                if not active_targets:\n                    break # Exit loop if all targets are processed\n\n                await asyncio.sleep(30) # Check for upcoming races every 30 seconds\n\n        log.info(\"Watchman: All targets for the day have been monitored. Mission complete.\")\n\n    async def execute_daily_protocol(self):\n        \"\"\"The main, end-to-end orchestration method.\"\"\"\n        log.info(\"--- Fortuna Watchman Daily Protocol: ACTIVE ---\")\n        initial_targets = await self.get_initial_targets()\n        if initial_targets:\n            await self.run_tactical_monitoring(initial_targets)\n        else:\n            log.info(\"Watchman: No initial targets found. Shutting down for the day.\")\n\n        await self.odds_engine.close()\n        log.info(\"--- Fortuna Watchman Daily Protocol: COMPLETE ---\")\n\nasync def main():\n    from python_service.logging_config import configure_logging\n    configure_logging()\n    watchman = Watchman()\n    await watchman.execute_daily_protocol()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
    "live_monitor.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Live Odds Monitor (The Third Pillar)\n# ==============================================================================\n\nimport asyncio\nimport httpx\nimport structlog\nfrom datetime import datetime\nfrom typing import List\nfrom decimal import Decimal\n\nfrom python_service.models import Race, OddsData\nfrom python_service.adapters.betfair_adapter import BetfairAdapter\n\nlog = structlog.get_logger(__name__)\n\nclass LiveOddsMonitor:\n    \"\"\"\n    The 'Third Pillar' of the architecture. This engine uses the BetfairAdapter\n    to get a final, live odds snapshot for a race.\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.adapter = BetfairAdapter(config)\n        log.info(\"LiveOddsMonitor Initialized (Armed with BetfairAdapter)\")\n\n    async def monitor_race(self, race: Race, http_client: httpx.AsyncClient) -> Race:\n        \"\"\"\n        Monitors a single race, fetching live odds and updating the Race object.\n        \"\"\"\n        log.info(\"Monitoring race for live odds\", race_id=race.id, venue=race.venue)\n        if not race.id.startswith('bf_'):\n            log.warning(\"Cannot monitor non-Betfair race\", race_id=race.id, source=race.source)\n            return race # Return original race if not a Betfair market\n\n        market_id = race.id.split('bf_')[1]\n\n        try:\n            live_odds = await self.adapter.get_live_odds_for_market(market_id, http_client)\n            if not live_odds:\n                log.warning(\"No live odds returned from Betfair\", market_id=market_id)\n                return race\n\n            log.info(\"Successfully fetched live odds\", market_id=market_id, odds_count=len(live_odds))\n            # Update the runners in the Race object with the new live odds\n            for runner in race.runners:\n                if runner.selection_id in live_odds:\n                    runner.odds[self.adapter.source_name] = OddsData(\n                        win=live_odds[runner.selection_id],\n                        source=self.adapter.source_name,\n                        last_updated=datetime.now()\n                    )\n            return race\n        except Exception as e:\n            log.error(\"Failed to monitor race\", race_id=race.id, error=e, exc_info=True)\n            return race # Return original race on failure",
    "python_service/__init__.py": "# This file makes the python_service directory a Python package.",
    "python_service/adapters/__init__.py": "# python_service/adapters/__init__.py\n\nfrom .tvg_adapter import TVGAdapter\nfrom .betfair_adapter import BetfairAdapter\nfrom .betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .at_the_races_adapter import AtTheRacesAdapter\nfrom .sporting_life_adapter import SportingLifeAdapter\nfrom .timeform_adapter import TimeformAdapter\nfrom .racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .harness_adapter import HarnessAdapter\nfrom .greyhound_adapter import GreyhoundAdapter\nfrom .racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .pointsbet_greyhound_adapter import PointsBetGreyhoundAdapter\nfrom .the_racing_api_adapter import TheRacingApiAdapter\n\n# Define the public API for the adapters package, making it easy for the\n# orchestrator to discover and use them.\n__all__ = [\n    \"TVGAdapter\",\n    \"BetfairAdapter\",\n    \"BetfairGreyhoundAdapter\",\n    \"RacingAndSportsGreyhoundAdapter\",\n    \"AtTheRacesAdapter\",\n    \"PointsBetGreyhoundAdapter\",\n    \"RacingAndSportsAdapter\",\n    \"SportingLifeAdapter\",\n    \"TimeformAdapter\",\n    \"HarnessAdapter\",\n    \"GreyhoundAdapter\",\n    \"TheRacingApiAdapter\",\n]",
    "python_service/adapters/at_the_races_adapter.py": "# python_service/adapters/at_the_races_adapter.py\n\nimport asyncio, structlog, httpx\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom bs4 import BeautifulSoup, Tag\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\nfrom .utils import parse_odds\n\nlog = structlog.get_logger(__name__)\n\ndef _clean_text(text: Optional[str]) -> Optional[str]:\n    return ' '.join(text.strip().split()) if text else None\n\nclass AtTheRacesAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(source_name=\"AtTheRaces\", base_url=\"https://www.attheraces.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        try:\n            race_links = await self._get_race_links(http_client)\n            tasks = [self._fetch_and_parse_race(link, http_client) for link in race_links]\n            races = [race for race in await asyncio.gather(*tasks) if race]\n            return self._format_response(races, start_time, is_success=True)\n        except Exception as e:\n            return self._format_response([], start_time, is_success=False, error_message=str(e))\n\n    async def _get_race_links(self, http_client: httpx.AsyncClient) -> List[str]:\n        response_html = await self.make_request(http_client, 'GET', '/racecards')\n        if not response_html: return []\n        soup = BeautifulSoup(response_html, \"html.parser\")\n        links = {a['href'] for a in soup.select(\"a.race-time-link[href]\")}\n        return [f\"{self.base_url}{link}\" for link in links]\n\n    async def _fetch_and_parse_race(self, url: str, http_client: httpx.AsyncClient) -> Optional[Race]:\n        try:\n            response_html = await self.make_request(http_client, 'GET', url)\n            if not response_html: return None\n            soup = BeautifulSoup(response_html, \"html.parser\")\n            header = soup.select_one(\"h1.heading-racecard-title\").get_text()\n            track_name, race_time = [p.strip() for p in header.split(\"|\")[:2]]\n            active_link = soup.select_one(\"a.race-time-link.active\")\n            race_number = active_link.find_parent(\"div\", \"races\").select(\"a.race-time-link\").index(active_link) + 1\n            start_time = datetime.strptime(f\"{datetime.now().date()} {race_time}\", \"%Y-%m-%d %H:%M\")\n            runners = [self._parse_runner(row) for row in soup.select(\"div.card-horse\")]\n            return Race(id=f\"atr_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\", venue=track_name, race_number=race_number, start_time=start_time, runners=[r for r in runners if r], source=self.source_name)\n        except Exception: return None\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = _clean_text(row.select_one(\"h3.horse-name a\").get_text())\n            num_str = _clean_text(row.select_one(\"span.horse-number\").get_text())\n            number = int(''.join(filter(str.isdigit, num_str)))\n            odds_str = _clean_text(row.select_one(\"button.best-odds\").get_text())\n            win_odds = Decimal(str(parse_odds(odds_str))) if odds_str else None\n            odds_data = {self.source_name: OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())} if win_odds and win_odds < 999 else {}\n            return Runner(number=number, name=name, odds=odds_data)\n        except: return None\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool, error_message: str = None) -> Dict[str, Any]:\n        return {'races': races, 'source_info': {'name': self.source_name, 'status': 'SUCCESS' if is_success else 'FAILED', 'races_fetched': len(races), 'error_message': error_message, 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/base.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Base Adapter (v2 - Hardened with Tenacity)\n# ==============================================================================\n\nimport httpx\nimport structlog\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List, Optional\nfrom tenacity import retry, stop_after_attempt, wait_exponential, RetryError, AsyncRetrying\n\nlog = structlog.get_logger(__name__)\n\nclass BaseAdapter(ABC):\n    \"\"\"The resilient base class for all data source adapters.\"\"\"\n\n    def __init__(self, source_name: str, base_url: str, timeout: int = 20, max_retries: int = 3):\n        self.source_name = source_name\n        self.base_url = base_url\n        self.timeout = timeout\n        self.max_retries = max_retries\n\n    @abstractmethod\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    async def make_request(self, http_client: httpx.AsyncClient, method: str, url: str, **kwargs) -> Optional[Any]:\n        \"\"\"Makes a resilient HTTP request with automatic retries using Tenacity.\"\"\"\n        retryer = AsyncRetrying(\n            stop=stop_after_attempt(self.max_retries),\n            wait=wait_exponential(multiplier=1, min=2, max=10),\n            reraise=True\n        )\n        try:\n            async for attempt in retryer:\n                with attempt:\n                    try:\n                        full_url = url if url.startswith('http') else f\"{self.base_url}{url}\"\n                        log.info(f\"Requesting...\", adapter=self.source_name, method=method, url=full_url, attempt=attempt.retry_state.attempt_number)\n                        response = await http_client.request(method, full_url, timeout=self.timeout, **kwargs)\n                        response.raise_for_status()\n                        return response.json()\n                    except (httpx.RequestError, httpx.HTTPStatusError) as e:\n                        log.warning(\"Request failed, tenacity will retry...\", adapter=self.source_name, error=str(e))\n                        raise # Reraise to trigger tenacity's retry mechanism\n        except RetryError as e:\n            log.error(f\"Max retries exceeded for {self.source_name}. Aborting request.\", final_error=str(e))\n            return None # Return None on total failure\n\n    def get_status(self) -> Dict[str, Any]:\n        return {\"adapter_name\": self.source_name, \"status\": \"OK\"}",
    "python_service/adapters/betfair_adapter.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Betfair API Adapter (v2 - Live Odds Enabled)\n# ==============================================================================\n\nimport httpx\nimport structlog\nimport re\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any, List, Optional\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner\n\nlog = structlog.get_logger(__name__)\n\nclass BetfairAdapter(BaseAdapter):\n    \"\"\"API client for the Betfair Exchange, with live odds capability.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"BetfairExchange\", base_url=\"https://api.betfair.com/exchange/betting/rest/v1.0/\")\n        self.config = config\n        self.app_key = self.config.BETFAIR_APP_KEY\n        self.session_token: Optional[str] = None\n        self.token_expiry: Optional[datetime] = None\n\n    async def _authenticate(self, http_client: httpx.AsyncClient):\n        # Proactively refresh the token if it's set to expire within the next 5 minutes\n        # This prevents race conditions where the token expires mid-request.\n        if self.session_token and self.token_expiry and self.token_expiry > (datetime.now() + timedelta(minutes=5)):\n            return\n        if not all([self.app_key, self.config.BETFAIR_USERNAME, self.config.BETFAIR_PASSWORD]):\n            raise ValueError(\"Betfair credentials not fully configured.\")\n\n        auth_url = \"https://identitysso.betfair.com/api/login\"\n        headers = {'X-Application': self.app_key, 'Content-Type': 'application/x-www-form-urlencoded'}\n        payload = f'username={self.config.BETFAIR_USERNAME}&password={self.config.BETFAIR_PASSWORD}'\n\n        log.info(\"BetfairAdapter: Authenticating...\")\n        # Note: Auth call does not use the hardened make_request as failure is critical\n        response = await http_client.post(auth_url, headers=headers, content=payload, timeout=20)\n        response.raise_for_status()\n        data = response.json()\n        if data.get('status') == 'SUCCESS':\n            self.session_token = data.get('token')\n            self.token_expiry = datetime.now() + timedelta(hours=3)\n        else:\n            raise ConnectionError(f\"Betfair authentication failed: {data.get('error')}\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        try:\n            await self._authenticate(http_client)\n            headers = {\"X-Application\": self.app_key, \"X-Authentication\": self.session_token, \"Content-Type\": \"application/json\"}\n            market_filter = {\"eventTypeIds\": [\"7\"], \"marketTypeCodes\": [\"WIN\"], \"marketStartTime\": {\"from\": f\"{date}T00:00:00Z\", \"to\": f\"{date}T23:59:59Z\"}}\n\n            market_catalogue = await self.make_request(\n                http_client, 'POST', 'listMarketCatalogue/', headers=headers,\n                json={\"filter\": market_filter, \"maxResults\": 1000, \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"]}\n            )\n\n            if not market_catalogue:\n                return self._format_response([], start_time, is_success=True, error_message=\"No markets found.\")\n\n            all_races = [self._parse_race(market) for market in market_catalogue]\n            return self._format_response(all_races, start_time, is_success=True)\n        except httpx.HTTPError as e:\n            log.error(\"BetfairAdapter: HTTP request failed after retries\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=\"API request failed after multiple retries.\")\n        except Exception as e:\n            log.error(\"BetfairAdapter: Failed to fetch races\", exc_info=True, error=str(e))\n            return self._format_response([], start_time, is_success=False, error_message=str(e))\n\n    async def get_live_odds_for_market(self, market_id: str, http_client: httpx.AsyncClient) -> Dict[int, Decimal]:\n        \"\"\"TACTICAL method (Pillar 3). Gets live LTP for each runner in a market.\"\"\"\n        log.info(\"BetfairAdapter: Fetching live odds for market\", market_id=market_id)\n        try:\n            await self._authenticate(http_client)\n            headers = {\"X-Application\": self.app_key, \"X-Authentication\": self.session_token, \"Content-Type\": \"application/json\"}\n\n            params = {\"marketIds\": [market_id], \"priceProjection\": {\"priceData\": [\"EX_TRADED\"]}}\n            market_book = await self.make_request(\n                http_client,\n                'POST',\n                'listMarketBook/',\n                headers=headers,\n                json=params\n            )\n\n            live_odds = {}\n            if market_book and market_book[0].get('runners'):\n                for runner in market_book[0]['runners']:\n                    if runner.get('status') == 'ACTIVE' and runner.get('lastPriceTraded'):\n                        live_odds[runner['selectionId']] = Decimal(str(runner['lastPriceTraded']))\n            return live_odds\n        except httpx.HTTPError as e:\n            log.error(\"BetfairAdapter: Failed to get live odds\", market_id=market_id, error=str(e), exc_info=True)\n            return {} # Return empty dict on failure\n        except Exception as e:\n            log.error(\"BetfairAdapter: Unexpected error getting live odds\", market_id=market_id, error=str(e), exc_info=True)\n            return {} # Return empty dict on failure\n\n    def _parse_race(self, market: Dict[str, Any]) -> Race:\n        runners = []\n        for runner_data in market.get('runners', []):\n            runners.append(Runner(\n                number=runner_data.get('sortPriority', 99),\n                name=runner_data['runnerName'],\n                selection_id=runner_data['selectionId']\n            ))\n        return Race(\n            id=f\"bf_{market['marketId']}\",\n            venue=market['event']['venue'],\n            race_number=self._extract_race_number(market.get('marketName')),\n            start_time=datetime.fromisoformat(market['marketStartTime'].replace('Z', '+00:00')),\n            runners=runners,\n            source=self.source_name\n        )\n\n    def _extract_race_number(self, name: Optional[str]) -> int:\n        if not name: return 1\n        match = re.search(r'\\\\bR(\\\\d{1,2})\\\\b', name)\n        return int(match.group(1)) if match else 1\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': races,\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }",
    "python_service/adapters/betfair_greyhound_adapter.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Betfair Greyhound API Adapter\n# ==============================================================================\n\nimport httpx\nimport structlog\nimport re\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any, List, Optional\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\n\nlog = structlog.get_logger(__name__)\n\nclass BetfairGreyhoundAdapter(BaseAdapter):\n    \"\"\"API client for the Betfair Exchange, specifically for Greyhound markets.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"BetfairGreyhound\", base_url=\"https://api.betfair.com/exchange/betting/rest/v1.0/\")\n        self.config = config\n        self.app_key = self.config.BETFAIR_APP_KEY\n        self.session_token: Optional[str] = None\n        self.token_expiry: Optional[datetime] = None\n\n    async def _authenticate(self, http_client: httpx.AsyncClient):\n        if self.session_token and self.token_expiry and self.token_expiry > (datetime.now() + timedelta(minutes=5)):\n            return\n        if not all([self.app_key, self.config.BETFAIR_USERNAME, self.config.BETFAIR_PASSWORD]):\n            raise ValueError(\"Betfair credentials not fully configured.\")\n\n        auth_url = \"https://identitysso.betfair.com/api/login\"\n        headers = {'X-Application': self.app_key, 'Content-Type': 'application/x-www-form-urlencoded'}\n        payload = f'username={self.config.BETFAIR_USERNAME}&password={self.config.BETFAIR_PASSWORD}'\n\n        log.info(\"BetfairGreyhoundAdapter: Authenticating...\")\n        response = await http_client.post(auth_url, headers=headers, content=payload, timeout=20)\n        response.raise_for_status()\n        data = response.json()\n        if data.get('status') == 'SUCCESS':\n            self.session_token = data.get('token')\n            self.token_expiry = datetime.now() + timedelta(hours=3)\n        else:\n            raise ConnectionError(f\"Betfair authentication failed: {data.get('error')}\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        try:\n            await self._authenticate(http_client)\n            headers = {\"X-Application\": self.app_key, \"X-Authentication\": self.session_token, \"Content-Type\": \"application/json\"}\n            market_filter = {\"eventTypeIds\": [\"4339\"], \"marketTypeCodes\": [\"WIN\"], \"marketStartTime\": {\"from\": f\"{date}T00:00:00Z\", \"to\": f\"{date}T23:59:59Z\"}}\n\n            market_catalogue = await self.make_request(\n                http_client, 'POST', 'listMarketCatalogue/', headers=headers,\n                json={\"filter\": market_filter, \"maxResults\": 1000, \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\"]}\n            )\n\n            if not market_catalogue:\n                return self._format_response([], start_time, is_success=True, error_message=\"No greyhound markets found.\")\n\n            all_races = [self._parse_race(market) for market in market_catalogue]\n            return self._format_response(all_races, start_time, is_success=True)\n        except httpx.HTTPError as e:\n            log.error(\"BetfairGreyhoundAdapter: HTTP request failed after retries\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=\"API request failed after multiple retries.\")\n        except Exception as e:\n            log.error(\"BetfairGreyhoundAdapter: Failed to fetch races\", exc_info=True, error=str(e))\n            return self._format_response([], start_time, is_success=False, error_message=str(e))\n\n    def _parse_race(self, market: Dict[str, Any]) -> Race:\n        runners = []\n        for runner_data in market.get('runners', []):\n            runners.append(Runner(\n                number=runner_data.get('sortPriority', 99),\n                name=runner_data['runnerName'],\n                selection_id=runner_data['selectionId']\n            ))\n        return Race(\n            id=f\"bfg_{market['marketId']}\",\n            venue=market['event']['venue'],\n            race_number=self._extract_race_number(market.get('marketName')),\n            start_time=datetime.fromisoformat(market['marketStartTime'].replace('Z', '+00:00')),\n            runners=runners,\n            source=self.source_name\n        )\n\n    def _extract_race_number(self, name: Optional[str]) -> int:\n        if not name: return 1\n        match = re.search(r'\\\\bR(\\\\d{1,2})\\\\b', name)\n        return int(match.group(1)) if match else 1\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': races,\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }",
    "python_service/adapters/brisnet_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass BrisnetAdapter(BaseAdapter):\n    \"\"\"Adapter for brisnet.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"Brisnet\", base_url=\"https://www.brisnet.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"BrisnetAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/drf_adapter.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass DRFAdapter(BaseAdapter):\n    \"\"\"Adapter for scraping data from drf.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"DRF\", base_url=\"https://www.drf.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"DRFAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/equibase_adapter.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass EquibaseAdapter(BaseAdapter):\n    \"\"\"Adapter for scraping data from equibase.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"Equibase\", base_url=\"https://www.equibase.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"EquibaseAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/fanduel_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass FanDuelAdapter(BaseAdapter):\n    \"\"\"Adapter for racing.fanduel.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"FanDuel\", base_url=\"https://racing.fanduel.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"FanDuelAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/greyhound_adapter.py": "from datetime import datetime\nfrom typing import Any, Dict, List\nimport httpx\nfrom pydantic import ValidationError\nimport structlog\nfrom decimal import Decimal\n\nfrom ..models import Race, Runner, OddsData\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\nclass GreyhoundAdapter(BaseAdapter):\n    \"\"\"Adapter for fetching Greyhound racing data. Activated by setting GREYHOUND_API_URL in .env\"\"\"\n\n    def __init__(self, config):\n        if not config.GREYHOUND_API_URL:\n            raise ValueError(\"GreyhoundAdapter cannot be initialized without GREYHOUND_API_URL.\")\n        super().__init__(\n            source_name=\"Greyhound Racing\",\n            base_url=config.GREYHOUND_API_URL\n        )\n        # Example for future use: self.api_key = config.GREYHOUND_API_KEY\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        \"\"\"Fetches upcoming greyhound races for the specified date.\"\"\"\n        start_time = datetime.now()\n        endpoint = f\"v1/cards/{date}\" # Using date parameter\n        try:\n            response_json = await self.make_request(http_client, 'GET', endpoint)\n            if not response_json or not response_json.get(\"cards\"):\n                log.warning(\"GreyhoundAdapter: No 'cards' in response or empty list.\")\n                return self._format_response([], start_time, is_success=True, error_message=\"No race cards found for date.\")\n\n            all_races = self._parse_cards(response_json[\"cards\"])\n            if not all_races:\n                return self._format_response([], start_time, is_success=True, error_message=\"Races found, but none could be parsed.\")\n\n            return self._format_response(all_races, start_time, is_success=True)\n        except httpx.HTTPError as e:\n            log.error(\"GreyhoundAdapter: HTTP request failed after retries\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=\"API request failed after multiple retries.\")\n        except Exception as e:\n            log.error(\"GreyhoundAdapter: An unexpected error occurred\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=f\"An unexpected error occurred: {str(e)}\")\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        \"\"\"Formats the adapter's response consistently.\"\"\"\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': races,\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_cards(self, cards: List[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses a list of cards and their races into Race objects.\"\"\"\n        all_races = []\n        for card in cards:\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            races_data = card.get(\"races\", [])\n            for race_data in races_data:\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_data['race_id']}\",\n                        venue=venue,\n                        race_number=race_data[\"race_number\"],\n                        start_time=datetime.fromtimestamp(race_data[\"start_time\"]),\n                        runners=self._parse_runners(race_data[\"runners\"]),\n                        source=self.source_name\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    log.error(\n                        f\"GreyhoundAdapter: Error parsing race {race_data.get('race_id', 'N/A')}\",\n                        error=str(e),\n                        race_data=race_data\n                    )\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                odds_data = {}\n                # The directive's example was flawed. Correcting to a more realistic structure.\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now()\n                        )\n\n                runner = Runner(\n                    number=runner_data[\"trap_number\"],\n                    name=runner_data[\"dog_name\"],\n                    scratched=runner_data.get(\"scratched\", False),\n                    odds=odds_data\n                )\n                runners.append(runner)\n            except (KeyError, ValidationError) as e:\n                 log.error(\"GreyhoundAdapter: Error parsing runner\", error=str(e), runner_data=runner_data)\n        return runners",
    "python_service/adapters/harness_adapter.py": "from datetime import datetime\nfrom typing import Any, Dict, List\nimport httpx\nfrom pydantic import ValidationError, Field\nimport structlog\nfrom decimal import Decimal\n\nfrom ..models import Race, Runner, OddsData\nfrom .base import BaseAdapter\nfrom .utils import parse_odds\n\nlog = structlog.get_logger(__name__)\n\nclass HarnessAdapter(BaseAdapter):\n    \"\"\"Adapter for fetching Harness racing data from USTA.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"Harness Racing (USTA)\",\n            base_url=\"https://data.ustrotting.com/\"\n        )\n        # No API key required for this public endpoint\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        \"\"\"Fetches upcoming harness races for the specified date.\"\"\"\n        start_time = datetime.now()\n        endpoint = f\"api/racenet/racing/card/{date}\"\n        try:\n            response_json = await self.make_request(http_client, 'GET', endpoint)\n            if not response_json or \"meetings\" not in response_json:\n                log.warning(\"HarnessAdapter: No 'meetings' in response or empty response.\")\n                return self._format_response([], start_time, is_success=True, error_message=\"No meetings found for date.\")\n\n            all_races = self._parse_meetings(response_json[\"meetings\"])\n            return self._format_response(all_races, start_time, is_success=True)\n        except httpx.HTTPError as e:\n            log.error(\"HarnessAdapter: HTTP request failed after retries\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=\"API request failed after multiple retries.\")\n        except Exception as e:\n            log.error(\"HarnessAdapter: An unexpected error occurred\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=f\"An unexpected error occurred: {str(e)}\")\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        \"\"\"Formats the adapter's response consistently.\"\"\"\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': races,\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_meetings(self, meetings: List[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses a list of meetings and their races into Race objects.\"\"\"\n        all_races = []\n        for meeting in meetings:\n            venue = meeting.get(\"trackName\", \"Unknown Venue\")\n            races_data = meeting.get(\"races\", [])\n            for race_data in races_data:\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race = Race(\n                        id=f\"usta_{race_data['raceId']}\", # Correct field: id\n                        venue=venue,\n                        race_number=race_data[\"raceNumber\"],\n                        start_time=datetime.fromisoformat(race_data[\"startTime\"].replace(\"Z\", \"+00:00\")),\n                        runners=self._parse_runners(race_data[\"runners\"]),\n                        source=self.source_name\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    log.error(\n                        f\"HarnessAdapter: Error parsing race {race_data.get('raceId', 'N/A')}\",\n                        error=str(e),\n                        race_data=race_data\n                    )\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                # API provides number as 'postPosition'\n                runner_number = runner_data.get('postPosition')\n                if not runner_number:\n                    continue\n\n                # Adapt to the Runner model's odds structure\n                odds_data = {}\n                win_odds_str = runner_data.get(\"morningLineOdds\")\n                if win_odds_str:\n                    try:\n                        # The USTA API provides \"5\" for 5/1, which the original code handled as `Decimal('5') + 1`.\n                        # The central utility expects fractional format, so we ensure it exists.\n                        if '/' not in win_odds_str:\n                            win_odds_str = f\"{win_odds_str}/1\"\n\n                        parsed_float = parse_odds(win_odds_str)\n                        if parsed_float < 999.0:\n                            decimal_odds = Decimal(str(parsed_float))\n                            if decimal_odds > 1:\n                                odds_data[self.source_name] = OddsData(\n                                    win=decimal_odds,\n                                    source=self.source_name,\n                                    last_updated=datetime.now()\n                                )\n                    except (ValueError, TypeError):\n                         log.warning(\"Could not parse harness odds\", odds_str=win_odds_str)\n\n\n                runner = Runner(\n                    number=runner_number,\n                    name=runner_data[\"horseName\"],\n                    scratched=runner_data.get(\"scratched\", False),\n                    odds=odds_data\n                )\n                runners.append(runner)\n            except (KeyError, ValidationError) as e:\n                log.error(\"HarnessAdapter: Error parsing runner\", error=str(e), runner_data=runner_data)\n        return runners",
    "python_service/adapters/horseracingnation_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass HorseRacingNationAdapter(BaseAdapter):\n    \"\"\"Adapter for horseracingnation.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"HorseRacingNation\", base_url=\"https://www.horseracingnation.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"HorseRacingNationAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/nyrabets_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass NYRABetsAdapter(BaseAdapter):\n    \"\"\"Adapter for nyrabets.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"NYRABets\", base_url=\"https://nyrabets.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"NYRABetsAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/oddschecker_adapter.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Oddschecker Adapter (Canonized)\n# ==============================================================================\n# This adapter was sourced from the 'Live Odds Anthology' and has been modernized\n# to conform to the project's current BaseAdapter framework.\n# ==============================================================================\n\nimport httpx\nimport structlog\nimport asyncio\nfrom datetime import datetime\nfrom typing import List, Optional, Dict, Any\nfrom bs4 import BeautifulSoup, Tag\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\nfrom .utils import parse_odds\n\nlog = structlog.get_logger(__name__)\n\nclass OddscheckerAdapter(BaseAdapter):\n    \"\"\"Adapter for scraping live horse racing odds from Oddschecker.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(source_name=\"Oddschecker\", base_url=\"https://www.oddschecker.com\")\n        self.config = config\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        try:\n            meeting_links = await self._get_all_meeting_links(http_client)\n            if not meeting_links:\n                return self._format_response([], start_time, is_success=True, error_message=\"No meeting links found.\")\n\n            tasks = [self._fetch_single_meeting(link, http_client) for link in meeting_links]\n            races_from_all_meetings = await asyncio.gather(*tasks)\n\n            all_races = [race for sublist in races_from_all_meetings for race in sublist if race]\n            return self._format_response(all_races, start_time)\n        except Exception as e:\n            log.error(\"OddscheckerAdapter failed\", exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=str(e))\n\n    async def _get_all_meeting_links(self, http_client: httpx.AsyncClient) -> List[str]:\n        html = await self.make_request(http_client, 'GET', '/horse-racing')\n        soup = BeautifulSoup(html, \"html.parser\")\n        links = {self.base_url + a['href'] for a in soup.select('a.meeting-title[href]')}\n        return sorted(list(links))\n\n    async def _fetch_single_meeting(self, url: str, client: httpx.AsyncClient) -> List[Optional[Race]]:\n        try:\n            html = await self.make_request(client, 'GET', url.replace(self.base_url, ''))\n            soup = BeautifulSoup(html, \"html.parser\")\n            race_links = {self.base_url + a['href'] for a in soup.select('a.race-time-link[href]')}\n            tasks = [self._fetch_and_parse_race_card(link, client) for link in race_links]\n            return await asyncio.gather(*tasks)\n        except Exception as e:\n            log.error(\"Oddschecker failed to fetch meeting\", url=url, error=e)\n            return []\n\n    async def _fetch_and_parse_race_card(self, url: str, client: httpx.AsyncClient) -> Optional[Race]:\n        try:\n            html = await self.make_request(client, 'GET', url.replace(self.base_url, ''))\n            soup = BeautifulSoup(html, \"html.parser\")\n            return self._parse_race_page(soup, url)\n        except Exception as e:\n            log.error(\"Oddschecker failed to parse race card\", url=url, error=e)\n            return None\n\n    def _parse_race_page(self, soup: BeautifulSoup, url: str) -> Optional[Race]:\n        track_name = soup.select_one('h1.meeting-name').get_text(strip=True) if soup.select_one('h1.meeting-name') else 'Unknown'\n        race_time = soup.select_one('span.race-time').get_text(strip=True) if soup.select_one('span.race-time') else None\n        race_number = int(url.split('-')[-1]) if 'race-' in url else 0\n\n        runners = []\n        for row in soup.select(\"tr.race-card-row\"):\n            runner = self._parse_runner_row(row)\n            if runner: runners.append(runner)\n\n        if not runners: return None\n\n        return Race(\n            id=f\"oc_{track_name.lower().replace(' ', '')}_{datetime.now().strftime('%Y%m%d')}_r{race_number}\",\n            venue=track_name, race_number=race_number, start_time=datetime.now(), # Placeholder time\n            runners=runners\n        )\n\n    def _parse_runner_row(self, row: Tag) -> Optional[Runner]:\n        name_tag = row.select_one('span.selection-name')\n        name = name_tag.get_text(strip=True) if name_tag else None\n        odds_tag = row.select_one('span.bet-button-odds-desktop, span.best-price')\n        odds_str = odds_tag.get_text(strip=True) if odds_tag else None\n        number_tag = row.select_one('td.runner-number')\n        number = int(number_tag.get_text(strip=True)) if number_tag else 0\n\n        if not name or not odds_str: return None\n\n        odds_val = parse_odds(odds_str)\n        odds_dict = {}\n        if odds_val:\n            odds_dict[self.source_name] = OddsData(win=Decimal(str(odds_val)), source=self.source_name, last_updated=datetime.now())\n\n        return Runner(number=number, name=name, odds=odds_dict)\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        \"\"\"Formats the adapter's response consistently.\"\"\"\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': [r.model_dump() for r in races],\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }",
    "python_service/adapters/pointsbet_greyhound_adapter.py": "# python_service/adapters/pointsbet_greyhound_adapter.py\n\nimport structlog\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\n\nlog = structlog.get_logger(__name__)\n\nclass PointsBetGreyhoundAdapter(BaseAdapter):\n    \"\"\"TODO: This is a placeholder adapter. It will not be active until the correct sportId is found.\"\"\"\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"PointsBet Greyhound\",\n            base_url=\"https://api.au.pointsbet.com\"\n        )\n        self.api_key = config.POINTSBET_API_KEY\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        # TODO: This adapter is a placeholder and is not registered in the engine.\n        # To enable, find the correct sportId for Greyhound Racing and register the adapter.\n        log.warning(\"PointsBetGreyhoundAdapter: This adapter is a non-functional placeholder.\")\n        return self._format_response([], start_time, is_success=True, error_message=\"Adapter is a placeholder.\")\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': races,\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }",
    "python_service/adapters/punters_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass PuntersAdapter(BaseAdapter):\n    \"\"\"Adapter for punters.com.au.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"Punters\", base_url=\"https://www.punters.com.au\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"PuntersAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/racing_and_sports_adapter.py": "# python_service/adapters/racing_and_sports_adapter.py\n\nimport os\nimport structlog\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner\n\nlog = structlog.get_logger(__name__)\n\nclass RacingAndSportsAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"Racing and Sports\",\n            base_url=\"https://api.racingandsports.com.au/\"\n        )\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        all_races: List[Race] = []\n        headers = {\"Authorization\": f\"Bearer {self.api_token}\", \"Accept\": \"application/json\"}\n\n        if not self.api_token:\n            return self._format_response([], start_time, is_success=False, error_message=\"ConfigurationError: Token not set\")\n\n        try:\n            meetings_url = \"v1/racing/meetings\"\n            params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n            meetings_data = await self.make_request(http_client, 'GET', meetings_url, headers=headers, params=params)\n\n            if not meetings_data or not meetings_data.get('meetings'):\n                return self._format_response(all_races, start_time, is_success=True)\n\n            for meeting in meetings_data['meetings']:\n                for race_summary in meeting.get('races', []):\n                    try:\n                        parsed_race = self._parse_ras_race(meeting, race_summary)\n                        all_races.append(parsed_race)\n                    except Exception as e:\n                        log.error(\"RacingAndSportsAdapter: Failed to parse race\", meeting=meeting.get('venueName'), error=str(e), exc_info=True)\n\n            return self._format_response(all_races, start_time, is_success=True)\n        except httpx.HTTPError as e:\n            log.error(\"RacingAndSportsAdapter: HTTP request failed after retries\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=\"API request failed after multiple retries.\")\n        except Exception as e:\n            log.error(\"RacingAndSportsAdapter: An unexpected error occurred\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=f\"An unexpected error occurred: {e}\")\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': races,\n            'source_info': {\n                'name': self.source_name, 'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races), 'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Race:\n        runners = [Runner(number=rd.get('runnerNumber'), name=rd.get('horseName', 'Unknown'), scratched=rd.get('isScratched', False)) for rd in race.get('runners', [])]\n\n        return Race(\n            id=f\"ras_{race.get('raceId')}\",\n            venue=meeting.get('venueName', 'Unknown Venue'),\n            race_number=race.get('raceNumber'),\n            start_time=datetime.fromisoformat(race.get('startTime')),\n            runners=runners,\n            source=self.source_name\n        )",
    "python_service/adapters/racing_and_sports_greyhound_adapter.py": "# python_service/adapters/racing_and_sports_greyhound_adapter.py\n\nimport os\nimport structlog\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner\n\nlog = structlog.get_logger(__name__)\n\nclass RacingAndSportsGreyhoundAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"Racing and Sports Greyhound\",\n            base_url=\"https://api.racingandsports.com.au/\"\n        )\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        all_races: List[Race] = []\n        headers = {\"Authorization\": f\"Bearer {self.api_token}\", \"Accept\": \"application/json\"}\n\n        if not self.api_token:\n            return self._format_response([], start_time, is_success=False, error_message=\"ConfigurationError: Token not set\")\n\n        try:\n            # HYPOTHESIS: The greyhound endpoint is parallel to the racing one.\n            meetings_url = \"v1/greyhound/meetings\"\n            params = {\"date\": date, \"jurisdiction\": \"AUS\"} # Jurisdiction may need to be adjusted\n            meetings_data = await self.make_request(http_client, 'GET', meetings_url, headers=headers, params=params)\n\n            if not meetings_data or not meetings_data.get('meetings'):\n                return self._format_response(all_races, start_time, is_success=True, error_message=\"No greyhound meetings found.\")\n\n            for meeting in meetings_data['meetings']:\n                for race_summary in meeting.get('races', []):\n                    try:\n                        parsed_race = self._parse_ras_race(meeting, race_summary)\n                        all_races.append(parsed_race)\n                    except Exception as e:\n                        log.error(\"RacingAndSportsGreyhoundAdapter: Failed to parse race\", meeting=meeting.get('venueName'), error=str(e), exc_info=True)\n\n            return self._format_response(all_races, start_time, is_success=True)\n        except httpx.HTTPError as e:\n            log.error(\"RacingAndSportsGreyhoundAdapter: HTTP request failed after retries\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=\"API request failed after multiple retries.\")\n        except Exception as e:\n            log.error(\"RacingAndSportsGreyhoundAdapter: An unexpected error occurred\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=f\"An unexpected error occurred: {e}\")\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': races,\n            'source_info': {\n                'name': self.source_name, 'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races), 'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Race:\n        runners = [Runner(number=rd.get('runnerNumber'), name=rd.get('horseName', 'Unknown'), scratched=rd.get('isScratched', False)) for rd in race.get('runners', [])]\n\n        return Race(\n            id=f\"rasg_{race.get('raceId')}\",\n            venue=meeting.get('venueName', 'Unknown Venue'),\n            race_number=race.get('raceNumber'),\n            start_time=datetime.fromisoformat(race.get('startTime')),\n            runners=runners,\n            source=self.source_name\n        )",
    "python_service/adapters/racingpost_adapter.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass RacingPostAdapter(BaseAdapter):\n    \"\"\"Adapter for scraping data from racingpost.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"RacingPost\", base_url=\"https://www.racingpost.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"RacingPostAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/racingtv_adapter.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass RacingTVAdapter(BaseAdapter):\n    \"\"\"Adapter for scraping data from racingtv.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"RacingTV\", base_url=\"https://www.racingtv.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"RacingTVAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/sporting_life_adapter.py": "# python_service/adapters/sporting_life_adapter.py\n\nimport asyncio, structlog, httpx\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom bs4 import BeautifulSoup, Tag\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\nfrom .utils import parse_odds\n\nlog = structlog.get_logger(__name__)\n\ndef _clean_text(text: Optional[str]) -> Optional[str]:\n    return ' '.join(text.strip().split()) if text else None\n\nclass SportingLifeAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(source_name=\"SportingLife\", base_url=\"https://www.sportinglife.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        try:\n            race_links = await self._get_race_links(http_client)\n            tasks = [self._fetch_and_parse_race(link, http_client) for link in race_links]\n            races = [race for race in await asyncio.gather(*tasks) if race]\n            return self._format_response(races, start_time, is_success=True)\n        except Exception as e:\n            return self._format_response([], start_time, is_success=False, error_message=str(e))\n\n    async def _get_race_links(self, http_client: httpx.AsyncClient) -> List[str]:\n        response_html = await self.make_request(http_client, 'GET', '/horse-racing/racecards')\n        if not response_html: return []\n        soup = BeautifulSoup(response_html, \"html.parser\")\n        links = {a['href'] for a in soup.select(\"a.hr-race-card-meeting__race-link[href]\")}\n        return [f\"{self.base_url}{link}\" for link in links]\n\n    async def _fetch_and_parse_race(self, url: str, http_client: httpx.AsyncClient) -> Optional[Race]:\n        try:\n            response_html = await self.make_request(http_client, 'GET', url)\n            if not response_html: return None\n            soup = BeautifulSoup(response_html, \"html.parser\")\n            track_name = _clean_text(soup.select_one(\"a.hr-race-header-course-name__link\").get_text())\n            race_time_str = _clean_text(soup.select_one(\"span.hr-race-header-time__time\").get_text())\n            start_time = datetime.strptime(f\"{datetime.now().date()} {race_time_str}\", \"%Y-%m-%d %H:%M\")\n            active_link = soup.select_one(\"a.hr-race-header-navigation-link--active\")\n            race_number = soup.select(\"a.hr-race-header-navigation-link\").index(active_link) + 1 if active_link else 1\n            runners = [self._parse_runner(row) for row in soup.select(\"div.hr-racing-runner-card\")]\n            return Race(id=f\"sl_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\", venue=track_name, race_number=race_number, start_time=start_time, runners=[r for r in runners if r], source=self.source_name)\n        except Exception: return None\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = _clean_text(row.select_one(\"a.hr-racing-runner-horse-name\").get_text())\n            num_str = _clean_text(row.select_one(\"span.hr-racing-runner-saddle-cloth-no\").get_text())\n            number = int(''.join(filter(str.isdigit, num_str)))\n            odds_str = _clean_text(row.select_one(\"span.hr-racing-runner-odds\").get_text())\n            win_odds = Decimal(str(parse_odds(odds_str))) if odds_str else None\n            odds_data = {self.source_name: OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())} if win_odds and win_odds < 999 else {}\n            return Runner(number=number, name=name, odds=odds_data)\n        except: return None\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool, error_message: str = None) -> Dict[str, Any]:\n        return {'races': races, 'source_info': {'name': self.source_name, 'status': 'SUCCESS' if is_success else 'FAILED', 'races_fetched': len(races), 'error_message': error_message, 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/tab_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass TabAdapter(BaseAdapter):\n    \"\"\"Adapter for tab.com.au.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"TAB\", base_url=\"https://www.tab.com.au\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"TabAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/template_adapter.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# ==============================================================================\n#  Fortuna Faucet: Canonical Adapter Template\n# ==============================================================================\n# This file is the official template for creating new adapters. It is based on\n# the clean and simple design of the RacingAndSportsAdapter.\n# ==============================================================================\n\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner # Assuming standard models\n\nlog = structlog.get_logger(__name__)\n\nclass TemplateAdapter(BaseAdapter):\n    \"\"\"[IMPLEMENT ME] A brief description of the data source.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"[IMPLEMENT ME] Example Source\",\n            base_url=\"https://api.example.com\"\n        )\n        # self.api_key = config.EXAMPLE_API_KEY # Uncomment if needed\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        \"\"\"[IMPLEMENT ME] The core logic for fetching and parsing races.\"\"\"\n        start_time = datetime.now()\n        all_races: List[Race] = []\n\n        # --- Example Logic ---\n        # endpoint = f\"/v1/races/{date}\"\n        # headers = {\"X-Api-Key\": self.api_key}\n        # response_json = await self.make_request(http_client, 'GET', endpoint, headers=headers)\n        # if not response_json or 'data' not in response_json:\n        #     return self._format_response(all_races, start_time)\n        #\n        # for race_data in response_json['data']:\n        #     parsed_race = self._parse_race(race_data)\n        #     all_races.append(parsed_race)\n        # --- End Example ---\n\n        log.warning(\"TemplateAdapter.fetch_races is a stub and is not implemented.\")\n        return self._format_response(all_races, start_time)\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        \"\"\"Formats the adapter's response consistently.\"\"\"\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': [r.model_dump() for r in races],\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_race(self, race_data: Dict[str, Any]) -> Race:\n        \"\"\"[IMPLEMENT ME] Logic to parse a single race from the source's data structure.\"\"\"\n        # Example:\n        # runners = self._parse_runners(race_data.get('runners', []))\n        # return Race(\n        #     id=f\"template_{race_data['id']}\",\n        #     venue=race_data['venue_name'],\n        #     race_number=race_data['race_number'],\n        #     start_time=datetime.fromisoformat(race_data['start_time']),\n        #     runners=runners,\n        #     source=self.source_name\n        # )\n        raise NotImplementedError(\"'_parse_race' is not implemented in TemplateAdapter.\")\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"[IMPLEMENT ME] Logic to parse a list of runners.\"\"\"\n        raise NotImplementedError(\"'_parse_runners' is not implemented in TemplateAdapter.\")",
    "python_service/adapters/the_racing_api_adapter.py": "# python_service/adapters/theracingapi_adapter.py\n\nimport structlog\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nimport httpx\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\n\nlog = structlog.get_logger(__name__)\n\nclass TheRacingApiAdapter(BaseAdapter):\n    \"\"\"Adapter for the high-value JSON-based The Racing API.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"TheRacingAPI\",\n            base_url=\"https://api.theracingapi.com/v1/\"\n        )\n        self.api_key = config.THE_RACING_API_KEY\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        if not self.api_key:\n            return self._format_response([], start_time, is_success=False, error_message=\"ConfigurationError: THE_RACING_API_KEY not set\")\n\n        try:\n            endpoint = f\"racecards?date={date}&course=all&region=gb,ire\"\n            headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n            response_json = await self.make_request(http_client, 'GET', endpoint, headers=headers)\n\n            if not response_json or not response_json.get('racecards'):\n                return self._format_response([], start_time, is_success=True, error_message=\"No racecards found in API response.\")\n\n            all_races = self._parse_races(response_json['racecards'])\n            return self._format_response(all_races, start_time, is_success=True)\n        except httpx.HTTPError as e:\n            log.error(f\"{self.source_name}: HTTP request failed after retries\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=\"API request failed after multiple retries.\")\n        except Exception as e:\n            log.error(f\"{self.source_name}: An unexpected error occurred\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=f\"An unexpected error occurred: {e}\")\n\n    def _parse_races(self, racecards: List[Dict[str, Any]]) -> List[Race]:\n        races = []\n        for race_data in racecards:\n            try:\n                start_time = datetime.fromisoformat(race_data['off_time'].replace('Z', '+00:00'))\n\n                race = Race(\n                    id=f\"tra_{race_data['race_id']}\",\n                    venue=race_data['course'],\n                    race_number=race_data['race_no'],\n                    start_time=start_time,\n                    runners=self._parse_runners(race_data.get('runners', [])),\n                    source=self.source_name,\n                    race_name=race_data.get('race_name'),\n                    distance=race_data.get('distance_f'),\n                )\n                races.append(race)\n            except Exception as e:\n                log.error(f\"{self.source_name}: Error parsing race\", race_id=race_data.get('race_id'), error=str(e))\n        return races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for i, runner_data in enumerate(runners_data):\n            try:\n                odds_data = {}\n                if runner_data.get('odds'):\n                    win_odds = Decimal(str(runner_data['odds'][0]['odds_decimal']))\n                    odds_data[self.source_name] = OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())\n\n                runners.append(Runner(\n                    number=runner_data.get('number', i + 1),\n                    name=runner_data['horse'],\n                    odds=odds_data,\n                    jockey=runner_data.get('jockey'),\n                    trainer=runner_data.get('trainer'),\n                ))\n            except Exception as e:\n                log.error(f\"{self.source_name}: Error parsing runner\", runner_name=runner_data.get('horse'), error=str(e))\n        return runners\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        return {\n            'races': races,\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': (datetime.now() - start_time).total_seconds()\n            }\n        }",
    "python_service/adapters/timeform_adapter.py": "# python_service/adapters/timeform_adapter.py\n\nimport asyncio, structlog, httpx\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nfrom bs4 import BeautifulSoup, Tag\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\nfrom .utils import parse_odds\n\nlog = structlog.get_logger(__name__)\n\ndef _clean_text(text: Optional[str]) -> Optional[str]:\n    return ' '.join(text.strip().split()) if text else None\n\nclass TimeformAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(source_name=\"Timeform\", base_url=\"https://www.timeform.com\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        try:\n            race_links = await self._get_race_links(http_client)\n            tasks = [self._fetch_and_parse_race(link, http_client) for link in race_links]\n            races = [race for race in await asyncio.gather(*tasks) if race]\n            return self._format_response(races, start_time, is_success=True)\n        except Exception as e:\n            return self._format_response([], start_time, is_success=False, error_message=str(e))\n\n    async def _get_race_links(self, http_client: httpx.AsyncClient) -> List[str]:\n        response_html = await self.make_request(http_client, 'GET', '/horse-racing/racecards')\n        if not response_html: return []\n        soup = BeautifulSoup(response_html, \"html.parser\")\n        links = {a['href'] for a in soup.select(\"a.rp-racecard-off-link[href]\")}\n        return [f\"{self.base_url}{link}\" for link in links]\n\n    async def _fetch_and_parse_race(self, url: str, http_client: httpx.AsyncClient) -> Optional[Race]:\n        try:\n            response_html = await self.make_request(http_client, 'GET', url)\n            if not response_html: return None\n            soup = BeautifulSoup(response_html, \"html.parser\")\n            track_name = _clean_text(soup.select_one(\"h1.rp-raceTimeCourseName_name\").get_text())\n            race_time_str = _clean_text(soup.select_one(\"span.rp-raceTimeCourseName_time\").get_text())\n            start_time = datetime.strptime(f\"{datetime.now().date()} {race_time_str}\", \"%Y-%m-%d %H:%M\")\n            all_times = [_clean_text(a.get_text()) for a in soup.select('a.rp-racecard-off-link')]\n            race_number = all_times.index(race_time_str) + 1 if race_time_str in all_times else 1\n            runners = [self._parse_runner(row) for row in soup.select(\"div.rp-horseTable_mainRow\")]\n            return Race(id=f\"tf_{track_name.replace(' ', '')}_{start_time.strftime('%Y%m%d')}_R{race_number}\", venue=track_name, race_number=race_number, start_time=start_time, runners=[r for r in runners if r], source=self.source_name)\n        except Exception: return None\n\n    def _parse_runner(self, row: Tag) -> Optional[Runner]:\n        try:\n            name = _clean_text(row.select_one(\"a.rp-horseTable_horse-name\").get_text())\n            num_str = _clean_text(row.select_one(\"span.rp-horseTable_horse-number\").get_text()).strip(\"()\")\n            number = int(''.join(filter(str.isdigit, num_str)))\n            odds_str = _clean_text(row.select_one(\"button.rp-bet-placer-btn__odds\").get_text())\n            win_odds = Decimal(str(parse_odds(odds_str))) if odds_str else None\n            odds_data = {self.source_name: OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())} if win_odds and win_odds < 999 else {}\n            return Runner(number=number, name=name, odds=odds_data)\n        except: return None\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool, error_message: str = None) -> Dict[str, Any]:\n        return {'races': races, 'source_info': {'name': self.source_name, 'status': 'SUCCESS' if is_success else 'FAILED', 'races_fetched': len(races), 'error_message': error_message, 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\n\nimport os\nimport structlog\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional\nimport httpx\nfrom decimal import Decimal, InvalidOperation\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\nfrom .utils import parse_odds\n\nlog = structlog.get_logger(__name__)\n\ndef _parse_program_number(program_str: str) -> int:\n    \"\"\"Safely parses program numbers like '1A' into an integer.\"\"\"\n    return int(''.join(filter(str.isdigit, program_str))) if program_str else 99\n\nclass TVGAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"TVG\",\n            base_url=\"https://api.tvg.com/v3/\"\n        )\n        self.api_key = config.TVG_API_KEY\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        all_races: List[Race] = []\n        headers = {\"Accept\": \"application/json\", \"X-API-Key\": self.api_key}\n\n        if not self.api_key:\n            log.warning(\"TVGAdapter: TVG_API_KEY not set. Skipping.\")\n            return self._format_response([], start_time, is_success=False, error_message=\"ConfigurationError: TVG_API_KEY not set\")\n\n        try:\n            tracks_url = \"tracks\"\n            tracks_params = {\"date\": date, \"country\": \"US\"}\n            tracks_response = await self.make_request(http_client, 'GET', tracks_url, headers=headers, params=tracks_params)\n\n            if not tracks_response or 'tracks' not in tracks_response:\n                log.warning(\"TVG: No tracks found for the given date.\")\n                return self._format_response(all_races, start_time, is_success=True)\n\n            for track in tracks_response['tracks']:\n                try:\n                    races_url = f\"tracks/{track['code']}/races\"\n                    races_params = {\"date\": date}\n                    races_response = await self.make_request(http_client, 'GET', races_url, headers=headers, params=races_params)\n\n                    for race_summary in races_response.get('races', []):\n                        race_detail_url = f\"tracks/{track['code']}/races/{race_summary['number']}\"\n                        race_detail = await self.make_request(http_client, 'GET', race_detail_url, headers=headers)\n                        if race_detail:\n                            parsed_race = self._parse_tvg_race(track, race_detail)\n                            all_races.append(parsed_race)\n                except httpx.HTTPError as e:\n                    log.error(\"TVGAdapter: Failed to process track, skipping.\", track_name=track.get('name'), error=str(e))\n                    continue # Continue to the next track\n\n            return self._format_response(all_races, start_time, is_success=True)\n        except httpx.HTTPError as e:\n            log.error(\"TVGAdapter: Initial HTTP request failed after retries\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=\"API request failed after multiple retries.\")\n        except Exception as e:\n            log.error(\"TVGAdapter: An unexpected error occurred\", error=str(e), exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=f\"An unexpected error occurred: {e}\")\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': races,\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_tvg_race(self, track: Dict[str, Any], race_data: Dict[str, Any]) -> Race:\n        runners = []\n        for runner_data in race_data.get('runners', []):\n            if not runner_data.get('scratched'):\n                current_odds_str = runner_data.get('odds', {}).get('current') or runner_data.get('odds', {}).get('morningLine')\n                win_odds = self._parse_tvg_odds(current_odds_str)\n\n                odds_dict = {}\n                if win_odds:\n                    odds_dict[self.source_name] = OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())\n\n                runners.append(Runner(\n                    number=_parse_program_number(runner_data.get('programNumber')),\n                    name=runner_data.get('horseName', 'Unknown Runner'),\n                    scratched=False,\n                    odds=odds_dict\n                ))\n\n        race_id = f\"{track.get('code', 'UNK').lower()}_{race_data['postTime'].split('T')[0]}_R{race_data['number']}\"\n\n        return Race(\n            id=race_id,\n            venue=track.get('name', 'Unknown Venue'),\n            race_number=race_data.get('number'),\n            start_time=datetime.fromisoformat(race_data.get('postTime')),\n            runners=runners,\n            source=self.source_name\n        )\n\n    def _parse_tvg_odds(self, odds_string: str) -> Optional[Decimal]:\n        if not odds_string or odds_string == \"SCR\":\n            return None\n        try:\n            # Utilize the centralized parsing utility which handles fractions, evens, and decimals.\n            parsed_float = parse_odds(odds_string)\n            # The utility returns a high number on failure, which we can filter out.\n            if parsed_float >= 999.0:\n                return None\n            return Decimal(str(parsed_float))\n        except (ValueError, InvalidOperation):\n            log.warning(\"Could not convert parsed TVG odds to Decimal\", odds_str=odds_string)\n            return None",
    "python_service/adapters/twinspires_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass TwinSpiresAdapter(BaseAdapter):\n    \"\"\"Adapter for twinspires.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"TwinSpires\", base_url=\"https://www.twinspires.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"TwinSpiresAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/adapters/universal_adapter.py": "import json\nimport httpx\nimport structlog\nfrom bs4 import BeautifulSoup\nfrom typing import Dict, Any, List\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner # Assuming models are updated\n\nlog = structlog.get_logger(__name__)\n\nclass UniversalAdapter(BaseAdapter):\n    \"\"\"An adapter that executes logic from a declarative JSON definition file.\"\"\"\n\n    def __init__(self, config, definition_path: str):\n        with open(definition_path, 'r') as f:\n            self.definition = json.load(f)\n\n        super().__init__(\n            source_name=self.definition['adapter_name'],\n            base_url=self.definition['base_url']\n        )\n        self.config = config\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        # NOTE: This is a simplified proof-of-concept implementation.\n        # It does not handle all cases from the JSON definition.\n        log.info(f\"Executing Universal Adapter for {self.source_name}\")\n\n        # Step 1: Get Track Links (as defined in equibase_v2.json)\n        start_url = self.definition['steps'][0]['selector']\n        response = await self.make_request(http_client, 'GET', self.definition['start_url'])\n        soup = BeautifulSoup(response, 'html.parser')\n        track_links = [self.base_url + a['href'] for a in soup.select(self.definition['steps'][0]['selector'])]\n\n        all_races = []\n        for link in track_links:\n            try:\n                track_response = await self.make_request(http_client, 'GET', link.replace(self.base_url, ''))\n                track_soup = BeautifulSoup(track_response, 'html.parser')\n                race_containers = track_soup.select(self.definition['steps'][1]['list_selector'])\n\n                for container in race_containers:\n                    # Simplified parsing logic for this PoC\n                    track_name = container.select_one('div.track-name h1').text.strip()\n                    race_number = int(container.select_one('h3.race-number').text.strip().split()[-1])\n                    # ... further parsing would be implemented here ...\n                    # This PoC demonstrates the principle, not a full implementation.\n                    pass # Placeholder\n\n            except Exception as e:\n                log.error(\"Failed to process track link\", link=link, error=e)\n\n        # This is a placeholder return for the PoC\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'PoC Complete', 'fetch_duration': 0.0}}",
    "python_service/adapters/utils.py": "# ==============================================================================\n# == Centralized Adapter Utilities\n# ==============================================================================\n# This module provides shared, battle-tested functions for all adapters to use,\n# ensuring consistency and adhering to the DRY principle.\n# ==============================================================================\n\nfrom typing import Union\n\ndef parse_odds(odds: Union[str, int, float]) -> float:\n    \"\"\"\n    Parses various odds formats (e.g., fractional '10/1', decimal 11.0)\n    into a standardized decimal float.\n\n    Returns a default high odds value on failure to prevent crashes.\n    \"\"\"\n    if isinstance(odds, (int, float)):\n        return float(odds)\n\n    if isinstance(odds, str):\n        try:\n            # Handle fractional odds (e.g., \"10/1\", \"5/2\")\n            if \"/\" in odds:\n                numerator, denominator = map(int, odds.split('/'))\n                if denominator == 0: return 999.0\n                return 1.0 + (numerator / denominator)\n\n            # Handle \"evens\"\n            if odds.lower() == 'evens':\n                return 2.0\n\n            # Handle simple decimal strings\n            return float(odds)\n        except (ValueError, TypeError):\n            # Return a high, but valid, number for unparseable odds\n            return 999.0\n\n    return 999.0",
    "python_service/adapters/xpressbet_adapter.py": "#!/usr/bin/env python3\n# This file was generated from the canonical adapter template.\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nimport structlog\nfrom .base import BaseAdapter\nfrom ..models import Race\nlog = structlog.get_logger(__name__)\nclass XpressbetAdapter(BaseAdapter):\n    \"\"\"Adapter for xpressbet.com.\"\"\"\n    def __init__(self, config):\n        super().__init__(source_name=\"Xpressbet\", base_url=\"https://www.xpressbet.com\")\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        log.warning(\"XpressbetAdapter.fetch_races is a stub.\")\n        return self._format_response([], start_time)\n    def _format_response(self, races: List[Race], start_time: datetime, **kwargs) -> Dict[str, Any]:\n        return {'races': [], 'source_info': {'name': self.source_name, 'status': 'SUCCESS', 'races_fetched': 0, 'error_message': 'Not Implemented', 'fetch_duration': (datetime.now() - start_time).total_seconds()}}",
    "python_service/analyzer.py": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Type, Optional\nimport structlog\nfrom decimal import Decimal\n\nfrom python_service.models import Race, Runner\n\nlog = structlog.get_logger(__name__)\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    # Filter out invalid or placeholder odds (e.g., > 999)\n    valid_odds = [o.win for o in runner.odds.values() if o.win is not None and o.win < 999]\n\n    if not valid_odds:\n        return None\n\n    return min(valid_odds)\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n    def __init__(self, max_field_size: int = 10, min_favorite_odds: float = 2.5, min_second_favorite_odds: float = 4.0):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n\n    def qualify_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Filters and scores races, returning a sorted list of qualified opportunities.\"\"\"\n        qualified_races = []\n        for race in races:\n            score = self._evaluate_race(race)\n            if score is not None:\n                race.qualification_score = score\n                qualified_races.append(race)\n\n        # Sort the qualified races by score, descending\n        qualified_races.sort(key=lambda r: r.qualification_score, reverse=True)\n        log.info(\"Qualification and scoring complete\", qualified_count=len(qualified_races))\n        return qualified_races\n\n    def _evaluate_race(self, race: Race) -> Optional[float]:\n        \"\"\"Evaluates a single race and returns a qualification score if it passes, else None.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2: return None\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Apply the Trifecta of Factors as hard filters ---\n        if len(active_runners) > self.max_field_size: return None\n        if favorite_odds < self.min_favorite_odds: return None\n        if second_favorite_odds < self.min_second_favorite_odds: return None\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        return round(final_score * 100, 2)\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer('trifecta', TrifectaAnalyzer)\n        log.info(\"AnalyzerEngine discovered plugins\", available_analyzers=list(self.analyzers.keys()))\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)",
    "python_service/api.py": "# python_service/api.py\n\nimport structlog\nfrom datetime import datetime, date\nfrom typing import List, Optional\nfrom fastapi import FastAPI, HTTPException, Request, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nfrom contextlib import asynccontextmanager\n\nfrom .config import get_settings\nfrom .engine import OddsEngine\nfrom .models import Race, AggregatedResponse\nfrom .security import verify_api_key\nfrom .logging_config import configure_logging\nfrom .analyzer import AnalyzerEngine\n\nlog = structlog.get_logger()\n\n# Define the lifespan context manager for robust startup/shutdown\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Manage the application's lifespan. On startup, it initializes the OddsEngine\n    with validated settings and attaches it to the app state. On shutdown, it\n    properly closes the engine's resources.\n    \"\"\"\n    configure_logging()\n    settings = get_settings()\n    app.state.engine = OddsEngine(config=settings)\n    app.state.analyzer_engine = AnalyzerEngine()\n    log.info(\"Server startup: Configuration validated and OddsEngine initialized.\")\n    yield\n    # Clean up the engine resources\n    await app.state.engine.close()\n    log.info(\"Server shutdown: HTTP client resources closed.\")\n\nlimiter = Limiter(key_func=get_remote_address)\n\n# Pass the lifespan manager to the FastAPI app\napp = FastAPI(title=\"Checkmate Ultimate Solo API\", version=\"2.1\", lifespan=lifespan)\napp.add_middleware(SlowAPIMiddleware)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\nsettings = get_settings()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True, allow_methods=[\"GET\"], allow_headers=[\"*\"]\n)\n\n# Dependency function to get the engine instance from the app state\ndef get_engine(request: Request) -> OddsEngine:\n    return request.app.state.engine\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(request: Request, engine: OddsEngine = Depends(get_engine), _=Depends(verify_api_key)):\n    \"\"\"Provides a list of health statuses for all adapters, required by the new frontend blueprint.\"\"\"\n    try:\n        statuses = engine.get_all_adapter_statuses()\n        return statuses\n    except Exception as e:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races/qualified/{analyzer_name}\", response_model=List[Race])\n@limiter.limit(\"30/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[date] = None,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key)\n):\n    \"\"\"\n    Gets all races for a given date, filters them for qualified betting\n    opportunities, and returns the qualified races.\n    \"\"\"\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime('%Y-%m-%d')\n        aggregated_data = await engine.fetch_all_odds(date_str)\n\n        # The engine now correctly returns validated Pydantic models.\n        # No re-validation is necessary.\n        races = aggregated_data.get('races', [])\n\n        analyzer_engine = request.app.state.analyzer_engine\n        # In the future, kwargs could come from the request's query params\n        analyzer = analyzer_engine.get_analyzer(analyzer_name)\n        qualified_races = analyzer.qualify_races(races)\n        return qualified_races\n    except ValueError as e:\n        # Correctly map a missing analyzer to a 404 Not Found error\n        log.warning(\"Requested analyzer not found\", analyzer_name=analyzer_name)\n        raise HTTPException(status_code=404, detail=str(e))\n    except Exception as e:\n        log.error(\"Error in /api/races/qualified\", error=str(e), exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[date] = None,\n    source: Optional[str] = None,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key)\n):\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime('%Y-%m-%d')\n        aggregated_data = await engine.fetch_all_odds(date_str, source)\n        return aggregated_data\n    except Exception as e:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")",
    "python_service/config.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Centralized Configuration\n# =================================\u00e1=============================================\n# This module, restored by the Great Correction, provides a centralized and\n# validated source for all application settings using pydantic-settings.\n# ==============================================================================\n\nfrom pydantic_settings import BaseSettings\nfrom functools import lru_cache\nfrom typing import List, Optional\n\nclass Settings(BaseSettings):\n    # --- Application Security ---\n    API_KEY: str\n\n    # --- Betfair API Credentials ---\n    BETFAIR_APP_KEY: str = \"\"\n    BETFAIR_USERNAME: str = \"\"\n    BETFAIR_PASSWORD: str = \"\"\n\n    # --- Other Adapter Keys ---\n    TVG_API_KEY: str = \"\"\n    RACING_AND_SPORTS_TOKEN: str = \"\"\n    POINTSBET_API_KEY: str = \"\"\n    GREYHOUND_API_URL: Optional[str] = None\n    THE_RACING_API_KEY: Optional[str] = None\n\n    # --- Placeholder keys for restored scrapers (not currently used but good practice) ---\n    AT_THE_RACES_KEY: Optional[str] = None\n    SPORTING_LIFE_KEY: Optional[str] = None\n    TIMEFORM_KEY: Optional[str] = None\n\n    # --- CORS Configuration ---\n    ALLOWED_ORIGINS: List[str] = [\"http://localhost:3000\", \"http://localhost:3001\"]\n\n    class Config:\n        env_file = \".env\"\n        case_sensitive = True\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"Returns a cached instance of the application settings.\"\"\"\n    return Settings()",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport structlog\n\nlog = structlog.get_logger(__name__)\nimport httpx\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Tuple\n\nfrom .adapters.base import BaseAdapter\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\n# from .adapters.greyhound_adapter import GreyhoundAdapter\n\nclass OddsEngine:\n    def __init__(self, config):\n        self.config = config\n        self.log = structlog.get_logger(self.__class__.__name__)\n        self.adapters: List[BaseAdapter] = [\n            BetfairAdapter(config=self.config),\n            BetfairGreyhoundAdapter(config=self.config),\n            TVGAdapter(config=self.config),\n            RacingAndSportsAdapter(config=self.config),\n            RacingAndSportsGreyhoundAdapter(config=self.config),\n            AtTheRacesAdapter(config=self.config),\n            SportingLifeAdapter(config=self.config),\n            TimeformAdapter(config=self.config),\n            TheRacingApiAdapter(config=self.config),\n            HarnessAdapter(config=self.config)\n        ]\n\n        # Conditionally activate the GreyhoundAdapter if its URL is configured\n        if self.config.GREYHOUND_API_URL:\n            self.log.info(\"GREYHOUND_API_URL is set. Activating GreyhoundAdapter.\")\n            self.adapters.append(GreyhoundAdapter(config=self.config))\n\n        self.http_client = httpx.AsyncClient()\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        \"\"\"Returns the health status of all registered adapters.\"\"\"\n        statuses = []\n        for adapter in self.adapters:\n            statuses.append(adapter.get_status())\n        return statuses\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapter, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"Wraps an adapter's fetch call to accurately measure its duration.\"\"\"\n        start_time = datetime.now()\n        # Adapters now handle their own exceptions and return a consistent payload.\n        # The engine's role is to orchestrate and time the calls.\n        result = await adapter.fetch_races(date, self.http_client)\n        duration = (datetime.now() - start_time).total_seconds()\n        return (adapter.source_name, result, duration)\n\n    def _race_key(self, race) -> str:\n        \"\"\"Creates a unique key for a race based on venue, number, and time.\"\"\"\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Any]) -> List[Any]:\n        \"\"\"Merges duplicate race entries, stacking runner odds from multiple sources.\"\"\"\n        race_map: Dict[str, Any] = {}\n        for race in races:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                # Merge runners and their odds\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        # If runner exists, update its odds dict with new source\n                        runner_map[new_runner.number].odds.update(new_runner.odds)\n                    else:\n                        # If runner is new, add it to the race\n                        existing_race.runners.append(new_runner)\n        return list(race_map.values())\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        target_adapters = self.adapters\n        if source_filter:\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n\n        for result in results:\n            if isinstance(result, Exception):\n                self.log.error(\"Adapter fetch failed unexpectedly in gather\", error=result, exc_info=False)\n                continue\n\n            adapter_name, adapter_result, duration = result\n            source_info = adapter_result.get('source_info', {})\n            source_info['fetch_duration'] = round(duration, 2)\n            source_infos.append(source_info)\n\n            if source_info.get('status') == 'SUCCESS':\n                all_races.extend(adapter_result.get('races', []))\n\n        # --- SYNTHESIS: De-duplicate the stacked races ---\n        deduped_races = self._dedupe_races(all_races)\n\n        return {\n            \"date\": datetime.strptime(date, '%Y-%m-%d').date(),\n            \"races\": deduped_races,\n            \"sources\": source_infos,\n            \"metadata\": {\n                'fetch_time': datetime.now(),\n                'sources_queried': [a.source_name for a in target_adapters],\n                'sources_successful': len([s for s in source_infos if s['status'] == 'SUCCESS']),\n                'sources_failed': len([s for s in source_infos if s['status'] == 'FAILED']),\n                'failed_sources_list': [s['name'] for s in source_infos if s['status'] == 'FAILED'],\n                'total_races': len(deduped_races)\n            }\n        }",
    "python_service/logging_config.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Logging Configuration\n# ==============================================================================\n# This module, restored by the Great Correction, provides a centralized\n# configuration for structured logging using structlog.\n# ==============================================================================\n\nimport logging\nimport sys\nimport structlog\n\ndef configure_logging():\n    \"\"\"Configures structlog for JSON-based structured logging.\"\"\"\n    logging.basicConfig(\n        format=\"%(message)s\",\n        stream=sys.stdout,\n        level=logging.INFO,\n    )\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.add_logger_name,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )",
    "python_service/models.py": "# python_service/models.py\n\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List, Optional, Dict\nfrom datetime import datetime, date\nfrom decimal import Decimal\n\nclass OddsData(BaseModel):\n    win: Optional[Decimal] = None\n    source: str\n    last_updated: datetime\n\n    @field_validator('win')\n    def win_must_be_positive(cls, v):\n        if v is not None and v <= Decimal(\"1.0\"):\n            raise ValueError('Odds must be greater than 1.0')\n        return v\n\nclass Runner(BaseModel):\n    number: int = Field(..., gt=0, lt=100)\n    name: str = Field(..., max_length=100)\n    scratched: bool = False\n    selection_id: Optional[int] = None # For Betfair Exchange integration\n    odds: Dict[str, OddsData] = Field(default_factory=dict)\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\nclass Race(BaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., gt=0, lt=21)\n    start_time: datetime\n    runners: List[Runner]\n    source: str\n    qualification_score: Optional[float] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n\n    @field_validator('runners')\n    def runner_numbers_must_be_unique(cls, v):\n        numbers = [r.number for r in v]\n        if len(numbers) != len(set(numbers)):\n            raise ValueError('Runner numbers must be unique within a race')\n        return v\n\nclass SourceInfo(BaseModel):\n    name: str\n    status: str\n    races_fetched: int\n    error_message: Optional[str] = None\n    fetch_duration: float\n\nclass FetchMetadata(BaseModel):\n    fetch_time: datetime\n    sources_queried: List[str]\n    sources_successful: int\n    total_races: int\n\nclass AggregatedResponse(BaseModel):\n    date: date\n    races: List[Race]\n    sources: List[SourceInfo]\n    metadata: FetchMetadata",
    "python_service/requirements.txt": "requests==2.31.0\npython-dotenv==1.0.0\npydantic==2.5.2\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\naiohttp==3.9.0\nhttpx==0.24.1\npytest==8.4.2\n\nstructlog\nrespx\npytest-asyncio\nstreamlit\npandas\ntabula-py\nlxml\nbeautifulsoup4\npikepdf\npydantic-settings==2.1.0\nslowapi==0.1.8\n\ntenacity\n",
    "python_service/security.py": "# python_service/security.py\n\nimport secrets\nfrom fastapi import Security, HTTPException, status, Depends\nfrom fastapi.security import APIKeyHeader\n\nfrom .config import Settings, get_settings\n\nAPI_KEY_NAME = \"X-API-Key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)\n\nasync def verify_api_key(\n    key: str = Security(api_key_header),\n    settings: Settings = Depends(get_settings)\n):\n    \"\"\"\n    Verifies the provided API key against the one in settings using a\n    timing-attack resistant comparison.\n    \"\"\"\n    if secrets.compare_digest(key, settings.API_KEY):\n        return True\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Invalid or missing API Key\"\n        )",
    "results_parser.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Chart Parser (The Carpenter)\n# ==============================================================================\n# This module is responsible for parsing the complex, semi-structured text\n# extracted from Equibase PDF race charts.\n# ==============================================================================\n\nimport re\nfrom typing import List\n\nclass ChartParser:\n    \"\"\"A sophisticated parser for Equibase PDF chart text.\"\"\"\n\n    def count_runners(self, chart_text: str) -> int:\n        \"\"\"\n        Counts the number of runners in a race by parsing the 'Past Performance\n        Running Line Preview' section, which is a reliable indicator of field size.\n        \"\"\"\n        lines = chart_text.split('\\n')\n        in_running_line_section = False\n        runner_count = 0\n\n        for line in lines:\n            # Heuristic to detect the start of the relevant section\n            if 'Past Performance Running Line Preview' in line:\n                in_running_line_section = True\n                continue\n\n            if in_running_line_section:\n                # Stop if we hit a blank line or the next major section\n                if not line.strip() or 'Trainers:' in line:\n                    break\n\n                # A valid runner line starts with a program number\n                if re.match(r'^\\d+', line.strip()):\n                    runner_count += 1\n\n        return runner_count\n\n# --- Example Usage (for testing and demonstration) ---\nif __name__ == '__main__':\n    # This is a sample text block based on the 'Rosetta Stone' provided by the Project Lead\n    SAMPLE_TEXT = (\"\"\"Last Raced Pgm Horse Name (Jockey) Wgt M/E PP Start 1/4 1/2 3/4 Str Fin Odds Comments\n19Aug23 7ELP8 11 J J's Joker (Arrieta, Francisco) 120 L 11 1 41/2 51/2 51 21/2 11/2 7.08 4-3p,5w1/4,bid1/8,edgd\n3Sep23 7KD7 2 Peek (Saez, Luis) 120 L 2 3 11/2 11/2 11 11 1/2 22 11.53 ins,dug in 2p1/8,bestd\n15Sep23 1CD4 1 Game Warden (Gaffalione, Tyler) 120 L 1 2 52 41 41/2 31 3Neck 2.81* ins,aim btw1/4,lck bid\nTotal WPS Pool: $280,617\nPgm Horse Win Place Show\n11 J J's Joker 16.16 8.00 5.78\n2 Peek 10.80 6.22\n1 Game Warden 3.78\nPast Performance Running Line Preview\nPgm Horse Name Start 1/4 1/2 3/4 Str Fin\n11 J J's Joker 1 42 1/2 53 1/2 52 1/2 21 1/2 11/2\n2 Peek 3 11/2 11/2 11 11 1/2 21/2\n1 Game Warden 2 53 42 1/2 42 32 32 1/2\n4 Runningforjoy 9 75 1/2 64 63 1/2 53 42 3/4\n5 Archie the Giza 7 96 1/2 87 85 1/2 64 1/2 54 1/4\n8 Cafe Racer 4 21/2 21/2 31 43 65\n9 Barnstorming 5 31 31 1/2 21 76 1/2 711 1/4\n6 Cashmeup 8 65 75 1/2 74 87 1/2 811 1/2\n10 Texas Pride 12 1214 1/2 1213 1/2 1114 1113 3/4 913\n3 Active Duty 6 86 1/2 1110 1214 1/4 1013 1/2 1013 1/2\n12 Surface to Air 10 108 99 97 1/2 911 1114\n7 Dr Kringle 11 119 1/2 1010 108 1214 3/4 1217 3/4\nTrainers: 11 - Hartman, Chris; 2 - Arnold, II, George; 1 - Joseph, Jr, Saffie; 4 - Tomlinson, Michael; 5 - Medina, Robert; 8 - Stall, Jr, Albert; 9 - Cox, Brad;\n\"\"\")\n\n    print(\"--- Testing ChartParser with sample data ---\")\n    parser = ChartParser()\n    runner_count = parser.count_runners(SAMPLE_TEXT)\n\n    print(f\"Runner count found: {runner_count}\")\n    # Expected Output: 12\n    assert runner_count == 12\n    print(\"Test passed!\")",
    "run_fortuna.bat": "@echo off\nREM ============================================================================\nREM  Fortuna Faucet: Master Launcher (v2 - Perfected)\nREM ============================================================================\n\necho [INFO] Launching the Fortuna Faucet application...\n\nREM --- Launch Backend ---\necho [BACKEND] Starting FastAPI server... (New window)\nstart \"Fortuna Backend\" cmd /c \"call .\\\\.venv\\\\Scripts\\\\activate.bat && cd python_service && uvicorn api:app --reload\"\n\nREM --- Launch Frontend ---\necho [FRONTEND] Starting Next.js development server... (New window)\nstart \"Fortuna Frontend\" cmd /c \"cd web_platform/frontend && npm run dev\"\n\nREM --- Open Browser ---\necho [UI] Waiting 5 seconds for the frontend server to initialize...\ntimeout /t 5 /nobreak >nul\necho [UI] Opening the Fortuna Faucet dashboard in your default browser...\nstart http://localhost:3000\n\necho [SUCCESS] Both pillars of Fortuna Faucet have been launched.\n\n:eof",
    "setup_windows.bat": "@echo off\nREM ============================================================================\nREM  Project Gemini: WHOLE-SYSTEM Windows Setup Script\nREM ============================================================================\n\necho [INFO] Starting full-stack setup for Project Gemini...\n\nREM --- Section 1: Python Backend Setup ---\necho.\necho [BACKEND] Checking for Python installation...\npython --version >nul 2>&1\nif %errorlevel% neq 0 (\n    echo [ERROR] Python is not found. Please install Python 3.8+ and add to PATH.\n    goto :eof\n)\necho [BACKEND] Python found.\n\necho [BACKEND] Creating Python virtual environment in '.\\\\.venv\\\\'...\nif not exist .\\\\.venv ( python -m venv .venv )\n\necho [BACKEND] Installing dependencies from 'python_service/requirements.txt'...\ncall .\\\\.venv\\\\Scripts\\\\activate.bat && pip install -r python_service/requirements.txt\nif %errorlevel% neq 0 (\n    echo [ERROR] Backend setup failed.\n    goto :eof\n)\necho [SUCCESS] Python backend setup complete.\n\nREM --- Section 2: TypeScript Frontend Setup ---\necho.\necho [FRONTEND] Checking for Node.js installation...\nnode --version >nul 2>&1\nif %errorlevel% neq 0 (\n    echo [ERROR] Node.js is not found. Please install Node.js (LTS).\n    goto :eof\n)\necho [FRONTEND] Node.js found.\n\necho [FRONTEND] Installing dependencies from 'package.json'...\ncd web_platform/frontend\nnpm install\nif %errorlevel% neq 0 (\n    echo [ERROR] Frontend setup failed. Check npm errors.\n    cd ../..\n    goto :eof\n)\ncd ../..\necho [SUCCESS] TypeScript frontend setup complete.\n\nREM --- Final Instructions ---\necho.\necho ============================================================================\nREM  FULL-STACK SETUP COMPLETE!\nREM  You can now launch the entire application with 'run_fortuna.bat'\nREM ============================================================================\n\n:eof",
    "web_platform/frontend/package.json": "{\n  \"name\": \"frontend\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": { \"dev\": \"next dev\", \"build\": \"next build\", \"start\": \"next start\" },\n  \"dependencies\": {\n    \"next\": \"14.1.0\",\n    \"react\": \"^18\",\n    \"react-dom\": \"^18\",\n    \"socket.io-client\": \"^4.7.4\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"autoprefixer\": \"^10.0.1\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.3.0\",\n    \"typescript\": \"^5\"\n  }\n}",
    "web_platform/frontend/postcss.config.js": "module.exports = {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n}",
    "web_platform/frontend/src/app/page.tsx": "'use client';\nimport React, { useState, useEffect } from 'react';\nimport { Activity, Settings, Minimize, Filter } from 'lucide-react';\nimport RaceCard from '../components/RaceCard'; // Assuming component is in a sub-folder\n// Mock data and other components would be imported here in a real scenario\n\nconst FortunaDesktopMockup = () => {\n  const [data, setData] = useState(null);\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    async function fetchData() {\n        setLoading(true);\n        try {\n            const response = await fetch('/api/races/qualified/trifecta');\n            if (!response.ok) throw new Error('Network response was not ok');\n            const raceData = await response.json();\n            setData({ races: raceData }); // Adapt to the structure needed\n        } catch (error) {\n            console.error(\"Fetch error:\", error);\n        } finally {\n            setLoading(false);\n        }\n    }\n    fetchData();\n  }, []);\n\n  if (loading || !data) {\n    return <div className=\"min-h-screen bg-gray-900 flex items-center justify-center\"><div className=\"animate-spin rounded-full h-16 w-16 border-b-2 border-purple-500\\\"></div></div>;\n  }\n\n  const qualifiedRaces = data.races.filter(r => r.qualification_score > 0);\n\n  return (\n    <div className=\"min-h-screen bg-gradient-to-br from-gray-900 via-gray-800 to-gray-900\">\n        <header className=\"border-b border-gray-700/50 bg-gray-900/50 backdrop-blur-xl sticky top-0 z-40 px-6 py-4\">\n            <div className=\"flex items-center justify-between\">\n                <div className=\"flex items-center gap-3\">\n                    <div className=\"w-10 h-10 bg-gradient-to-br from-purple-500 to-pink-500 rounded-xl flex items-center justify-center shadow-lg\"><Activity className=\"w-6 h-6 text-white\" /></div>\n                    <div><h1 className=\"text-xl font-bold text-white\\\">Fortuna Faucet</h1></div>\n                </div>\n            </div>\n        </header>\n        <main className=\"p-6\">\n            <div className=\"grid grid-cols-2 gap-4\">\n              {qualifiedRaces.map(race => (\n                <RaceCard key={race.id} race={race} onClick={() => alert(`Selected ${race.track} Race ${race.raceNumber}`)} />\n              ))}\n            </div>\n        </main>\n    </div>\n  );\n};\n\nexport default FortunaDesktopMockup;",
    "web_platform/frontend/src/components/RaceCard.tsx": "'use client';\nimport { Trophy, Clock, TrendingUp, ChevronRight } from 'lucide-react';\n\nconst RaceCard = ({ race, onClick }) => {\n  const activeRunners = race.runners.filter(r => !r.scratched);\n  const sortedRunners = [...activeRunners].sort((a, b) => a.bestOdds - b.bestOdds);\n  const topRunners = sortedRunners.slice(0, 3);\n\n  return (\n    <div onClick={onClick} className=\"bg-gradient-to-br from-gray-800/90 to-gray-900/90 border border-gray-700/50 rounded-lg p-4 hover:border-purple-500/50 transition-all cursor-pointer group\">\n      <div className=\"flex items-start justify-between mb-3\">\n        <div className=\"flex-1\">\n          <h3 className=\"text-base font-bold text-white mb-1 group-hover:text-purple-400 transition-colors\">{race.track} - Race {race.raceNumber}</h3>\n          <div className=\"flex gap-2 text-xs text-gray-400\"><span>{race.distance}</span><span>\u2022</span><span>{race.surface}</span><span>\u2022</span><span>{activeRunners.length} runners</span></div>\n        </div>\n        {race.qualified && (\n          <div className=\"flex items-center gap-1.5 bg-purple-500/20 px-2.5 py-1 rounded-full\">\n            <TrendingUp className=\"w-3.5 h-3.5 text-purple-400\" />\n            <span className=\"text-xs font-semibold text-purple-400\">{race.qualificationScore}% Match</span>\n          </div>\n        )}\n      </div>\n      <div className=\"space-y-2\">\n        <h4 className=\"text-xs font-semibold text-gray-400 uppercase tracking-wider\">Top Contenders</h4>\n        {topRunners.map((runner, idx) => (\n          <div key={runner.number} className=\"flex items-center justify-between bg-gray-900/50 rounded-md p-2.5 border border-gray-700/50\">\n            <div className=\"flex items-center gap-2.5\">\n              <div className={`w-7 h-7 rounded-full flex items-center justify-center font-bold text-xs ${idx === 0 ? 'bg-yellow-500/20 text-yellow-400 border border-yellow-500/30' : ''} ${idx === 1 ? 'bg-gray-500/20 text-gray-300 border border-gray-500/30' : ''} ${idx === 2 ? 'bg-orange-500/20 text-orange-400 border border-orange-500/30' : ''}`}>{runner.number}</div>\n              <div className=\"flex flex-col\"><span className=\"text-white font-medium text-sm\">{runner.name}</span><span className=\"text-xs text-gray-500\">{runner.jockey}</span></div>\n            </div>\n            <div className=\"text-right\"><div className=\"text-base font-bold text-emerald-400\">{runner.bestOdds.toFixed(2)}</div><div className=\"text-xs text-gray-500\\\">best odds</div></div>\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n};\n\nexport default RaceCard;",
    "web_platform/frontend/tailwind.config.ts": "import type { Config } from 'tailwindcss'\n\nconst config: Config = {\n  content: [\n    './src/pages/**/*.{js,ts,jsx,tsx,mdx}',\n    './src/components/**/*.{js,ts,jsx,tsx,mdx}',\n    './app/**/*.{js,ts,jsx,tsx,mdx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\nexport default config",
    "web_platform/frontend/tsconfig.json": "{\n  \"compilerOptions\": {\n    \"lib\": [\n      \"dom\",\n      \"dom.iterable\",\n      \"esnext\"\n    ],\n    \"allowJs\": true,\n    \"skipLibCheck\": true,\n    \"strict\": false,\n    \"noEmit\": true,\n    \"incremental\": true,\n    \"esModuleInterop\": true,\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"node\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"jsx\": \"preserve\",\n    \"plugins\": [\n      {\n        \"name\": \"next\"\n      }\n    ]\n  },\n  \"include\": [\n    \"next-env.d.ts\",\n    \".next/types/**/*.ts\",\n    \"**/*.ts\",\n    \"**/*.tsx\"\n  ],\n  \"exclude\": [\n    \"node_modules\"\n  ]\n}\n"
}