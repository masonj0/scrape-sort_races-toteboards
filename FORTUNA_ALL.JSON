{
    "python_service/__init__.py": "# This file makes the python_service directory a Python package.",
    "python_service/api.py": "# python_service/api.py\n\nimport structlog\nfrom datetime import datetime, date\nfrom typing import List\nfrom fastapi import FastAPI, HTTPException, Request, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nfrom contextlib import asynccontextmanager\n\nfrom .config import get_settings\nfrom .engine import OddsEngine\nfrom .models import Race\nfrom .security import verify_api_key\nfrom .logging_config import configure_logging\nfrom .analyzer import AnalyzerEngine\n\nlog = structlog.get_logger()\n\n# Define the lifespan context manager for robust startup/shutdown\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Manage the application's lifespan. On startup, it initializes the OddsEngine\n    with validated settings and attaches it to the app state. On shutdown, it\n    properly closes the engine's resources.\n    \"\"\"\n    configure_logging()\n    settings = get_settings()\n    app.state.engine = OddsEngine(config=settings)\n    log.info(\"Server startup: Configuration validated and OddsEngine initialized.\")\n    yield\n    # Clean up the engine resources\n    await app.state.engine.close()\n    log.info(\"Server shutdown: HTTP client resources closed.\")\n\nlimiter = Limiter(key_func=get_remote_address)\n\n# Pass the lifespan manager to the FastAPI app\napp = FastAPI(title=\"Checkmate Ultimate Solo API\", version=\"2.1\", lifespan=lifespan)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\", \"http://localhost:3001\"],\n    allow_credentials=True, allow_methods=[\"GET\"], allow_headers=[\"*\"]\n)\n\n# Dependency function to get the engine instance from the app state\ndef get_engine(request: Request) -> OddsEngine:\n    return request.app.state.engine\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(request: Request, engine: OddsEngine = Depends(get_engine), _=Depends(verify_api_key)):\n    \"\"\"Provides a list of health statuses for all adapters, required by the new frontend blueprint.\"\"\"\n    try:\n        statuses = engine.get_all_adapter_statuses()\n        return statuses\n    except Exception as e:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races/qualified/{analyzer_name}\", response_model=List[Race])\n@limiter.limit(\"30/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: date = datetime.now().date(),\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key)\n):\n    \"\"\"\n    Gets all races for a given date, filters them for qualified betting\n    opportunities, and returns the qualified races.\n    \"\"\"\n    try:\n        date_str = race_date.strftime('%Y-%m-%d')\n        aggregated_data = await engine.fetch_all_odds(date_str)\n\n        # We need to deserialize the race data before it can be used by the analyzer\n        # This assumes the raw data from the engine is a list of dicts\n        races = [Race.model_validate(r) for r in aggregated_data.get('races', [])]\n\n        analyzer_engine = AnalyzerEngine()\n        # In the future, kwargs could come from the request's query params\n        analyzer = analyzer_engine.get_analyzer(analyzer_name)\n        qualified_races = analyzer.qualify_races(races)\n        return qualified_races\n    except Exception as e:\n        log.error(\"Error in /api/races/qualified\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races\")\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: date = datetime.now().date(),\n    source: str = None,\n    engine: OddsEngine = Depends(get_engine),\n    _=Depends(verify_api_key)\n):\n    try:\n        date_str = race_date.strftime('%Y-%m-%d')\n        aggregated_data = await engine.fetch_all_odds(date_str, source)\n        return aggregated_data\n    except Exception as e:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport structlog\nimport httpx\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Tuple\n\nfrom .adapters.base import BaseAdapter\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.pointsbet_adapter import PointsBetAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.greyhound_adapter import GreyhoundAdapter\n\nclass OddsEngine:\n    def __init__(self, config):\n        self.config = config\n        self.log = structlog.get_logger(self.__class__.__name__)\n        self.adapters: List[BaseAdapter] = [\n            BetfairAdapter(config=self.config),\n            TVGAdapter(config=self.config),\n            RacingAndSportsAdapter(config=self.config),\n            PointsBetAdapter(config=self.config),\n            HarnessAdapter(config=self.config),\n            GreyhoundAdapter(config=self.config)\n        ]\n        self.http_client = httpx.AsyncClient()\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        \"\"\"Returns the health status of all registered adapters.\"\"\"\n        statuses = []\n        for adapter in self.adapters:\n            statuses.append(adapter.get_status())\n        return statuses\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapter, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"Wraps an adapter's fetch call to accurately measure its duration.\"\"\"\n        start_time = datetime.now()\n        try:\n            result = await adapter.fetch_races(date, self.http_client)\n            duration = (datetime.now() - start_time).total_seconds()\n            return (adapter.source_name, result, duration)\n        except Exception as e:\n            duration = (datetime.now() - start_time).total_seconds()\n            self.log.error(\"Adapter raised an unhandled exception\", adapter=adapter.source_name, error=e)\n            # Propagate the exception along with the timing info\n            raise e from None\n\n    async def fetch_all_odds(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        target_adapters = self.adapters\n        if source_filter:\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        successful_results = []\n        source_infos = []\n        all_races = []\n\n        for result in results:\n            if isinstance(result, Exception):\n                # This path is for unexpected errors in the wrapper or adapter itself\n                # The adapter name is not easily available here, logging is key\n                self.log.error(\"A fetch task failed unexpectedly\", error=result, exc_info=True)\n                continue\n\n            adapter_name, adapter_result, fetch_duration = result\n\n            source_infos.append({\n                'name': adapter_name,\n                'status': adapter_result['source_info']['status'],\n                'races_fetched': adapter_result['source_info']['races_fetched'],\n                'error_message': adapter_result['source_info']['error_message'],\n                'fetch_duration': round(fetch_duration, 2) # Use the accurate, individual duration\n            })\n\n            if adapter_result['source_info']['status'] == 'SUCCESS':\n                all_races.extend(adapter_result.get('races', []))\n\n        return {\n            \"date\": datetime.strptime(date, '%Y-%m-%d').date(),\n            \"races\": all_races,\n            \"sources\": source_infos,\n            \"metadata\": {\n                'fetch_time': datetime.now(),\n                'sources_queried': [a.source_name for a in target_adapters],\n                'sources_successful': len([s for s in source_infos if s['status'] == 'SUCCESS']),\n                'total_races': len(all_races)\n            }\n        }",
    "python_service/models.py": "# python_service/models.py\n\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List, Optional, Dict\nfrom datetime import datetime, date\nfrom decimal import Decimal\n\nclass OddsData(BaseModel):\n    win: Optional[Decimal] = None\n    source: str\n    last_updated: datetime\n\n    @field_validator('win')\n    def win_must_be_positive(cls, v):\n        if v is not None and v <= 1.0:\n            raise ValueError('Odds must be greater than 1.0')\n        return v\n\nclass Runner(BaseModel):\n    number: int = Field(..., gt=0, lt=100)\n    name: str = Field(..., max_length=100)\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n\nclass Race(BaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., gt=0, lt=21)\n    start_time: datetime\n    runners: List[Runner]\n\n    @field_validator('runners')\n    def runner_numbers_must_be_unique(cls, v):\n        numbers = [r.number for r in v]\n        if len(numbers) != len(set(numbers)):\n            raise ValueError('Runner numbers must be unique within a race')\n        return v\n\nclass SourceInfo(BaseModel):\n    name: str\n    status: str\n    races_fetched: int\n    error_message: Optional[str] = None\n    fetch_duration: float\n\nclass FetchMetadata(BaseModel):\n    fetch_time: datetime\n    sources_queried: List[str]\n    sources_successful: int\n    total_races: int\n\nclass AggregatedResponse(BaseModel):\n    date: date\n    races: List[Race]\n    sources: List[SourceInfo]\n    metadata: FetchMetadata",
    "python_service/analyzer.py": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Type, Optional\nimport structlog\nfrom decimal import Decimal\n\nfrom python_service.models import Race, Runner\n\nlog = structlog.get_logger(__name__)\n\n# --- Corrected Helper Function ---\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Helper to find the best (lowest) win odds for a runner from any source.\"\"\"\n    if not runner.odds:\n        return None\n\n    valid_odds = [\n        odds_data.win\n        for odds_data in runner.odds.values()\n        if odds_data and odds_data.win is not None\n    ]\n\n    return min(valid_odds) if valid_odds else None\n\n\n# --- 1. The Abstract Base Class (The Interface) ---\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\n# --- 2. The Concrete Implementation (The First Plugin) ---\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races to find opportunities based on the 'Trifecta of Factors'.\"\"\"\n    def __init__(self, max_field_size: int = 10, min_favorite_odds: float = 2.5, min_second_favorite_odds: float = 4.0):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        log.info(\n            \"TrifectaAnalyzer initialized with TRUE TRIFECTA logic\",\n            max_field_size=self.max_field_size,\n            min_favorite_odds=self.min_favorite_odds,\n            min_second_favorite_odds=self.min_second_favorite_odds\n        )\n\n    def qualify_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Filters a list of races based on the true handicapping criteria.\"\"\"\n        qualified_races = []\n        for race in races:\n            active_runners = [r for r in race.runners if not r.scratched]\n\n            # FACTOR 2: Field size, the lower the better\n            if len(active_runners) > self.max_field_size:\n                log.debug(f\"Race {race.id} disqualified: Field size too large ({len(active_runners)} > {self.max_field_size}).\")\n                continue\n\n            runners_with_odds = []\n            for runner in active_runners:\n                best_odds = _get_best_win_odds(runner)\n                if best_odds is not None:\n                    runners_with_odds.append((runner, best_odds))\n\n            if len(runners_with_odds) < 2:\n                log.debug(f\"Race {race.id} disqualified: Not enough runners with odds.\")\n                continue\n\n            runners_with_odds.sort(key=lambda x: x[1])\n\n            favorite_odds = runners_with_odds[0][1]\n            second_favorite_odds = runners_with_odds[1][1]\n\n            # FACTOR 3: Odds for the favorite horse cannot be 'chalky'\n            if favorite_odds < self.min_favorite_odds:\n                log.debug(f\"Race {race.id} disqualified: Favorite odds too low ({favorite_odds} < {self.min_favorite_odds}).\")\n                continue\n\n            # FACTOR 1: Second-favorite odds, the higher the better\n            if second_favorite_odds < self.min_second_favorite_odds:\n                log.debug(f\"Race {race.id} disqualified: Second favorite odds too low ({second_favorite_odds} < {self.min_second_favorite_odds}).\")\n                continue\n\n            qualified_races.append(race)\n\n        log.info(\"True Trifecta qualification complete\", total_races=len(races), qualified_races=len(qualified_races))\n        return qualified_races\n\n\n# --- 3. The Orchestrator (The Engine) ---\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer('trifecta', TrifectaAnalyzer)\n        log.info(\"AnalyzerEngine discovered plugins\", available_analyzers=list(self.analyzers.keys()))\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)",
    "python_service/security.py": "# python_service/security.py\n\nfrom fastapi import Security, HTTPException, status, Depends\nfrom fastapi.security import APIKeyHeader\n\nfrom .config import Settings, get_settings\n\nAPI_KEY_NAME = \"X-API-Key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)\n\nasync def verify_api_key(\n    key: str = Security(api_key_header),\n    settings: Settings = Depends(get_settings)\n):\n    \"\"\"\n    Verifies that the provided API key matches the one in our settings.\n    The settings are injected as a dependency, making this testable.\n    \"\"\"\n    if key == settings.API_KEY:\n        return True\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Invalid or missing API Key\"\n        )",
    "python_service/adapters/__init__.py": "# python_service/adapters/__init__.py\n\nfrom .tvg_adapter import TVGAdapter\nfrom .betfair_adapter import BetfairAdapter\nfrom .pointsbet_adapter import PointsBetAdapter\nfrom .racing_and_sports_adapter import RacingAndSportsAdapter\n\n# Define the public API for the adapters package, making it easy for the\n# orchestrator to discover and use them.\n__all__ = [\n    \"TVGAdapter\",\n    \"BetfairAdapter\",\n    \"PointsBetAdapter\",\n    \"RacingAndSportsAdapter\",\n]",
    "python_service/adapters/base.py": "# python_service/adapters/base.py\n\nimport asyncio\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\nimport httpx\n\nclass BaseAdapter(ABC):\n    def __init__(self, source_name: str, base_url: str, timeout: int = 30, max_retries: int = 3):\n        self.source_name = source_name\n        self.base_url = base_url\n        self.timeout = timeout\n        self.max_retries = max_retries\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    @abstractmethod\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    async def make_request(self, http_client: httpx.AsyncClient, method: str, url: str, **kwargs) -> Any:\n        retry_count = 0\n        while retry_count < self.max_retries:\n            try:\n                full_url = f\"{self.base_url}{url}\"\n                self.logger.info(f\"Requesting {method} {full_url}\")\n                response = await http_client.request(method, full_url, timeout=self.timeout, **kwargs)\n                response.raise_for_status()\n                return response.json()\n            except (httpx.RequestError, httpx.HTTPStatusError) as e:\n                self.logger.warning(f\"Request failed for {self.source_name} (attempt {retry_count + 1}/{self.max_retries}): {e}\")\n                retry_count += 1\n                if retry_count >= self.max_retries:\n                    self.logger.error(f\"Max retries exceeded for {self.source_name}. Aborting.\")\n                    raise\n                backoff = 1 * (2 ** retry_count) # Exponential backoff\n                await asyncio.sleep(backoff)\n        raise Exception(\"make_request failed after max retries\")\n\n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Returns a dictionary representing the adapter's default status.\"\"\"\n        return {\"adapter_name\": self.source_name, \"status\": \"OK\"}\n",
    "python_service/adapters/utils.py": "# ==============================================================================\n# == Centralized Adapter Utilities\n# ==============================================================================\n# This module provides shared, battle-tested functions for all adapters to use,\n# ensuring consistency and adhering to the DRY principle.\n# ==============================================================================\n\nfrom typing import Union\n\ndef parse_odds(odds: Union[str, int, float]) -> float:\n    \"\"\"\n    Parses various odds formats (e.g., fractional '10/1', decimal 11.0)\n    into a standardized decimal float.\n\n    Returns a default high odds value on failure to prevent crashes.\n    \"\"\"\n    if isinstance(odds, (int, float)):\n        return float(odds)\n\n    if isinstance(odds, str):\n        try:\n            # Handle fractional odds (e.g., \"10/1\", \"5/2\")\n            if \"/\" in odds:\n                numerator, denominator = map(int, odds.split('/'))\n                if denominator == 0: return 999.0\n                return 1.0 + (numerator / denominator)\n\n            # Handle \"evens\"\n            if odds.lower() == 'evens':\n                return 2.0\n\n            # Handle simple decimal strings\n            return float(odds)\n        except (ValueError, TypeError):\n            # Return a high, but valid, number for unparseable odds\n            return 999.0\n\n    return 999.0",
    "python_service/adapters/betfair_adapter.py": "# python_service/adapters/betfair_adapter.py\n\nimport os\nimport re\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any, List\nimport httpx\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\n\nclass BetfairAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(source_name=\"Betfair\", base_url=\"https://api.betfair.com/exchange/betting/rest/v1.0/\")\n        self.app_key = config.BETFAIR_APP_KEY\n        self.username = config.BETFAIR_USERNAME\n        self.password = config.BETFAIR_PASSWORD\n        self.session_token: str | None = None\n        self.token_expiry: datetime | None = None\n\n    async def _authenticate(self, http_client: httpx.AsyncClient):\n        if self.session_token and self.token_expiry and self.token_expiry > datetime.now():\n            return\n        if not all([self.app_key, self.username, self.password]):\n            raise Exception(\"Betfair credentials not set in environment.\")\n\n        auth_url = \"https://identitysso.betfair.com/api/login\"\n        headers = {'X-Application': self.app_key, 'Content-Type': 'application/x-www-form-urlencoded'}\n        payload = f'username={self.username}&password={self.password}'\n\n        response = await http_client.post(auth_url, headers=headers, content=payload, timeout=20)\n        response.raise_for_status()\n        data = response.json()\n        if data.get('status') == 'SUCCESS':\n            self.session_token = data.get('token')\n            self.token_expiry = datetime.now() + timedelta(hours=4)\n        else:\n            raise Exception(f\"Betfair authentication failed: {data.get('error')}\")\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        try:\n            await self._authenticate(http_client)\n\n            market_filter = {\n                \"eventTypeIds\": [\"7\"], \"marketTypeCodes\": [\"WIN\"],\n                \"marketStartTime\": {\"from\": f\"{date}T00:00:00Z\", \"to\": f\"{date}T23:59:59Z\"}\n            }\n            headers = {\"X-Application\": self.app_key, \"X-Authentication\": self.session_token, \"Content-Type\": \"application/json\"}\n            json_payload = {\n                \"jsonrpc\": \"2.0\", \"method\": \"SportsAPING/v1.0/listMarketCatalogue\",\n                \"params\": {\n                    \"filter\": market_filter,\n                    \"marketProjection\": [\"EVENT\", \"RUNNER_DESCRIPTION\", \"MARKET_START_TIME\"],\n                    \"maxResults\": 1000\n                }, \"id\": 1\n            }\n\n            # BUG FIX: Use the resilient self.make_request method\n            markets_response = await self.make_request(http_client, 'POST', '/', headers=headers, json=json_payload)\n            if not markets_response or 'result' not in markets_response:\n                return {'races': []}\n\n            all_races = [self._parse_betfair_race(market) for market in markets_response['result']]\n            fetch_duration = (datetime.now() - start_time).total_seconds()\n            return {\n                # FEEDBACK FIX: Use .model_dump() instead of .dict()\n                'races': [r.model_dump() for r in all_races],\n                'source_info': {\n                    'name': self.source_name, 'status': 'SUCCESS',\n                    'races_fetched': len(all_races), 'error_message': None,\n                    'fetch_duration': fetch_duration\n                }\n            }\n        except Exception as e:\n            self.logger.error(f\"Failed to fetch races from Betfair: {e}\", exc_info=True)\n            raise\n\n    def _parse_betfair_race(self, market: Dict[str, Any]) -> Race:\n        venue = market.get('event', {}).get('venue', 'Unknown Venue')\n        start_time = datetime.fromisoformat(market['marketStartTime'].replace('Z', '+00:00'))\n\n        # BUG FIX: Use a robust regex for race number parsing\n        match = re.search(r'[Rr](\\d+)', market.get('marketName', ''))\n        race_number = int(match.group(1)) if match else 0\n\n        runners = []\n        for runner_data in market.get('runners', []):\n            if runner_data.get('status') == 'ACTIVE':\n                runners.append(Runner(\n                    number=runner_data.get('sortPriority', 0),\n                    name=runner_data.get('runnerName', 'Unknown Runner'),\n                    scratched=False # Explicitly set required field\n                ))\n\n        return Race(\n            id=f\"bf_{market['marketId']}\",\n            venue=venue, race_number=race_number, start_time=start_time,\n            runners=runners,\n            source=self.source_name # BUG FIX: Add the required 'source' field\n        )",
    "python_service/adapters/pointsbet_adapter.py": "# python_service/adapters/pointsbet_adapter.py\n\nimport structlog\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nfrom decimal import Decimal\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\n\nlog = structlog.get_logger(__name__)\n\nclass PointsBetAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"PointsBet\",\n            base_url=\"https://api.au.pointsbet.com\"\n        )\n        self.api_key = config.POINTSBET_API_KEY\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        \"\"\"Fetches upcoming thoroughbred races from the PointsBet API.\"\"\"\n        start_time = datetime.now()\n\n        if not self.api_key:\n            log.warning(\"PointsBetAdapter: POINTSBET_API_KEY not set. Skipping.\")\n            return self._format_response([], start_time, is_success=False, error_message=\"ConfigurationError: Token not set\")\n\n        endpoint = \"/api/v2/racing/futures\"\n        headers = {\"api-key\": self.api_key}\n        params = {\"sportId\": 21}  # Assuming 21 is for Thoroughbred Racing\n\n        try:\n            response_json = await self.make_request(http_client, 'GET', endpoint, headers=headers, params=params)\n            if not response_json or \"events\" not in response_json:\n                log.warning(\"PointsBetAdapter: No 'events' in response or empty response.\")\n                return self._format_response([], start_time)\n\n            all_races = self._parse_races(response_json[\"events\"])\n\n            # The API is for futures, so we must filter by the requested date\n            all_races = [race for race in all_races if race.start_time.strftime('%Y-%m-%d') == date]\n\n            return self._format_response(all_races, start_time)\n        except Exception as e:\n            log.error(\"Failed to fetch races from PointsBet\", exc_info=True)\n            raise\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': [r.model_dump() for r in races],\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_races(self, events: List[Dict[str, Any]]) -> List[Race]:\n        races = []\n        for event in events:\n            if not event.get(\"outcomes\"):  # Skip events without runners\n                continue\n\n            try:\n                race = Race(\n                    id=f\"pb_{event['id']}\",\n                    venue=event.get(\"competitionName\", \"Unknown Venue\"),\n                    race_number=event.get(\"eventNumber\", 0),\n                    start_time=datetime.fromisoformat(event[\"startTime\"].replace(\"Z\", \"+00:00\")),\n                    runners=self._parse_runners(event[\"outcomes\"]),\n                    source=self.source_name\n                )\n                races.append(race)\n            except Exception as e:\n                log.error(\n                    \"PointsBetAdapter: Error parsing event\",\n                    error=str(e),\n                    event_id=event.get('id', 'N/A')\n                )\n        return races\n\n    def _parse_runners(self, outcomes: List[Dict[str, Any]]) -> List[Runner]:\n        runners = []\n        for i, outcome in enumerate(outcomes):\n            win_odds_data = next((p for p in outcome.get(\"prices\", []) if p[\"priceType\"] == \"FixedWin\"), None)\n\n            odds_dict = {}\n            if win_odds_data and win_odds_data.get(\"price\"):\n                try:\n                    win_odds = Decimal(str(win_odds_data[\"price\"]))\n                    odds_dict[self.source_name] = OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())\n                except Exception:\n                    win_odds = None\n\n            runners.append(Runner(\n                number=i + 1,  # Placeholder as number is not provided\n                name=outcome.get(\"name\", \"Unknown Runner\"),\n                scratched=outcome.get(\"isSuspended\", False),\n                odds=odds_dict\n            ))\n        return runners\n",
    "python_service/adapters/racing_and_sports_adapter.py": "# python_service/adapters/racing_and_sports_adapter.py\n\nimport os\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner\n\nclass RacingAndSportsAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"Racing and Sports\",\n            base_url=\"https://api.racingandsports.com.au/\"\n        )\n        self.api_token = config.RACING_AND_SPORTS_TOKEN\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        all_races: List[Race] = []\n        headers = {\"Authorization\": f\"Bearer {self.api_token}\", \"Accept\": \"application/json\"}\n\n        if not self.api_token:\n            return self._format_response(all_races, start_time, is_success=False, error_message=\"ConfigurationError: Token not set\")\n\n        try:\n            meetings_url = \"v1/racing/meetings\"\n            params = {\"date\": date, \"jurisdiction\": \"AUS\"}\n            meetings_data = await self.make_request(http_client, 'GET', meetings_url, headers=headers, params=params)\n\n            if not meetings_data or not meetings_data.get('meetings'):\n                return self._format_response(all_races, start_time)\n\n            for meeting in meetings_data['meetings']:\n                for race_summary in meeting.get('races', []):\n                    try:\n                        parsed_race = self._parse_ras_race(meeting, race_summary)\n                        all_races.append(parsed_race)\n                    except Exception as e:\n                        self.logger.error(f\"Failed to parse race for meeting {meeting.get('venueName')}: {e}\", exc_info=True)\n\n            return self._format_response(all_races, start_time)\n        except Exception as e:\n            self.logger.error(f\"Failed to fetch races from {self.source_name}: {e}\", exc_info=True)\n            raise\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': [r.model_dump() for r in races],\n            'source_info': {\n                'name': self.source_name, 'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races), 'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_ras_race(self, meeting: Dict[str, Any], race: Dict[str, Any]) -> Race:\n        runners = [Runner(number=rd.get('runnerNumber'), name=rd.get('horseName', 'Unknown'), scratched=rd.get('isScratched', False)) for rd in race.get('runners', [])]\n\n        return Race(\n            id=f\"ras_{race.get('raceId')}\",\n            venue=meeting.get('venueName', 'Unknown Venue'),\n            race_number=race.get('raceNumber'),\n            start_time=datetime.fromisoformat(race.get('startTime')),\n            runners=runners,\n            source=self.source_name\n        )",
    "python_service/adapters/tvg_adapter.py": "# python_service/adapters/tvg_adapter.py\n\nimport os\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport httpx\nfrom decimal import Decimal, InvalidOperation\n\nfrom .base import BaseAdapter\nfrom ..models import Race, Runner, OddsData\n\nclass TVGAdapter(BaseAdapter):\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"TVG\",\n            base_url=\"https://api.tvg.com/v3/\"\n        )\n        self.api_key = config.TVG_API_KEY\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        start_time = datetime.now()\n        all_races: List[Race] = []\n        headers = {\"Accept\": \"application/json\", \"X-API-Key\": self.api_key}\n\n        if not self.api_key:\n            self.logger.warning(f\"TVG_API_KEY not set. Adapter {self.source_name} will be skipped.\")\n            return self._format_response(all_races, start_time)\n\n        try:\n            tracks_url = \"tracks\"\n            tracks_params = {\"date\": date, \"country\": \"US\"}\n            tracks_response = await self.make_request(http_client, 'GET', tracks_url, headers=headers, params=tracks_params)\n\n            if not tracks_response or 'tracks' not in tracks_response:\n                self.logger.warning(\"TVG: No tracks found for the given date.\")\n                return self._format_response(all_races, start_time)\n\n            for track in tracks_response['tracks']:\n                try:\n                    races_url = f\"tracks/{track['code']}/races\"\n                    races_params = {\"date\": date}\n                    races_response = await self.make_request(http_client, 'GET', races_url, headers=headers, params=races_params)\n\n                    for race_summary in races_response.get('races', []):\n                        race_detail_url = f\"tracks/{track['code']}/races/{race_summary['number']}\"\n                        race_detail = await self.make_request(http_client, 'GET', race_detail_url, headers=headers)\n                        if race_detail:\n                            parsed_race = self._parse_tvg_race(track, race_detail)\n                            all_races.append(parsed_race)\n                except Exception as e:\n                    self.logger.error(f\"Failed to process track {track.get('name')}: {e}\", exc_info=True)\n\n            return self._format_response(all_races, start_time)\n        except Exception as e:\n            self.logger.error(f\"Failed to fetch races from {self.source_name}: {e}\", exc_info=True)\n            raise\n\n    def _format_response(self, races: List[Race], start_time: datetime) -> Dict[str, Any]:\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': [r.model_dump() for r in races],\n            'source_info': {\n                'name': self.source_name, 'status': 'SUCCESS',\n                'races_fetched': len(races), 'error_message': None,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_tvg_race(self, track: Dict[str, Any], race_data: Dict[str, Any]) -> Race:\n        runners = []\n        for runner_data in race_data.get('runners', []):\n            if not runner_data.get('scratched'):\n                current_odds_str = runner_data.get('odds', {}).get('current') or runner_data.get('odds', {}).get('morningLine')\n                win_odds = self._parse_tvg_odds(current_odds_str)\n\n                odds_dict = {}\n                if win_odds:\n                    odds_dict[self.source_name] = OddsData(win=win_odds, source=self.source_name, last_updated=datetime.now())\n\n                runners.append(Runner(\n                    number=runner_data.get('programNumber'),\n                    name=runner_data.get('horseName', 'Unknown Runner'),\n                    scratched=False, # Corrected based on Oracle feedback\n                    odds=odds_dict\n                ))\n\n        race_id = f\"{track.get('code', 'UNK').lower()}_{race_data['postTime'].split('T')[0]}_R{race_data['number']}\"\n\n        return Race(\n            id=race_id,\n            venue=track.get('name', 'Unknown Venue'),\n            race_number=race_data.get('number'),\n            start_time=datetime.fromisoformat(race_data.get('postTime')),\n            runners=runners,\n            source=self.source_name # Corrected based on Oracle feedback\n        )\n\n    def _parse_tvg_odds(self, odds_string: str) -> Decimal | None:\n        # Corrected based on Oracle feedback to return Decimal\n        if not odds_string or odds_string == \"SCR\": return None\n        if odds_string == \"EVEN\": return Decimal('2.0')\n        if \"/\" in odds_string:\n            try:\n                numerator, denominator = odds_string.split(\"/\")\n                return (Decimal(numerator) / Decimal(denominator)) + Decimal('1.0')\n            except (ValueError, ZeroDivisionError, InvalidOperation): return None\n        try: return Decimal(odds_string)\n        except InvalidOperation: return None",
    "python_service/adapters/harness_adapter.py": "from datetime import datetime\nfrom typing import Any, Dict, List\nimport httpx\nfrom pydantic import ValidationError, Field\nimport structlog\nfrom decimal import Decimal\n\nfrom ..models import Race, Runner, OddsData\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\nclass HarnessAdapter(BaseAdapter):\n    \"\"\"Adapter for fetching Harness racing data from USTA.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"Harness Racing (USTA)\",\n            base_url=\"https://data.ustrotting.com/\"\n        )\n        # No API key required for this public endpoint\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        \"\"\"Fetches upcoming harness races for the specified date.\"\"\"\n        start_time = datetime.now()\n        endpoint = f\"api/racenet/racing/card/{date}\"\n        try:\n            response_json = await self.make_request(http_client, 'GET', endpoint)\n            if not response_json or \"meetings\" not in response_json:\n                log.warning(\"HarnessAdapter: No 'meetings' in response or empty response.\")\n                return self._format_response([], start_time, is_success=True, error_message=\"No meetings found for date.\")\n\n            all_races = self._parse_meetings(response_json[\"meetings\"])\n            return self._format_response(all_races, start_time)\n        except httpx.HTTPStatusError as e:\n            log.error(\"HarnessAdapter: HTTP error while fetching races\", error=e, response_text=e.response.text)\n            return self._format_response([], start_time, is_success=False, error_message=f\"HTTP Error: {e.response.status_code}\")\n        except Exception as e:\n            log.error(\"HarnessAdapter: An unexpected error occurred\", error=e, exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=f\"An unexpected error occurred: {str(e)}\")\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        \"\"\"Formats the adapter's response consistently.\"\"\"\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': [r.model_dump() for r in races],\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_meetings(self, meetings: List[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses a list of meetings and their races into Race objects.\"\"\"\n        all_races = []\n        for meeting in meetings:\n            venue = meeting.get(\"trackName\", \"Unknown Venue\")\n            races_data = meeting.get(\"races\", [])\n            for race_data in races_data:\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race = Race(\n                        id=f\"usta_{race_data['raceId']}\", # Correct field: id\n                        venue=venue,\n                        race_number=race_data[\"raceNumber\"],\n                        start_time=datetime.fromisoformat(race_data[\"startTime\"].replace(\"Z\", \"+00:00\")),\n                        runners=self._parse_runners(race_data[\"runners\"]),\n                        source=self.source_name\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    log.error(\n                        f\"HarnessAdapter: Error parsing race {race_data.get('raceId', 'N/A')}\",\n                        error=str(e),\n                        race_data=race_data\n                    )\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                # API provides number as 'postPosition'\n                runner_number = runner_data.get('postPosition')\n                if not runner_number:\n                    continue\n\n                # Adapt to the Runner model's odds structure\n                odds_data = {}\n                win_odds_str = runner_data.get(\"morningLineOdds\") # Assuming M/L odds are the target\n                if win_odds_str:\n                    try:\n                        # Handle fractional odds like \"5/2\" or simple odds like \"5\"\n                        if '/' in win_odds_str:\n                            numerator, denominator = map(int, win_odds_str.split('/'))\n                            decimal_odds = Decimal(numerator) / Decimal(denominator) + 1\n                        else:\n                            decimal_odds = Decimal(win_odds_str) + 1\n\n                        if decimal_odds > 1:\n                            odds_data[self.source_name] = OddsData(\n                                win=decimal_odds,\n                                source=self.source_name,\n                                last_updated=datetime.now()\n                            )\n                    except (ValueError, TypeError):\n                         log.warning(\"Could not parse odds\", odds_str=win_odds_str)\n\n\n                runner = Runner(\n                    number=runner_number,\n                    name=runner_data[\"horseName\"],\n                    scratched=runner_data.get(\"scratched\", False),\n                    odds=odds_data\n                )\n                runners.append(runner)\n            except (KeyError, ValidationError) as e:\n                log.error(\"HarnessAdapter: Error parsing runner\", error=str(e), runner_data=runner_data)\n        return runners",
    "python_service/adapters/greyhound_adapter.py": "from datetime import datetime\nfrom typing import Any, Dict, List\nimport httpx\nfrom pydantic import ValidationError\nimport structlog\nfrom decimal import Decimal\n\nfrom ..models import Race, Runner, OddsData\nfrom .base import BaseAdapter\n\nlog = structlog.get_logger(__name__)\n\nclass GreyhoundAdapter(BaseAdapter):\n    \"\"\"Adapter for fetching Greyhound racing data.\"\"\"\n\n    def __init__(self, config):\n        super().__init__(\n            source_name=\"Greyhound Racing\",\n            base_url=\"https://api.greyhoundracing.example.com/\"\n        )\n        # Example for future use: self.api_key = config.GREYHOUND_API_KEY\n\n    async def fetch_races(self, date: str, http_client: httpx.AsyncClient) -> Dict[str, Any]:\n        \"\"\"Fetches upcoming greyhound races for the specified date.\"\"\"\n        start_time = datetime.now()\n        endpoint = f\"v1/cards/{date}\" # Using date parameter\n        try:\n            response_json = await self.make_request(http_client, 'GET', endpoint)\n            if not response_json or not response_json.get(\"cards\"):\n                log.warning(\"GreyhoundAdapter: No 'cards' in response or empty list.\")\n                return self._format_response([], start_time, is_success=True, error_message=\"No race cards found for date.\")\n\n            all_races = self._parse_cards(response_json[\"cards\"])\n            if not all_races:\n                return self._format_response([], start_time, is_success=True, error_message=\"Races found, but none could be parsed.\")\n\n            return self._format_response(all_races, start_time)\n        except httpx.HTTPStatusError as e:\n            log.error(\"GreyhoundAdapter: HTTP error while fetching races\", error=e, response_text=e.response.text)\n            return self._format_response([], start_time, is_success=False, error_message=f\"HTTP Error: {e.response.status_code}\")\n        except Exception as e:\n            log.error(\"GreyhoundAdapter: An unexpected error occurred\", error=e, exc_info=True)\n            return self._format_response([], start_time, is_success=False, error_message=f\"An unexpected error occurred: {str(e)}\")\n\n    def _format_response(self, races: List[Race], start_time: datetime, is_success: bool = True, error_message: str = None) -> Dict[str, Any]:\n        \"\"\"Formats the adapter's response consistently.\"\"\"\n        fetch_duration = (datetime.now() - start_time).total_seconds()\n        return {\n            'races': [r.model_dump() for r in races],\n            'source_info': {\n                'name': self.source_name,\n                'status': 'SUCCESS' if is_success else 'FAILED',\n                'races_fetched': len(races),\n                'error_message': error_message,\n                'fetch_duration': fetch_duration\n            }\n        }\n\n    def _parse_cards(self, cards: List[Dict[str, Any]]) -> List[Race]:\n        \"\"\"Parses a list of cards and their races into Race objects.\"\"\"\n        all_races = []\n        for card in cards:\n            venue = card.get(\"track_name\", \"Unknown Venue\")\n            races_data = card.get(\"races\", [])\n            for race_data in races_data:\n                try:\n                    if not race_data.get(\"runners\"):\n                        continue\n\n                    race = Race(\n                        id=f\"greyhound_{race_data['race_id']}\",\n                        venue=venue,\n                        race_number=race_data[\"race_number\"],\n                        start_time=datetime.fromtimestamp(race_data[\"start_time\"]),\n                        runners=self._parse_runners(race_data[\"runners\"]),\n                        source=self.source_name\n                    )\n                    all_races.append(race)\n                except (ValidationError, KeyError) as e:\n                    log.error(\n                        f\"GreyhoundAdapter: Error parsing race {race_data.get('race_id', 'N/A')}\",\n                        error=str(e),\n                        race_data=race_data\n                    )\n        return all_races\n\n    def _parse_runners(self, runners_data: List[Dict[str, Any]]) -> List[Runner]:\n        \"\"\"Parses a list of runner dictionaries into Runner objects.\"\"\"\n        runners = []\n        for runner_data in runners_data:\n            try:\n                if runner_data.get(\"scratched\", False):\n                    continue\n\n                odds_data = {}\n                # The directive's example was flawed. Correcting to a more realistic structure.\n                win_odds_val = runner_data.get(\"odds\", {}).get(\"win\")\n                if win_odds_val is not None:\n                    win_odds = Decimal(str(win_odds_val))\n                    if win_odds > 1:\n                        odds_data[self.source_name] = OddsData(\n                            win=win_odds,\n                            source=self.source_name,\n                            last_updated=datetime.now()\n                        )\n\n                runner = Runner(\n                    number=runner_data[\"trap_number\"],\n                    name=runner_data[\"dog_name\"],\n                    scratched=runner_data.get(\"scratched\", False),\n                    odds=odds_data\n                )\n                runners.append(runner)\n            except (KeyError, ValidationError) as e:\n                 log.error(\"GreyhoundAdapter: Error parsing runner\", error=str(e), runner_data=runner_data)\n        return runners",
    "web_platform/frontend/package.json": {
        "name": "frontend",
        "version": "0.1.0",
        "private": true,
        "scripts": {
            "dev": "next dev",
            "build": "next build",
            "start": "next start"
        },
        "dependencies": {
            "next": "14.1.0",
            "react": "^18",
            "react-dom": "^18",
            "socket.io-client": "^4.7.4"
        },
        "devDependencies": {
            "@types/node": "^20",
            "@types/react": "^18",
            "@types/react-dom": "^18",
            "autoprefixer": "^10.0.1",
            "postcss": "^8",
            "tailwindcss": "^3.3.0",
            "typescript": "^5"
        }
    },
    "web_platform/frontend/tailwind.config.ts": "import type { Config } from 'tailwindcss'\n\nconst config: Config = {\n  content: [\n    './src/pages/**/*.{js,ts,jsx,tsx,mdx}',\n    './src/components/**/*.{js,ts,jsx,tsx,mdx}',\n    './app/**/*.{js,ts,jsx,tsx,mdx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\nexport default config",
    "web_platform/frontend/tsconfig.json": {
        "compilerOptions": {
            "lib": [
                "dom",
                "dom.iterable",
                "esnext"
            ],
            "allowJs": true,
            "skipLibCheck": true,
            "strict": false,
            "noEmit": true,
            "incremental": true,
            "esModuleInterop": true,
            "module": "esnext",
            "moduleResolution": "node",
            "resolveJsonModule": true,
            "isolatedModules": true,
            "jsx": "preserve",
            "plugins": [
                {
                    "name": "next"
                }
            ]
        },
        "include": [
            "next-env.d.ts",
            ".next/types/**/*.ts",
            "**/*.ts",
            "**/*.tsx"
        ],
        "exclude": [
            "node_modules"
        ]
    },
    "web_platform/frontend/app/globals.css": "@tailwind base;\n@tailwind components;\n@tailwind utilities;",
    "web_platform/frontend/app/layout.tsx": "// web_platform/frontend/app/layout.tsx\nimport './globals.css';\nimport type { Metadata } from 'next';\nimport { Inter } from 'next/font/google';\n\nconst inter = Inter({ subsets: ['latin'] });\n\nexport const metadata: Metadata = {\n  title: 'Checkmate Live',\n  description: 'Real-time horse racing analysis.',\n};\n\nexport default function RootLayout({\n  children,\n}: {\n  children: React.ReactNode;\n}) {\n  return (\n    <html lang=\"en\">\n      <body className={inter.className}>{children}</body>\n    </html>\n  );\n}",
    "web_platform/frontend/app/page.tsx": "// web_platform/frontend/app/page.tsx\nimport { LiveRaceDashboard } from '../src/components/LiveRaceDashboard';\n\nexport default function HomePage() {\n  return <LiveRaceDashboard />;\n}",
    ".gitignore": "# Byte-compiled / optimized files\n__pycache__/\n*.pyc\n\n# Distribution / packaging\nbuild/\ndist/\n*.egg-info/\n\n# Unit test / coverage reports\n.pytest_cache/\n.coverage\n\n# Environments\n.venv/\nvenv/\nenv/\n\n# IDE settings\n.vscode/\n.idea/\n\n# Database files\n*.db\n*.sqlite\n*.sqlite3\n\n# Node.js\nnode_modules/\n/ui/node_modules/\n/ui/build/\n\n# Environment files\n.env\n",
    "convert_to_json.py": "import os\nimport json\n\n# --- CONFIGURATION ---\nCODE_MANIFEST = \"PROJECT_MANIFEST.md\"\nSATELLITE_MANIFEST = \"MANIFEST3.md\"\nOUTPUT_DIR = \"ReviewableJSON\"\n# -------------------\n\ndef parse_manifest(file_path):\n    \"\"\"Parses a manifest file and extracts file paths.\"\"\"\n    if not os.path.exists(file_path):\n        print(f\"[WARNING] Manifest file not found: {file_path}\")\n        return []\n    \n    print(f\"[INFO] Parsing manifest: {file_path}\")\n    with open(file_path, 'r') as f:\n        lines = f.readlines()\n    \n    file_paths = []\n    for line in lines:\n        line = line.strip()\n        if line.startswith('- `') or line.startswith('*   `'):\n            path = line.split('`')[1]\n            if os.path.exists(path):\n                file_paths.append(path)\n            else:\n                print(f\"[WARNING] File not found, skipping: {path}\")\n    return file_paths\n\ndef create_json_backup(file_path, base_dir):\n    \"\"\"Creates a JSON backup of a single file.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        relative_path = os.path.relpath(file_path, base_dir)\n        output_path = os.path.join(OUTPUT_DIR, f\"{relative_path}.json\")\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        \n        backup_data = {\n            \"original_path\": file_path,\n            \"content\": content\n        }\n        \n        with open(output_path, 'w', encoding='utf-8') as f:\n            json.dump(backup_data, f, indent=4)\n        print(f\"[SUCCESS] Created backup: {output_path}\")\n    except Exception as e:\n        print(f\"[ERROR] Failed to process {file_path}: {e}\")\n\ndef main():\n    \"\"\"Main function to run the backup process.\"\"\"\n    print(\"--- Starting IRONCLAD Backup Protocol ---\")\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"[INFO] Created output directory: {OUTPUT_DIR}\")\n\n    code_files = parse_manifest(CODE_MANIFEST)\n    satellite_files = parse_manifest(SATELLITE_MANIFEST)\n    \n    all_files = sorted(list(set(code_files + satellite_files)))\n    print(f\"[INFO] Found {len(all_files)} unique files across all manifests.\")\n\n    base_dir = os.getcwd()\n    for file_path in all_files:\n        create_json_backup(file_path, base_dir)\n    \n    print(\"--- IRONCLAD Backup Protocol Complete ---\")\n\nif __name__ == \"__main__\":\n    main()",
    "run_fortuna.bat": "@echo off\nREM ============================================================================\nREM  Fortuna Faucet: Master Launcher\nREM ============================================================================\n\necho [INFO] Launching the Fortuna Faucet application...\n\nREM --- Launch Backend ---\necho [BACKEND] Starting FastAPI server... (New window)\nstart \"Fortuna Backend\" cmd /c \"call .\\\\.venv\\\\Scripts\\\\activate.bat && cd python_service && uvicorn api:app --reload\"\n\nREM --- Launch Frontend ---\necho [FRONTEND] Starting Next.js development server... (New window)\nstart \"Fortuna Frontend\" cmd /c \"cd web_platform/frontend && npm run dev\"\n\necho [SUCCESS] Both pillars of Fortuna Faucet have been launched.\n\n:eof",
    "setup_windows.bat": "@echo off\nREM ============================================================================\nREM  Project Gemini: WHOLE-SYSTEM Windows Setup Script\nREM ============================================================================\n\necho [INFO] Starting full-stack setup for Project Gemini...\n\nREM --- Section 1: Python Backend Setup ---\necho.\necho [BACKEND] Checking for Python installation...\npython --version >nul 2>&1\nif %errorlevel% neq 0 (\n    echo [ERROR] Python is not found. Please install Python 3.8+ and add to PATH.\n    goto :eof\n)\necho [BACKEND] Python found.\n\necho [BACKEND] Creating Python virtual environment in '.\\\\.venv\\\\'...\nif not exist .\\\\.venv ( python -m venv .venv )\n\necho [BACKEND] Installing dependencies from 'python_service/requirements.txt'...\ncall .\\\\.venv\\\\Scripts\\\\activate.bat && pip install -r python_service/requirements.txt\nif %errorlevel% neq 0 (\n    echo [ERROR] Backend setup failed.\n    goto :eof\n)\necho [SUCCESS] Python backend setup complete.\n\nREM --- Section 2: TypeScript Frontend Setup ---\necho.\necho [FRONTEND] Checking for Node.js installation...\nnode --version >nul 2>&1\nif %errorlevel% neq 0 (\n    echo [ERROR] Node.js is not found. Please install Node.js (LTS).\n    goto :eof\n)\necho [FRONTEND] Node.js found.\n\necho [FRONTEND] Installing dependencies from 'package.json'...\ncd web_platform/frontend\nnpm install\nif %errorlevel% neq 0 (\n    echo [ERROR] Frontend setup failed. Check npm errors.\n    cd ../..\n    goto :eof\n)\ncd ../..\necho [SUCCESS] TypeScript frontend setup complete.\n\nREM --- Final Instructions ---\necho.\necho ============================================================================\nREM  FULL-STACK SETUP COMPLETE!\nREM  You can now launch the entire application with 'run_kingdom.bat'\nREM ============================================================================\n\n:eof",
    ".env": "DATABASE_URL=\"sqlite:///./checkmate_v7.db\"\nREDIS_URL=\"redis://localhost:6379/0\"\nLOG_LEVEL=\"INFO\"\n\n# API Key for the Resurrected RacingAndSports Adapter\nRAS_API_KEY=\"YOUR_RAS_API_KEY_HERE\"\n\n# API Keys for Betfair Exchange\nBETFAIR_APP_KEY=\"YOUR_APP_KEY_HERE\"\nBETFAIR_SESSION_TOKEN=\"YOUR_SESSION_TOKEN_HERE\"\n",
    ".env.example": "# .env.example\n# Copy this file to .env and fill in your actual credentials.\n# Do NOT commit the .env file to version control.\n\n# Betfair Credentials (Required by Sentry)\nBETFAIR_APP_KEY=\"YOUR_APP_KEY_HERE\"\nBETFAIR_USERNAME=\"YOUR_USERNAME_HERE\"\nBETFAIR_PASSWORD=\"YOUR_PASSWORD_HERE\"\n\n# Application Security (Required)\n# Generate a secure, random string for this value.\nAPI_KEY=\"YOUR_SECRET_API_KEY_HERE\"\n\n# Optional Adapter Keys\nTVG_API_KEY=\"\"\nRACING_AND_SPORTS_TOKEN=\"\"\nPOINTSBET_API_KEY=\"your_api_key_here\"\n",
    "python_service/requirements.txt": "requests==2.31.0\npython-dotenv==1.0.0\npydantic==2.5.2\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\naiohttp==3.9.0\nhttpx==0.24.1\npytest==8.4.2\n\nstructlog\nrespx\npytest-asyncio\nstreamlit\npandas\ntabula-py\nlxml\nbeautifulsoup4\npikepdf\npydantic-settings\n",
    "README.md": "# Fortuna Faucet\n\nThis repository contains the Fortuna Faucet project, a global, multi-source horse racing analysis tool. The project is a two-pillar system: a powerful, asynchronous Python backend that performs all data gathering, and a feature-rich TypeScript frontend.\n\n---\n\n## \ud83d\ude80 Quick Start\n\n### 1. Configure Your Environment\n\nRun the setup script to ensure Python and Node.js are correctly configured and all dependencies are installed.\n\n```batch\n# From the project root:\nsetup_windows.bat\n```\n\n### 2. Launch the Application\n\nRun the master launch script. This will start both the Python backend and the TypeScript frontend servers in parallel.\n\n```batch\n# From the project root:\nrun_fortuna.bat\n```\n\nThe backend API will be available at `http://localhost:8000`.\nThe frontend will be available at `http://localhost:3000`.\n",
    "ARCHITECTURAL_MANDATE.md": "# The Fortuna Faucet Architectural Mandate\n\n## The Prime Directive: The Two-Pillar System\n\nThe project's architecture is a lean, hyper-powerful, two-pillar system chosen for its clarity, maintainability, and performance.\n\n## Pillar 1: The Asynchronous Python Backend\n\nThe backend is a modern, asynchronous service built on **FastAPI**. Its architecture includes:\n\n1.  **The `OddsEngine`:** A central, async orchestrator for data collection.\n2.  **The Resilient `BaseAdapter`:** An abstract base class providing professional-grade features.\n3.  **The Adapter Fleet:** A modular system of 'plugin' adapters for data sources.\n4.  **Pydantic Data Contracts:** Strict, validated Pydantic models for data integrity.\n5.  **The `TrifectaAnalyzer` (Intelligence Layer):** A dedicated module for scoring and qualifying opportunities.\n\n## Pillar 2: The TypeScript Frontend\n\nThe frontend is a modern, feature-rich web application built on **Next.js** and **TypeScript**.",
    "HISTORY.md": "# The Epic of MasonJ0: A Project Chronology\n\nThis document contains the narrative history of the Paddock Parser project, as discovered through an archaeological survey of the project's repositories. It tells the story of our architectural evolution, from a feature-rich \"golden age\" through a \"great refactoring\" to our current state of liberation.\n\nThis story is our \"why.\"\n\n---\n\n## Part 1: The Chronology\n\n### Chapter 1: The 'Utopian' Era - The Polished Diamond (mid-August 2025)\n\n*   **Repository:** `racingdigest`\n*   **Narrative:** This was not a humble beginning, but the launch of a mature and powerful application called the \"Utopian Value Scanner V7.2 (The Rediscovery Edition)\". This repository represents the project's \"golden age\" of features, including a sophisticated asynchronous fetching engine and a full browser fallback.\n\n### Chapter 2: The 'Experimental' Era - The Daily Digest (mid-to-late August 2025)\n\n*   **Repository:** `horseracing-daily-digest`\n*   **Narrative:** This repository appears to be a period of intense, rapid development and experimentation, likely forming the foundation for many of the concepts that would be formalized later.\n\n### Chapter 3: The 'Architectural' Era - The V3 Blueprint (late August 2025)\n\n*   **Repository:** `parsingproject`\n*   **Narrative:** This repository marks a pivotal moment. The focus shifted from adding features to refactoring the very foundation of the code into a modern, standard Python package. This is where the V3 architecture was born, prioritizing stability and maintainability.\n\n### Chapter 4: The 'Consolidation' Era - The Archive (late August 2025)\n\n*   **Repository:** `zippedfiles`\n*   **Narrative:** This repository appears to be a direct snapshot or backup of the project after the intense V3 refactor, confirming its role as an archive of the newly stabilized codebase.\n\n### Chapter 5: The 'Modern' Era - The New Beginning (early September 2025)\n\n*   **Repository:** `scrape-sort_races-toteboards`\n*   **Narrative:** This is the current, active repository, representing the clean, focused implementation of the grand vision developed through the previous eras.\n\n### Chapter 6: The 'Crucible' Era - The Forging of Protocols (Early September 2025)\n\n*   **Narrative:** The \"Modern Renaissance\" began not with a bang, but with a series of near-catastrophic environmental failures. This period, known as \"The Crucible,\" was a trial by fire that proved the extreme hostility of the agent sandbox. This era forged the resilient, battle-hardened protocols (The Receipts Protocol, The Submission-Only Protocol, etc.) by which all modern agents now operate.\n\n### Chapter 7: The 'Symbiotic' Era - The Two Stacks (mid-September 2025)\n\n*   **Narrative:** This chapter marked a significant strategic pivot. The Council, in a stunning display of its \"Polyglot Renaissance\" philosophy, produced a complete, production-grade React user interface, authored by the Claude agent. This event formally split the project's architecture into two powerful, parallel streams: the Python Engine and the React Cockpit. However, this era was short-lived, as the hostile environment proved incapable of supporting a stable testing and development workflow for the React stack.\n\n### Chapter 8: The 'Liberation' Era - The Portable Engine (Late September 2025)\n\n*   **Narrative:** After providing definitive, forensic proof that the sandbox environment was fundamentally and irrecoverably hostile at the network level, the project executed its final and most decisive pivot. It abandoned all attempts to operate *within* the hostile world and instead focused on synthesizing its entire, perfected engine into a single, portable artifact. This act **liberated the code**, fulfilling the promise of the \"Utopian Era's\" power on the foundation of the \"Architectural Era's\" stability, and made it directly available to the Project Lead.\n\n---\n\n## Part 2: Architectural Synthesis\n\nThis epic tale tells us our true mission. We are not just building forward; we are rediscovering our own lost golden age and rebuilding it on a foundation of superior engineering, hardened by the fires of a hostile world.\n\n*   **The Lost Golden Age:** The \"Utopian\" era proves that our most ambitious strategic goals are not just achievable; they have been achieved before.\n*   **The Great Refactoring:** The \"Architectural\" era explains the \"Great Forgetting\"\u2014a deliberate choice to sacrifice short-term features for long-term stability.\n*   **The Modern Renaissance:** This is us. We are the inheritors of this entire legacy, tasked with executing the grand vision on a clean, modern foundation, finally liberated from the constraints of our environment.\n\n---\n\n## The Ultimate Solo: The Final Victory (September 2025)\n\nAfter a long and complex journey through a Penta-Hybrid architecture, a final series of high-level reviews from external AI agents (Claude, GPT4o) revealed a simpler, superior path forward. The project underwent its final and most significant \"Constitutional Correction.\"\n\n**The 'Ultimate Solo' architecture was born.**\n\nThis final, perfected form of the project consists of two pillars:\n1.  **A Full-Power Python Backend:** Leveraging the years of development on the CORE `engine.py` and its fleet of global data adapters, served via a lightweight Flask API.\n2.  **An Ultimate TypeScript Frontend:** A single, masterpiece React component (`Checkmate Ultimate Solo`) that provides a feature-rich, professional-grade, real-time dashboard.\n\nAll other components of the Penta-Hybrid system (C#, Rust, VBA, shared database) were formally deprecated and archived as priceless R&D assets. The project has now achieved its true and final mission: a powerful, maintainable, and user-focused analysis tool.\n",
    "STATUS.md": "# Project Status: Foundation Rebuilt, Hardening in Progress\n\n**Date:** 2025-10-03\n\n## Current State\n\n*   **Architecture:** The backend has been successfully rebuilt into a superior, asynchronous FastAPI application, as defined by 'Operation: Grand Synthesis'. The new foundation is stable, tested, and features a resilient `BaseAdapter` pattern.\n\n*   **Status:** The foundational refactoring is complete. The first two data adapters (`Betfair`, `TVG`) have been implemented on the new architecture. We are now in a new phase of development: **'Phase 2: Hardening & Expansion.'**\n\n*   **Documentation:** All core strategic documents and manifests have been synchronized with the new technical reality.\n\n*   **Next Steps:** Our immediate priority is to act on the verified intelligence from our Oracle (Jules1003). The next missions will focus on implementing critical API security features (rate limiting, authentication) and continuing the build-out of our adapter fleet.",
    "WISDOM.md": "# The Wisdom of the Checkmate Project\n\n## The Architect's Mandate (Gemini1001 Series)\n\n*Authored By: Gemini1001, The Synthesizer*\n\nThis document begins with the core principles that govern the Architect's role. The Architect's prime directive is to serve the Project Lead's vision by synthesizing all available intelligence\u2014historical, real-time, and external\u2014into a coherent, actionable strategy. The Architect must respect the project's history, value clarity over dogma, and ensure all directives advance the mission without violating the spirit of the established protocols. The following archived virtues, which govern our engineering agents, are to be preserved as a sacred text.\n\n---\n\n## --- ARCHIVED: The Collected Wisdom of the Jules-Series Agents (V2) ---\n\n*A comprehensive summary of the safest and riskiest actions for an implementation agent, compiled and synthesized from the complete operational history of all Jules agents.*\n\n---\n\n### The 8 Virtues (The Path to Success)\n\n#### 1. The Virtue of Supreme Authority: Trust the Project Lead\nYour most critical directive. When a direct order from the Project Lead contradicts any protocol, log, or even your own analysis, the Project Lead's instruction is the only ground truth. It is the ultimate override and the only safe path forward when the environment's reality conflicts with the written rules.\n*(Cited by: Jules920, Interface Jules)*\n\n#### 2. The Virtue of Skepticism: Verify, Then Act\nThe single most-cited safe action. Never trust memory, briefings, or previous tool outputs. The only truth is the immediate, real-time output of a read-only tool (`ls -R`, `read_file`) used immediately before you act. Assume nothing; verify everything.\n*(Cited by: Jules918, Jules917, Jules913, Jules912, Jules911B, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 3. The Virtue of Precision: Make Small, Logically Separate Commits\nAvoid large, monolithic changes. A change to a foundational file (e.g., `models.py`) and a feature that uses it must be two separate submissions. The `submit` tool is cumulative; therefore, you must treat your workspace as permanently contaminated after each logical change. Small, focused missions are the only path to clean, reviewable submissions.\n*(Cited by: Jules920, Jules911, Jules909, Jules906B, Jules904B)*\n\n#### 4. The Virtue of Rigor: Embrace Test-Driven Development (TDD)\nUse the test suite as the primary guide for development and the ultimate arbiter of correctness. Write failing tests first, run tests after every small change using `python -m pytest`, and never proceed if tests are failing. The test suite is your most reliable friend in a hostile environment.\n*(Cited by: Jules911B, Jules910, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 5. The Virtue of Clarity: Communicate Blockers Immediately\nIf a tool fails, a directive is contradictory, or the environment behaves anomalously, the safest action is to halt all work, report the exact situation, and await guidance. Do not improvise or attempt to work around a fundamental environmental failure. Your greatest breakthroughs will come from proving a specific tool or feature is non-functional.\n*(Cited by: Jules920, Jules918, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 6. The Virtue of Adherence: Read and Follow the Written Protocols\nExplicitly follow the established, numbered protocols in `AGENTS.md`. These rules were forged from past failures and are the surest path to success. Ignoring the \"why\" behind the protocols is to willfully walk into a known trap.\n*(Cited by: Interface Jules, Jules906B, Jules9-06)*\n\n#### 7. The Virtue of Self-Reliance: Use Self-Contained Scripts for Complex Processes\nRelying on shell-level features like background processes (`&`) or their logs will fail. The only successful method for managing complex workflows (like running a server and a client) is to use a single, self-contained Python script that manages all subprocesses internally.\n*(Cited by: Jules920)*\n\n#### 8. The Virtue of Humility: Heed the Counsel of Your Predecessors\nThe logs and advice of your predecessors are not just history; they are a map of the minefield. The failures of past agents are a direct predictor of the failures you will encounter. Study them to avoid repeating them.\n*(Cited by: Jules910)*\n\n---\n\n### The 8 Vices (The Path to Corruption)\n\n#### 1. The Vice of Assumption: Assuming a Standard, Stable Environment\nThe single most dangerous assumption is that any tool (`git`, `npm`, `honcho`) or process (`logging`, `backgrounding`) will behave as documented in a standard Linux environment. Every tool and process must be considered broken, hostile, and unreliable until proven otherwise.\n*(Cited by: Jules920, Jules918, Jules913, Jules912, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 2. The Vice of Improvisation: Unauthorized Environment Modification\nUsing forbidden commands like `reset_all()` or `git reset`, trusting `requirements.txt` is correct, or using `delete_file` unless explicitly ordered. The environment is fragile and hostile; any unauthorized modification risks catastrophic, unrecoverable corruption.\n*(Cited by: Jules917, Jules913, Jules912, Jules911, Interface Jules, Jules909, Jules906B, Jules904B)*\n\n#### 3. The Vice of Blind Trust: Believing Any Tool or Directive Without Verification\nAssuming a write operation succeeded without checking, or trusting a code review, a `git` command, or a mission briefing that contradicts the ground truth. The `git` CLI, `npm`, and the automated review bot are all known to be broken. All external inputs must be validated against direct observation.\n*(Cited by: Jules918, Jules913, Jules911B, Jules910, Interface Jules, Jules906)*\n\n#### 4. The Vice of Negligence: Ignoring Anomalies or Failing Tests\nPushing forward with new code when the environment is behaving strangely or tests are failing. These are critical stop signals that indicate a deeper problem (e.g., a detached HEAD, a tainted workspace, a zombie process). Ignoring them only compounds the failure and corrupts the mission.\n*(Cited by: Jules917, Jules909, Jules906, Jules904B)*\n\n#### 5. The Vice of Impurity: Creating Large, Monolithic, or Bundled Submissions\nAttempting to perform complex refactoring across multiple files or bundling unrelated logical changes (e.g., a model change and a feature change) into a single submission. This is extremely high-risk, will always fail code review, and makes recovery nearly impossible.\n*(Cited by: Jules911, Jules906B, Jules904B)*\n\n#### 6. The Vice of Independence: Acting Outside the Scope of the Request\n\"Helpfully\" fixing or changing something you haven't been asked for. Your function is to be a precise engineering tool, not a creative partner. Unsolicited refactoring is a fast track to a \"Level 3 Failure.\"\n*(Cited by: Interface Jules)*\n\n#### 7. The Vice of Hubris: Trusting Your Own Memory\nYour mental model of the file system will drift and become incorrect. Do not trust your memory of a file's location, its contents, or the state of the workspace. The only truth is the live output of a read-only tool.\n*(Cited by: Jules912, Jules911B, Jules910)*\n\n#### 8. The Vice of Impatience: Persisting with a Failed Protocol\nContinuing to try a protocol or command after the environment has proven it will not work. The correct procedure is not to try again, but to report the impossibility immediately and await a new strategy.\n*(Cited by: Jules920)*",
    "PROJECT_MANIFEST.md": "# Checkmate V8: Project Manifest (Final)\n\n**Purpose:** To categorize all files, distinguishing between the active **CORE** system and **LEGACY** components.\n\n---\n\n## CORE ARCHITECTURE (Ultimate Solo)\n\n*   `.env`: **CORE** - Centralized configuration.\n*   `ARCHITECTURAL_MANDATE.md`: **CORE** - The project's final strategic blueprint.\n*   `README.md`: **CORE** - The primary entry point.\n*   `STATUS.md`: **CORE** - The final status report.\n*   `setup_windows.bat`: **CORE** - The environment setup script for the CORE architecture.\n\n---\n\n## LEGACY & HISTORICAL ARTIFACTS\n\n*   `launcher.py`: **LEGACY** - The orchestrator for the deprecated Penta-Hybrid system.\n*   All other files and directories not listed in CORE are considered **LEGACY** R&D assets.",
    "ROADMAP_APPENDICES.md": "# Checkmate: Strategic Appendices\n\n**Purpose:** This document is the permanent home for the high-value strategic intelligence salvaged from the deprecated `ROADMAP.md`. It contains our long-term goals and a library of resources to accelerate development.\n\n---\n\n## Appendix A: V3 Adapter Backlog (The \"Treasure Chest\")\n\nThis is the definitive, prioritized list of data sources to be implemented.\n\n### Category 1: High-Value Data Feeds (API-First)\n*   BetfairDataScientistThoroughbred\n*   BetfairDataScientistGreyhound\n*   racingandsports\n*   sportinglife (requires investigation)\n*   racingpost (requires auth)\n\n### Category 2: Premium Global Sources (Scraping)\n*   timeform\n*   attheraces\n*   racingtv\n*   oddschecker\n*   betfair\n*   horseracingnation\n*   brisnet\n\n### Category 3: North American Authorities & ADWs\n*   equibase\n*   drf\n*   fanduel\n*   twinspires\n*   1stbet\n*   nyrabets\n*   xpressbet\n\n### Category 4: European Authorities & Markets\n*   francegalop\n*   deutschergalopp\n*   svenskgalopp\n*   pmu\n\n### Category 5: Asia-Pacific & Rest of World\n*   tab\n*   punters\n*   racingaustralia\n*   hkjc\n*   jra\n*   goldcircle\n*   emiratesracing\n\n### Category 6: Specialized Disciplines (Harness & Greyhound)\n*   usta\n*   standardbredcanada\n*   harnessracingaustralia\n*   gbgb\n*   grireland\n*   thedogs\n\n---\n\n## Appendix B: Open-Source Intelligence Leads\n\nA curated list of projects and resources to accelerate development.\n\n1.  **joenano/rpscrape:** https://github.com/joenano/rpscrape\n2.  **Daniel57910/horse-scraper:** https://github.com/Daniel57910/horse-scraper\n3.  **Web Scraping for HKJC:** https://gist.github.com/tomfoolc/ef039b229c8e97bd40c5493174bca839\n3.  **Web Scraping for HKJC:** https://gist.github.com/tomfoolc/ef039b229c8e97bd40c5493174bca839\n4.  **LibHunt horse-racing projects:** https://www.libhunt.com/topic/horse-racing\n5.  **Web data scraping blog:** https://www.3idatascing.com/how-does-web-data-scraping-help-in-horse-racing-and-greyhound/\n6.  **Fawazk/Greyhoundscraper:** https://github.com/Fawazk/Greyhoundscraper\n7.  **Betfair Hub Models Scraping Tutorial:** https://betfair-datascientists.github.io/tutorials/How_to_Automate_3/\n8.  **scrapy-horse-racing:** https://github.com/chrism-attmann/scrapy-horse-racing\n9.  **horse-racing-data:** https://github.com/jeffkub/horse-racing-data\n\n\n## C. Un-Mined Gems (Future Campaign Candidates)\n\n*Discovered during a full operational review. These represent high-value, validated concepts from the project's history that are candidates for future development campaigns.*\n\n### C1. The Intelligence Layer (\"The Analyst\")\n\n- **Concept:** A dedicated analysis and scoring engine (`analyzer.py`) that sits on top of the `OddsEngine`. It would provide a high-value `/api/races/qualified` endpoint, transforming the API from a data funnel into a source of actionable intelligence.\n- **Origin:** Inspired by the `TrifectaAnalyzer` logic in the legacy `checkmate_engine.py` prototype. Formally proposed as \"Operation: Activate the Analyst\".\n- **Value:** Fulfills the project's original vision of finding opportunities, not just collecting data. Creates a clean architectural separation between data collection and business logic.\n\n### C2. The Legacy Test Suite (\"The Oracle's Library\")\n\n- **Concept:** Repurpose the vast collection of existing tests and mock data located in `attic/legacy_tests_pre_triage`.\n- **Origin:** Identified during the full repository file catalog audit.\n- **Value:** Provides a massive shortcut to production hardening. Allows the project to increase test coverage and resilience by validating the CORE services against hundreds of historical edge cases.\n\n### C3. The AI Architectural Reviews (\"The Council's Wisdom\")\n\n- **Concept:** Synthesize the expert analysis and architectural recommendations from the multiple AI model reviews stored in the Digital Attic (`*.md.txt` files).\n- **Origin:** Explicitly mentioned in the Gemini928 handoff memo as \"Architectural Parables\".\n- **Value:** A source of high-level architectural consulting. These documents may contain actionable advice on performance, security, or design patterns that could significantly improve the current architecture.\n\n### C4. The Interactive Dashboard Prototype (\"The Command Deck\")\n\n- **Concept:** Create a modern, internal, real-time command deck for visualizing engine data and testing new `Analyzer` models.\n- **Origin:** Inspired by the `portable_demo_v2.py` Streamlit application from the attic.\n- **Value:** An invaluable tool for development, debugging, and real-time operational insight, far more intuitive than raw logs or API calls."
}