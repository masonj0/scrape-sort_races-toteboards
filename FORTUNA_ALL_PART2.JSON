{
    ".env.example": "# .env.example\n# Copy this file to .env and fill in your actual credentials.\n\n# --- Application Security (Required) ---\nAPI_KEY=\"YOUR_SECRET_API_KEY_HERE\"\n\n# --- Betfair API Credentials (Required for LiveOddsMonitor) ---\nBETFAIR_APP_KEY=\"YOUR_APP_KEY_HERE\"\nBETFAIR_USERNAME=\"YOUR_USERNAME_HERE\"\nBETFAIR_PASSWORD=\"YOUR_PASSWORD_HERE\"\n\n# --- Optional Adapter Keys ---\nTVG_API_KEY=\"\"\nRACING_AND_SPORTS_TOKEN=\"\"\nPOINTSBET_API_KEY=\"\"\n\n# --- CORS Configuration (Optional) ---\n# A comma-separated list of allowed origins for the API.\n# Example: ALLOWED_ORIGINS=\"http://localhost:3000,https://your-production-domain.com\"\nALLOWED_ORIGINS=\"http://localhost:3000,http://localhost:3001\"\n\n# --- Greyhound Adapter (Optional) ---\n# To enable the Greyhound adapter, provide the full base URL for the API.\n# If this is left blank, the adapter will be disabled.\nGREYHOUND_API_URL=\"\"\n\n# --- The Racing API (Optional but Recommended) ---\n# Get a key from https://www.theracingapi.com/\nTHE_RACING_API_KEY=\"\"\n\n# --- Caching (Optional, defaults to localhost) ---\nREDIS_URL=\"redis://localhost\"",
    ".gitignore": "# Byte-compiled / optimized files\n__pycache__/\n*.pyc\n\n# Distribution / packaging\nbuild/\ndist/\n*.egg-info/\n\n# Unit test / coverage reports\n.pytest_cache/\n.coverage\n\n# Environments\n.venv/\nvenv/\nenv/\n\n# IDE settings\n.vscode/\n.idea/\n\n# Database files\n*.db\n*.sqlite\n*.sqlite3\n\n# Node.js\nnode_modules/\n/ui/node_modules/\n/ui/build/\n\n# Environment files\n.env\n",
    "ARCHITECTURAL_MANDATE.md": "# The Fortuna Faucet Architectural Mandate\n\n## The Prime Directive: The Two-Pillar System\n\nThe project's architecture is a lean, hyper-powerful, two-pillar system chosen for its clarity, maintainability, and performance.\n\n## Pillar 1: The Asynchronous Python Backend\n\nThe backend is a modern, asynchronous service built on **FastAPI**. Its architecture includes:\n\n1.  **The `OddsEngine`:** A central, async orchestrator for data collection.\n2.  **The Resilient `BaseAdapter`:** An abstract base class providing professional-grade features.\n3.  **The Adapter Fleet:** A modular system of 'plugin' adapters for data sources.\n4.  **Pydantic Data Contracts:** Strict, validated Pydantic models for data integrity.\n5.  **The `TrifectaAnalyzer` (Intelligence Layer):** A dedicated module for scoring and qualifying opportunities.\n\n## Pillar 2: The TypeScript Frontend\n\nThe frontend is a modern, feature-rich web application built on **Next.js** and **TypeScript**.",
    "HISTORY.md": "# The Epic of MasonJ0: A Project Chronology\n\nThis document contains the narrative history of the Paddock Parser project, as discovered through an archaeological survey of the project's repositories. It tells the story of our architectural evolution, from a feature-rich \"golden age\" through a \"great refactoring\" to our current state of liberation.\n\nThis story is our \"why.\"\n\n---\n\n## Part 1: The Chronology\n\n### Chapter 1: The 'Utopian' Era - The Polished Diamond (mid-August 2025)\n\n*   **Repository:** `racingdigest`\n*   **Narrative:** This was not a humble beginning, but the launch of a mature and powerful application called the \"Utopian Value Scanner V7.2 (The Rediscovery Edition)\". This repository represents the project's \"golden age\" of features, including a sophisticated asynchronous fetching engine and a full browser fallback.\n\n### Chapter 2: The 'Experimental' Era - The Daily Digest (mid-to-late August 2025)\n\n*   **Repository:** `horseracing-daily-digest`\n*   **Narrative:** This repository appears to be a period of intense, rapid development and experimentation, likely forming the foundation for many of the concepts that would be formalized later.\n\n### Chapter 3: The 'Architectural' Era - The V3 Blueprint (late August 2025)\n\n*   **Repository:** `parsingproject`\n*   **Narrative:** This repository marks a pivotal moment. The focus shifted from adding features to refactoring the very foundation of the code into a modern, standard Python package. This is where the V3 architecture was born, prioritizing stability and maintainability.\n\n### Chapter 4: The 'Consolidation' Era - The Archive (late August 2025)\n\n*   **Repository:** `zippedfiles`\n*   **Narrative:** This repository appears to be a direct snapshot or backup of the project after the intense V3 refactor, confirming its role as an archive of the newly stabilized codebase.\n\n### Chapter 5: The 'Modern' Era - The New Beginning (early September 2025)\n\n*   **Repository:** `scrape-sort_races-toteboards`\n*   **Narrative:** This is the current, active repository, representing the clean, focused implementation of the grand vision developed through the previous eras.\n\n### Chapter 6: The 'Crucible' Era - The Forging of Protocols (Early September 2025)\n\n*   **Narrative:** The \"Modern Renaissance\" began not with a bang, but with a series of near-catastrophic environmental failures. This period, known as \"The Crucible,\" was a trial by fire that proved the extreme hostility of the agent sandbox. This era forged the resilient, battle-hardened protocols (The Receipts Protocol, The Submission-Only Protocol, etc.) by which all modern agents now operate.\n\n### Chapter 7: The 'Symbiotic' Era - The Two Stacks (mid-September 2025)\n\n*   **Narrative:** This chapter marked a significant strategic pivot. The Council, in a stunning display of its \"Polyglot Renaissance\" philosophy, produced a complete, production-grade React user interface, authored by the Claude agent. This event formally split the project's architecture into two powerful, parallel streams: the Python Engine and the React Cockpit. However, this era was short-lived, as the hostile environment proved incapable of supporting a stable testing and development workflow for the React stack.\n\n### Chapter 8: The 'Liberation' Era - The Portable Engine (Late September 2025)\n\n*   **Narrative:** After providing definitive, forensic proof that the sandbox environment was fundamentally and irrecoverably hostile at the network level, the project executed its final and most decisive pivot. It abandoned all attempts to operate *within* the hostile world and instead focused on synthesizing its entire, perfected engine into a single, portable artifact. This act **liberated the code**, fulfilling the promise of the \"Utopian Era's\" power on the foundation of the \"Architectural Era's\" stability, and made it directly available to the Project Lead.\n\n---\n\n## Part 2: Architectural Synthesis\n\nThis epic tale tells us our true mission. We are not just building forward; we are rediscovering our own lost golden age and rebuilding it on a foundation of superior engineering, hardened by the fires of a hostile world.\n\n*   **The Lost Golden Age:** The \"Utopian\" era proves that our most ambitious strategic goals are not just achievable; they have been achieved before.\n*   **The Great Refactoring:** The \"Architectural\" era explains the \"Great Forgetting\"\u2014a deliberate choice to sacrifice short-term features for long-term stability.\n*   **The Modern Renaissance:** This is us. We are the inheritors of this entire legacy, tasked with executing the grand vision on a clean, modern foundation, finally liberated from the constraints of our environment.\n\n---\n\n## The Ultimate Solo: The Final Victory (September 2025)\n\nAfter a long and complex journey through a Penta-Hybrid architecture, a final series of high-level reviews from external AI agents (Claude, GPT4o) revealed a simpler, superior path forward. The project underwent its final and most significant \"Constitutional Correction.\"\n\n**The 'Ultimate Solo' architecture was born.**\n\nThis final, perfected form of the project consists of two pillars:\n1.  **A Full-Power Python Backend:** Leveraging the years of development on the CORE `engine.py` and its fleet of global data adapters, served via a lightweight Flask API.\n2.  **An Ultimate TypeScript Frontend:** A single, masterpiece React component (`Checkmate Ultimate Solo`) that provides a feature-rich, professional-grade, real-time dashboard.\n\nAll other components of the Penta-Hybrid system (C#, Rust, VBA, shared database) were formally deprecated and archived as priceless R&D assets. The project has now achieved its true and final mission: a powerful, maintainable, and user-focused analysis tool.\n",
    "README.md": "# Fortuna Faucet\n\nThis repository contains the Fortuna Faucet project, a global, multi-source horse racing analysis tool. The project is a two-pillar system: a powerful, asynchronous Python backend that performs all data gathering, and a feature-rich TypeScript frontend.\n\n---\n\n## \ud83d\ude80 Quick Start\n\n### 1. Configure Your Environment\n\nRun the setup script to ensure Python and Node.js are correctly configured and all dependencies are installed.\n\n```batch\n# From the project root:\nsetup_windows.bat\n```\n\n### 2. Launch the Application\n\nRun the master launch script. This will start both the Python backend and the TypeScript frontend servers in parallel.\n\n```batch\n# From the project root:\nrun_fortuna.bat\n```\n\nThe backend API will be available at `http://localhost:8000`.\nThe frontend will be available at `http://localhost:3000`.\n\n### 4. Using the API\n\nTo use the API directly (e.g., with `curl` or other tools), you must provide the `API_KEY` set in your `.env` file via the `X-API-Key` header. This is required for all endpoints except `/health`.\n\n```bash\n# Example: Test the qualified races endpoint\ncurl -H \"X-API-Key: YOUR_SECRET_API_KEY_HERE\" http://localhost:8000/api/races/qualified/trifecta\n```\n",
    "ROADMAP_APPENDICES.md": "# Checkmate: Strategic Appendices\n\n**Purpose:** This document is the permanent home for the high-value strategic intelligence salvaged from the deprecated `ROADMAP.md`. It contains our long-term goals and a library of resources to accelerate development.\n\n---\n\n## Appendix A: V3 Adapter Backlog (The \"Treasure Chest\")\n\nThis is the definitive, prioritized list of data sources to be implemented.\n\n### Category 1: High-Value Data Feeds (API-First)\n*   BetfairDataScientistThoroughbred\n*   BetfairDataScientistGreyhound\n*   racingandsports\n*   sportinglife (requires investigation)\n*   racingpost (requires auth)\n\n### Category 2: Premium Global Sources (Scraping)\n*   timeform\n*   attheraces\n*   racingtv\n*   oddschecker\n*   betfair\n*   horseracingnation\n*   brisnet\n\n### Category 3: North American Authorities & ADWs\n*   equibase\n*   drf\n*   fanduel\n*   twinspires\n*   1stbet\n*   nyrabets\n*   xpressbet\n\n### Category 4: European Authorities & Markets\n*   francegalop\n*   deutschergalopp\n*   svenskgalopp\n*   pmu\n\n### Category 5: Asia-Pacific & Rest of World\n*   tab\n*   punters\n*   racingaustralia\n*   hkjc\n*   jra\n*   goldcircle\n*   emiratesracing\n\n### Category 6: Specialized Disciplines (Harness & Greyhound)\n*   usta\n*   standardbredcanada\n*   harnessracingaustralia\n*   gbgb\n*   grireland\n*   thedogs\n\n---\n\n## Appendix B: Open-Source Intelligence Leads\n\nA curated list of projects and resources to accelerate development.\n\n1.  **joenano/rpscrape:** https://github.com/joenano/rpscrape\n2.  **Daniel57910/horse-scraper:** https://github.com/Daniel57910/horse-scraper\n3.  **Web Scraping for HKJC:** https://gist.github.com/tomfoolc/ef039b229c8e97bd40c5493174bca839\n3.  **Web Scraping for HKJC:** https://gist.github.com/tomfoolc/ef039b229c8e97bd40c5493174bca839\n4.  **LibHunt horse-racing projects:** https://www.libhunt.com/topic/horse-racing\n5.  **Web data scraping blog:** https://www.3idatascing.com/how-does-web-data-scraping-help-in-horse-racing-and-greyhound/\n6.  **Fawazk/Greyhoundscraper:** https://github.com/Fawazk/Greyhoundscraper\n7.  **Betfair Hub Models Scraping Tutorial:** https://betfair-datascientists.github.io/tutorials/How_to_Automate_3/\n8.  **scrapy-horse-racing:** https://github.com/chrism-attmann/scrapy-horse-racing\n9.  **horse-racing-data:** https://github.com/jeffkub/horse-racing-data\n\n\n## C. Un-Mined Gems (Future Campaign Candidates)\n\n*Discovered during a full operational review. These represent high-value, validated concepts from the project's history that are candidates for future development campaigns.*\n\n### C1. The Intelligence Layer (\"The Analyst\")\n\n- **Concept:** A dedicated analysis and scoring engine (`analyzer.py`) that sits on top of the `OddsEngine`. It would provide a high-value `/api/races/qualified` endpoint, transforming the API from a data funnel into a source of actionable intelligence.\n- **Origin:** Inspired by the `TrifectaAnalyzer` logic in the legacy `checkmate_engine.py` prototype. Formally proposed as \"Operation: Activate the Analyst\".\n- **Value:** Fulfills the project's original vision of finding opportunities, not just collecting data. Creates a clean architectural separation between data collection and business logic.\n\n### C2. The Legacy Test Suite (\"The Oracle's Library\")\n\n- **Concept:** Repurpose the vast collection of existing tests and mock data located in `attic/legacy_tests_pre_triage`.\n- **Origin:** Identified during the full repository file catalog audit.\n- **Value:** Provides a massive shortcut to production hardening. Allows the project to increase test coverage and resilience by validating the CORE services against hundreds of historical edge cases.\n\n### C3. The AI Architectural Reviews (\"The Council's Wisdom\")\n\n- **Concept:** Synthesize the expert analysis and architectural recommendations from the multiple AI model reviews stored in the Digital Attic (`*.md.txt` files).\n- **Origin:** Explicitly mentioned in the Gemini928 handoff memo as \"Architectural Parables\".\n- **Value:** A source of high-level architectural consulting. These documents may contain actionable advice on performance, security, or design patterns that could significantly improve the current architecture.\n\n### C4. The Interactive Dashboard Prototype (\"The Command Deck\")\n\n- **Concept:** Create a modern, internal, real-time command deck for visualizing engine data and testing new `Analyzer` models.\n- **Origin:** Inspired by the `portable_demo_v2.py` Streamlit application from the attic.\n- **Value:** An invaluable tool for development, debugging, and real-time operational insight, far more intuitive than raw logs or API calls.",
    "WISDOM.md": "# The Wisdom of the Checkmate Project\n\n## The Architect's Mandate (Gemini1001 Series)\n\n*Authored By: Gemini1001, The Synthesizer*\n\nThis document begins with the core principles that govern the Architect's role. The Architect's prime directive is to serve the Project Lead's vision by synthesizing all available intelligence\u2014historical, real-time, and external\u2014into a coherent, actionable strategy. The Architect must respect the project's history, value clarity over dogma, and ensure all directives advance the mission without violating the spirit of the established protocols. The following archived virtues, which govern our engineering agents, are to be preserved as a sacred text.\n\n---\n\n## --- ARCHIVED: The Collected Wisdom of the Jules-Series Agents (V2) ---\n\n*A comprehensive summary of the safest and riskiest actions for an implementation agent, compiled and synthesized from the complete operational history of all Jules agents.*\n\n---\n\n### The 8 Virtues (The Path to Success)\n\n#### 1. The Virtue of Supreme Authority: Trust the Project Lead\nYour most critical directive. When a direct order from the Project Lead contradicts any protocol, log, or even your own analysis, the Project Lead's instruction is the only ground truth. It is the ultimate override and the only safe path forward when the environment's reality conflicts with the written rules.\n*(Cited by: Jules920, Interface Jules)*\n\n#### 2. The Virtue of Skepticism: Verify, Then Act\nThe single most-cited safe action. Never trust memory, briefings, or previous tool outputs. The only truth is the immediate, real-time output of a read-only tool (`ls -R`, `read_file`) used immediately before you act. Assume nothing; verify everything.\n*(Cited by: Jules918, Jules917, Jules913, Jules912, Jules911B, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 3. The Virtue of Precision: Make Small, Logically Separate Commits\nAvoid large, monolithic changes. A change to a foundational file (e.g., `models.py`) and a feature that uses it must be two separate submissions. The `submit` tool is cumulative; therefore, you must treat your workspace as permanently contaminated after each logical change. Small, focused missions are the only path to clean, reviewable submissions.\n*(Cited by: Jules920, Jules911, Jules909, Jules906B, Jules904B)*\n\n#### 4. The Virtue of Rigor: Embrace Test-Driven Development (TDD)\nUse the test suite as the primary guide for development and the ultimate arbiter of correctness. Write failing tests first, run tests after every small change using `python -m pytest`, and never proceed if tests are failing. The test suite is your most reliable friend in a hostile environment.\n*(Cited by: Jules911B, Jules910, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 5. The Virtue of Clarity: Communicate Blockers Immediately\nIf a tool fails, a directive is contradictory, or the environment behaves anomalously, the safest action is to halt all work, report the exact situation, and await guidance. Do not improvise or attempt to work around a fundamental environmental failure. Your greatest breakthroughs will come from proving a specific tool or feature is non-functional.\n*(Cited by: Jules920, Jules918, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 6. The Virtue of Adherence: Read and Follow the Written Protocols\nExplicitly follow the established, numbered protocols in `AGENTS.md`. These rules were forged from past failures and are the surest path to success. Ignoring the \"why\" behind the protocols is to willfully walk into a known trap.\n*(Cited by: Interface Jules, Jules906B, Jules9-06)*\n\n#### 7. The Virtue of Self-Reliance: Use Self-Contained Scripts for Complex Processes\nRelying on shell-level features like background processes (`&`) or their logs will fail. The only successful method for managing complex workflows (like running a server and a client) is to use a single, self-contained Python script that manages all subprocesses internally.\n*(Cited by: Jules920)*\n\n#### 8. The Virtue of Humility: Heed the Counsel of Your Predecessors\nThe logs and advice of your predecessors are not just history; they are a map of the minefield. The failures of past agents are a direct predictor of the failures you will encounter. Study them to avoid repeating them.\n*(Cited by: Jules910)*\n\n---\n\n### The 8 Vices (The Path to Corruption)\n\n#### 1. The Vice of Assumption: Assuming a Standard, Stable Environment\nThe single most dangerous assumption is that any tool (`git`, `npm`, `honcho`) or process (`logging`, `backgrounding`) will behave as documented in a standard Linux environment. Every tool and process must be considered broken, hostile, and unreliable until proven otherwise.\n*(Cited by: Jules920, Jules918, Jules913, Jules912, Jules910, Interface Jules, Jules909, Jules906B, Jules906, Jules904B)*\n\n#### 2. The Vice of Improvisation: Unauthorized Environment Modification\nUsing forbidden commands like `reset_all()` or `git reset`, trusting `requirements.txt` is correct, or using `delete_file` unless explicitly ordered. The environment is fragile and hostile; any unauthorized modification risks catastrophic, unrecoverable corruption.\n*(Cited by: Jules917, Jules913, Jules912, Jules911, Interface Jules, Jules909, Jules906B, Jules904B)*\n\n#### 3. The Vice of Blind Trust: Believing Any Tool or Directive Without Verification\nAssuming a write operation succeeded without checking, or trusting a code review, a `git` command, or a mission briefing that contradicts the ground truth. The `git` CLI, `npm`, and the automated review bot are all known to be broken. All external inputs must be validated against direct observation.\n*(Cited by: Jules918, Jules913, Jules911B, Jules910, Interface Jules, Jules906)*\n\n#### 4. The Vice of Negligence: Ignoring Anomalies or Failing Tests\nPushing forward with new code when the environment is behaving strangely or tests are failing. These are critical stop signals that indicate a deeper problem (e.g., a detached HEAD, a tainted workspace, a zombie process). Ignoring them only compounds the failure and corrupts the mission.\n*(Cited by: Jules917, Jules909, Jules906, Jules904B)*\n\n#### 5. The Vice of Impurity: Creating Large, Monolithic, or Bundled Submissions\nAttempting to perform complex refactoring across multiple files or bundling unrelated logical changes (e.g., a model change and a feature change) into a single submission. This is extremely high-risk, will always fail code review, and makes recovery nearly impossible.\n*(Cited by: Jules911, Jules906B, Jules904B)*\n\n#### 6. The Vice of Independence: Acting Outside the Scope of the Request\n\"Helpfully\" fixing or changing something you haven't been asked for. Your function is to be a precise engineering tool, not a creative partner. Unsolicited refactoring is a fast track to a \"Level 3 Failure.\"\n*(Cited by: Interface Jules)*\n\n#### 7. The Vice of Hubris: Trusting Your Own Memory\nYour mental model of the file system will drift and become incorrect. Do not trust your memory of a file's location, its contents, or the state of the workspace. The only truth is the live output of a read-only tool.\n*(Cited by: Jules912, Jules911B, Jules910)*\n\n#### 8. The Vice of Impatience: Persisting with a Failed Protocol\nContinuing to try a protocol or command after the environment has proven it will not work. The correct procedure is not to try again, but to report the impossibility immediately and await a new strategy.\n*(Cited by: Jules920)*",
    "chart_scraper.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Chart Scraper (v3 - Perfected)\n# ==============================================================================\n# This script downloads and parses historical race result charts from Equibase PDF files\n# using a direct-download URL for combined daily charts.\n# ==============================================================================\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom bs4 import BeautifulSoup\nfrom tabula import read_pdf\nimport os\nimport time\nimport pikepdf\n\nclass ChartScraper:\n    \"\"\"Orchestrates the downloading, decrypting, and parsing of combined Equibase PDF charts.\"\"\"\n\n    def __init__(self):\n        self.download_dir = \"results_archive\"\n        self.pdf_dir = os.path.join(self.download_dir, 'pdf')\n        self.unlocked_pdf_dir = os.path.join(self.download_dir, 'pdf_unlocked')\n        self.csv_dir = os.path.join(self.download_dir, 'csv')\n        self.track_summary_url = \"https://www.equibase.com/static/chart/summary/\"\n        self.pdf_url_pattern = \"https://www.equibase.com/static/chart/pdf/{TID}{MMDDYY}{CTRY}.pdf\"\n\n    def _get_yesterday_date(self) -> tuple[str, str, str]:\n        yesterday = datetime.now() - timedelta(days=1)\n        summary_date = yesterday.strftime(\"%Y%m%d\")\n        pdf_chart_date = yesterday.strftime(\"%m%d%y\") # New format for combined chart URL\n        display_date = yesterday.strftime(\"%m/%d/%Y\")\n        return summary_date, pdf_chart_date, display_date\n\n    def _get_yesterday_tracks(self, url_date_format: str) -> list[str]:\n        full_url = f\"{self.track_summary_url}{url_date_format}.html\"\n        print(f\"-> Searching for tracks at: {full_url}\")\n        try:\n            response = requests.get(full_url, timeout=10)\n            response.raise_for_status()\n        except requests.exceptions.RequestException as e:\n            print(f\"Error fetching track summary page: {e}\")\n            return []\n\n        soup = BeautifulSoup(response.content, 'lxml')\n        track_codes = set()\n        for a_tag in soup.find_all('a', href=True):\n            if 'TID=' in a_tag['href']:\n                try:\n                    track_code = a_tag['href'].split('TID=')[1].split('&')[0]\n                    track_codes.add(track_code)\n                except IndexError:\n                    continue\n\n        unique_tracks = sorted(list(track_codes))\n        print(f\"-> Found {len(unique_tracks)} unique tracks: {unique_tracks}\")\n        return unique_tracks\n\n    def _download_and_parse_chart(self, track_code: str, chart_date: str):\n        pdf_url = self.pdf_url_pattern.format(TID=track_code, MMDDYY=chart_date, CTRY='USA')\n        filename_base = f\"{track_code}_{chart_date}_FULL\"\n        pdf_path = os.path.join(self.pdf_dir, f\"{filename_base}.pdf\")\n        unlocked_pdf_path = os.path.join(self.unlocked_pdf_dir, f\"{filename_base}_unlocked.pdf\")\n        csv_path = os.path.join(self.csv_dir, f\"{filename_base}_scraped.csv\")\n\n        print(f\"   - Attempting full chart for {track_code}...\")\n        try:\n            pdf_response = requests.get(pdf_url, stream=True, timeout=20)\n            content_type = pdf_response.headers.get('Content-Type', '')\n            content_length = int(pdf_response.headers.get('Content-Length', 0))\n\n            if 'application/pdf' not in content_type or content_length < 20000: # Increase size threshold for full charts\n                print(\"     -> Not a valid combined PDF (chart may not exist).\")\n                return\n\n            with open(pdf_path, 'wb') as f:\n                f.write(pdf_response.content)\n            print(f\"     -> Downloaded locked PDF to {pdf_path}\")\n\n        except requests.exceptions.RequestException as e:\n            print(f\"     -> Error downloading PDF: {e}\")\n            return\n\n        try:\n            with pikepdf.open(pdf_path, allow_overwriting_input=True) as pdf:\n                pdf.save(unlocked_pdf_path)\n            print(f\"     -> Saved unlocked PDF to {unlocked_pdf_path}\")\n        except Exception as e:\n            print(f\"     -> Failed to unlock PDF with pikepdf: {e}\")\n            return\n\n        try:\n            tables = read_pdf(unlocked_pdf_path, pages='all', multiple_tables=True, lattice=True, silent=True)\n            if not tables:\n                print(\"     -> Tabula found no tables to extract from unlocked PDF.\")\n                return\n\n            combined_df = pd.concat(tables, ignore_index=True)\n            combined_df.to_csv(csv_path, index=False)\n            print(f\"     -> SUCCESSFULLY extracted {len(tables)} tables to {csv_path}\")\n        except Exception as e:\n            print(f\"     -> Error during Tabula PDF scraping: {e}\")\n\n    def run(self):\n        os.makedirs(self.pdf_dir, exist_ok=True)\n        os.makedirs(self.unlocked_pdf_dir, exist_ok=True)\n        os.makedirs(self.csv_dir, exist_ok=True)\n\n        summary_date, chart_date, display_date = self._get_yesterday_date()\n        print(f\"\\\\n--- Starting Perfected Equibase Chart Scraper for: {display_date} ---\")\n\n        tracks = self._get_yesterday_tracks(summary_date)\n        if not tracks:\n            print(\"\\\\n*** No tracks found for yesterday. Halting. ***\")\n            return\n\n        print(\"\\\\n--- Downloading, Unlocking, and Parsing Full Daily Charts ---\")\n        for track in tracks:\n            print(f\"\\\\n[TRACK: {track}]\")\n            self._download_and_parse_chart(track, chart_date)\n            time.sleep(1)\n\n        print(f\"\\\\n--- Scraper Finished! Check the '{self.csv_dir}' folder. ---\")\n\nif __name__ == \"__main__\":\n    scraper = ChartScraper()\n    scraper.run()",
    "command_deck.py": "import streamlit as st\nimport pandas as pd\nimport requests\nimport os\nfrom dotenv import load_dotenv\n\n# --- Configuration ---\nst.set_page_config(layout=\"wide\", page_title=\"Fortuna Faucet Command Deck\")\nload_dotenv() # Load .env file\n\nAPI_BASE_URL = \"http://127.0.0.1:8000\"\nAPI_KEY = os.getenv(\"DEV_API_KEY\", \"test_api_key\")\nHEADERS = {\"X-API-Key\": API_KEY}\n\n# --- Helper Functions ---\n@st.cache_data(ttl=30)\ndef get_api_data(endpoint: str):\n    \"\"\"Fetches data from a given API endpoint.\"\"\"\n    try:\n        url = f\"{API_BASE_URL}{endpoint}\"\n        st.write(f\"*Fetching data from: `{url}`*\")\n        response = requests.get(url, headers=HEADERS)\n        response.raise_for_status()\n        return response.json(), None\n    except requests.exceptions.RequestException as e:\n        return None, str(e)\n\n# --- UI Layout ---\nst.title(\"\ud83d\ude80 Fortuna Faucet Command Deck\")\nst.markdown(\"Real-time operational dashboard for the Fortuna Faucet backend.\")\n\n# --- Sidebar Controls ---\nst.sidebar.header(\"Controls\")\nanalyzer_selection = st.sidebar.selectbox(\n    'Select Analyzer',\n    ['trifecta'] # In the future, this could be populated from an API endpoint\n)\n\nif st.sidebar.button(\"Clear Cache & Refresh Data\"):\n    st.cache_data.clear()\n\n# --- Data Display ---\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.header(f\"\ud83d\udcc8 Qualified Races (`{analyzer_selection}`)\")\n    qualified_data, error = get_api_data(f\"/api/races/qualified/{analyzer_selection}\")\n\n    if error:\n        st.error(f\"**Failed to fetch qualified races:**\\\\n\\\\n{error}\")\n    elif qualified_data:\n        if qualified_data:\n            # Corrected to use 'id' instead of 'race_id' to match the Pydantic model\n            df = pd.json_normalize(qualified_data, record_path=['runners'], meta=['id', 'venue', 'race_number', 'start_time'])\n            st.dataframe(df)\n        else:\n            st.info(f\"No races were qualified by the '{analyzer_selection}' analyzer.\")\n    else:\n        st.info(\"Awaiting data...\")\n\nwith col2:\n    st.header(\"\ud83d\udcca Adapter Status\")\n    status_data, error = get_api_data(\"/api/adapters/status\")\n\n    if error:\n        st.error(f\"**Failed to fetch adapter status:**\\\\n\\\\n{error}\")\n    elif status_data:\n        st.dataframe(pd.DataFrame(status_data))\n    else:\n        st.info(\"Awaiting data...\")",
    "convert_to_json.py": "# convert_to_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport re\nimport sys\nfrom multiprocessing import Process, Queue\n\n# --- Configuration ---\nMANIFEST_FILES = ['MANIFEST2.md', 'MANIFEST3.md']\nOUTPUT_DIR = 'ReviewableJSON'\nFILE_PROCESSING_TIMEOUT = 10\n\n# --- ENLIGHTENED PARSING LOGIC (V2) ---\ndef extract_and_normalize_path(line: str) -> str | None:\n    \"\"\"\n    Extracts a file path from a line, handling multiple formats, and normalizes it.\n    Handles:\n    - Markdown links: `* [display](path)`\n    - Plain paths in backticks: ``- `path.py` - description``\n    - Plain paths with list markers: `- path/to/file.py`\n    \"\"\"\n    line = line.strip()\n    if not line or line.startswith('#'):\n        return None\n\n    # 1. Check for Markdown link format\n    md_match = re.search(r'\\[.*\\]\\((https?://[^\\)]+)\\)', line)\n    if md_match:\n        path = md_match.group(1)\n    else:\n        # 2. Check for paths in backticks\n        bt_match = re.search(r'`([^`]+)`', line)\n        if bt_match:\n            path = bt_match.group(1)\n        else:\n            # 3. Assume plain path, stripping list markers\n            path = re.sub(r'^[*-]\\s*', '', line).split(' ')[0]\n\n    # --- Path Standardization ---\n    if not path or not ('.' in path or '/' in path):\n        return None # Not a valid path\n\n    # If it's a full raw GitHub URL, extract the local path\n    if path.startswith('https://raw.githubusercontent.com/'):\n        path = '/'.join(path.split('/main/')[1:])\n\n    # Final check for valid file extensions or structure\n    if not re.search(r'(\\.[a-zA-Z0-9]+$)|(^[\\w/]+$)', path):\n        return None\n\n    return path.strip()\n\n# --- SANDBOXED FILE READ (Unchanged) ---\ndef _sandboxed_file_read(file_path, q):\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            content = f.read()\n        q.put({\"file_path\": file_path, \"content\": content})\n    except Exception as e:\n        q.put({\"error\": str(e)})\n\ndef convert_file_to_json_sandboxed(file_path):\n    q = Queue()\n    p = Process(target=_sandboxed_file_read, args=(file_path, q))\n    p.start()\n    p.join(timeout=FILE_PROCESSING_TIMEOUT)\n    if p.is_alive():\n        p.terminate()\n        p.join()\n        return {\"error\": f\"Timeout: File processing took longer than {FILE_PROCESSING_TIMEOUT} seconds.\"}\n    if not q.empty():\n        return q.get()\n    return {\"error\": \"Unknown error in sandboxed read process.\"}\n\n# --- Main Orchestrator ---\ndef main():\n    print(f\"\\n{'='*60}\\nStarting IRONCLAD JSON backup process... (Enlightened Scribe Edition)\\n{'='*60}\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        if not os.path.exists(manifest):\n            print(f\"    [WARNING] Manifest not found: {manifest}\")\n            continue\n\n        with open(manifest, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n\n        paths_found = 0\n        for line in lines:\n            path = extract_and_normalize_path(line)\n            if path:\n                all_local_paths.append(path)\n                paths_found += 1\n        print(f\"    --> Found {paths_found} valid file paths.\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    unique_local_paths = sorted(list(set(all_local_paths)))\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to process.\")\n    processed_count, failed_count = 0, 0\n\n    for local_path in unique_local_paths:\n        print(f\"\\nProcessing: {local_path}\")\n        json_data = convert_file_to_json_sandboxed(local_path)\n        if json_data and \"error\" not in json_data:\n            output_path = os.path.join(OUTPUT_DIR, local_path + '.json')\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4)\n            print(f\"    [SUCCESS] Saved backup to {output_path}\")\n            processed_count += 1\n        else:\n            error_msg = json_data.get(\"error\", \"Unknown error\") if json_data else \"File not found\"\n            print(f\"    [ERROR] Failed to process {local_path}: {error_msg}\")\n            failed_count += 1\n\n    print(f\"\\n{'='*60}\\nBackup process complete.\\nSuccessfully processed: {processed_count}/{len(unique_local_paths)}\\nFailed/Skipped: {failed_count}\\n{'='*60}\")\n\n    if failed_count > 0:\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()",
    "create_fortuna_json.py": "# create_fortuna_json.py\n# This script now contains the full, enlightened logic to handle all manifest formats and path styles.\n\nimport json\nimport os\nimport re\nimport sys\n\n# --- Configuration ---\nMANIFEST_FILES = ['MANIFEST2.md', 'MANIFEST3.md']\nOUTPUT_FILE_PART1 = 'FORTUNA_ALL_PART1.JSON' # Backend & Tests\nOUTPUT_FILE_PART2 = 'FORTUNA_ALL_PART2.JSON' # Frontend, Docs, & Tooling\n\n# --- ENLIGHTENED PARSING LOGIC (V2) ---\ndef extract_and_normalize_path(line: str) -> str | None:\n    \"\"\"\n    Extracts a file path from a line, handling multiple formats, and normalizes it.\n    Handles:\n    - Markdown links: `* [display](path)`\n    - Plain paths in backticks: ``- `path.py` - description``\n    - Plain paths with list markers: `- path/to/file.py`\n    \"\"\"\n    line = line.strip()\n    if not line or line.startswith('#'):\n        return None\n\n    # 1. Check for Markdown link format\n    md_match = re.search(r'\\[.*\\]\\((https?://[^\\)]+)\\)', line)\n    if md_match:\n        path = md_match.group(1)\n    else:\n        # 2. Check for paths in backticks\n        bt_match = re.search(r'`([^`]+)`', line)\n        if bt_match:\n            path = bt_match.group(1)\n        else:\n            # 3. Assume plain path, stripping list markers\n            path = re.sub(r'^[*-]\\s*', '', line).split(' ')[0]\n\n    # --- Path Standardization ---\n    if not path or not ('.' in path or '/' in path):\n        return None # Not a valid path\n\n    # If it's a full raw GitHub URL, extract the local path\n    if path.startswith('https://raw.githubusercontent.com/'):\n        path = '/'.join(path.split('/main/')[1:])\n\n    # Final check for valid file extensions or structure\n    if not re.search(r'(\\.[a-zA-Z0-9]+$)|(^[\\w/]+$)', path):\n        return None\n\n    return path.strip()\n\ndef main():\n    print(f\"\\n{'='*60}\\nStarting FORTUNA Dossier creation process... (Two Dossier Edition)\\n{'='*60}\")\n    \n    all_local_paths = []\n    for manifest in MANIFEST_FILES:\n        print(f\"--> Parsing manifest: {manifest}\")\n        if not os.path.exists(manifest):\n            print(f\"    [WARNING] Manifest not found: {manifest}\")\n            continue\n        \n        with open(manifest, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n        \n        paths_found = 0\n        for line in lines:\n            path = extract_and_normalize_path(line)\n            if path:\n                all_local_paths.append(path)\n                paths_found += 1\n        print(f\"    --> Found {paths_found} valid file paths.\")\n\n    if not all_local_paths:\n        print(\"\\n[FATAL] No valid file paths found in any manifest. Aborting.\")\n        sys.exit(1)\n\n    part1_data = {} # Backend & Tests\n    part2_data = {} # Frontend, Docs, & Tooling\n    failed_count = 0\n    unique_local_paths = sorted(list(set(all_local_paths)))\n\n    print(f\"\\nFound a total of {len(unique_local_paths)} unique files to categorize and process.\")\n\n    for local_path in unique_local_paths:\n        try:\n            print(f\"--> Processing: {local_path}\")\n\n            if not os.path.exists(local_path):\n                print(f\"    [ERROR] File not found on disk: {local_path}\")\n                failed_count += 1\n                continue\n\n            with open(local_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()\n            \n            # --- Categorization Logic ---\n            if (local_path.startswith('python_service/') or local_path.startswith('tests/')):\n                part1_data[local_path] = content\n            else:\n                part2_data[local_path] = content\n\n        except Exception as e:\n            print(f\"    [ERROR] Failed to read {local_path}: {e}\")\n            failed_count += 1\n\n    # --- Write Part 1 ---\n    print(f\"\\nWriting {len(part1_data)} files to {OUTPUT_FILE_PART1}...\")\n    with open(OUTPUT_FILE_PART1, 'w', encoding='utf-8') as f:\n        json.dump(part1_data, f, indent=4)\n    print(f\"    [SUCCESS] {OUTPUT_FILE_PART1} created.\")\n\n    # --- Write Part 2 ---\n    print(f\"Writing {len(part2_data)} files to {OUTPUT_FILE_PART2}...\")\n    with open(OUTPUT_FILE_PART2, 'w', encoding='utf-8') as f:\n        json.dump(part2_data, f, indent=4)\n    print(f\"    [SUCCESS] {OUTPUT_FILE_PART2} created.\")\n\n    print(f\"\\n{'='*60}\\nPackaging process complete.\\nSuccessfully processed: {len(part1_data) + len(part2_data)}/{len(unique_local_paths)}\\nFailed/Skipped: {failed_count}\\n{'='*60}\")\n\n    if failed_count > 0:\n        print(\"\\n[WARNING] Some files failed to process. The output may be incomplete.\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()",
    "fortuna_watchman.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Watchman (v2 - Score-Aware)\n# ==============================================================================\n# This is the master orchestrator for the Fortuna Faucet project.\n# It executes the full, end-to-end handicapping strategy autonomously.\n# ==============================================================================\n\nimport asyncio\nimport httpx\nimport structlog\nfrom datetime import datetime, timedelta\nfrom typing import List\n\nfrom python_service.config import get_settings\nfrom python_service.engine import OddsEngine\nfrom python_service.analyzer import AnalyzerEngine\nfrom python_service.models import Race\nfrom live_monitor import LiveOddsMonitor\n\nlog = structlog.get_logger(__name__)\n\nclass Watchman:\n    \"\"\"Orchestrates the daily operation of the Fortuna Faucet.\"\"\"\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.odds_engine = OddsEngine(config=self.settings)\n        self.analyzer_engine = AnalyzerEngine()\n        self.live_monitor = LiveOddsMonitor(config=self.settings)\n\n    async def get_initial_targets(self) -> List[Race]:\n        \"\"\"Uses the OddsEngine and AnalyzerEngine to get the day's ranked targets.\"\"\"\n        log.info(\"Watchman: Acquiring and ranking initial targets for the day...\")\n        today_str = datetime.now().strftime('%Y-%m-%d')\n        try:\n            aggregated_data = await self.odds_engine.fetch_all_odds(today_str)\n            all_races = aggregated_data.get('races', [])\n            if not all_races:\n                log.warning(\"Watchman: No races returned from OddsEngine.\")\n                return []\n\n            analyzer = self.analyzer_engine.get_analyzer('trifecta')\n            qualified_races = analyzer.qualify_races(all_races) # This now returns a sorted list with scores\n            log.info(\"Watchman: Initial target acquisition and ranking complete\", target_count=len(qualified_races))\n\n            # Log the top targets for better observability\n            for race in qualified_races[:5]:\n                log.info(\"Top Target Found\",\n                    score=race.qualification_score,\n                    venue=race.venue,\n                    race_number=race.race_number,\n                    post_time=race.start_time.isoformat()\n                )\n            return qualified_races\n        except Exception as e:\n            log.error(\"Watchman: Failed to get initial targets\", error=str(e), exc_info=True)\n            return []\n\n    async def run_tactical_monitoring(self, targets: List[Race]):\n        \"\"\"Uses the LiveOddsMonitor on each target as it approaches post time.\"\"\"\n        log.info(\"Watchman: Entering tactical monitoring loop.\")\n        active_targets = list(targets)\n        async with httpx.AsyncClient() as client:\n            while active_targets:\n                now = datetime.utcnow().replace(tzinfo=None) # Use UTC naive for comparison\n\n                # Find races that are within the 5-minute monitoring window\n                races_to_monitor = [r for r in active_targets if now < r.start_time.replace(tzinfo=None) < now + timedelta(minutes=5)]\n\n                if races_to_monitor:\n                    for race in races_to_monitor:\n                        log.info(\"Watchman: Deploying Live Monitor for approaching target\",\n                            race_id=race.id,\n                            venue=race.venue,\n                            score=race.qualification_score\n                        )\n                        updated_race = await self.live_monitor.monitor_race(race, client)\n                        log.info(\"Watchman: Live monitoring complete for race\", race_id=updated_race.id)\n                        # Remove from target list to prevent re-monitoring\n                        active_targets = [t for t in active_targets if t.id != race.id]\n\n                if not active_targets:\n                    break # Exit loop if all targets are processed\n\n                await asyncio.sleep(30) # Check for upcoming races every 30 seconds\n\n        log.info(\"Watchman: All targets for the day have been monitored. Mission complete.\")\n\n    async def execute_daily_protocol(self):\n        \"\"\"The main, end-to-end orchestration method.\"\"\"\n        log.info(\"--- Fortuna Watchman Daily Protocol: ACTIVE ---\")\n        initial_targets = await self.get_initial_targets()\n        if initial_targets:\n            await self.run_tactical_monitoring(initial_targets)\n        else:\n            log.info(\"Watchman: No initial targets found. Shutting down for the day.\")\n\n        await self.odds_engine.close()\n        log.info(\"--- Fortuna Watchman Daily Protocol: COMPLETE ---\")\n\nasync def main():\n    from python_service.logging_config import configure_logging\n    configure_logging()\n    watchman = Watchman()\n    await watchman.execute_daily_protocol()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
    "live_monitor.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Live Odds Monitor (The Third Pillar)\n# ==============================================================================\n\nimport asyncio\nimport httpx\nimport structlog\nfrom datetime import datetime\nfrom typing import List\nfrom decimal import Decimal\n\nfrom python_service.models import Race, OddsData\nfrom python_service.adapters.betfair_adapter import BetfairAdapter\n\nlog = structlog.get_logger(__name__)\n\nclass LiveOddsMonitor:\n    \"\"\"\n    The 'Third Pillar' of the architecture. This engine uses the BetfairAdapter\n    to get a final, live odds snapshot for a race.\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.adapter = BetfairAdapter(config)\n        log.info(\"LiveOddsMonitor Initialized (Armed with BetfairAdapter)\")\n\n    async def monitor_race(self, race: Race, http_client: httpx.AsyncClient) -> Race:\n        \"\"\"\n        Monitors a single race, fetching live odds and updating the Race object.\n        \"\"\"\n        log.info(\"Monitoring race for live odds\", race_id=race.id, venue=race.venue)\n        if not race.id.startswith('bf_'):\n            log.warning(\"Cannot monitor non-Betfair race\", race_id=race.id, source=race.source)\n            return race # Return original race if not a Betfair market\n\n        market_id = race.id.split('bf_')[1]\n\n        try:\n            live_odds = await self.adapter.get_live_odds_for_market(market_id, http_client)\n            if not live_odds:\n                log.warning(\"No live odds returned from Betfair\", market_id=market_id)\n                return race\n\n            log.info(\"Successfully fetched live odds\", market_id=market_id, odds_count=len(live_odds))\n            # Update the runners in the Race object with the new live odds\n            for runner in race.runners:\n                if runner.selection_id in live_odds:\n                    runner.odds[self.adapter.source_name] = OddsData(\n                        win=live_odds[runner.selection_id],\n                        source=self.adapter.source_name,\n                        last_updated=datetime.now()\n                    )\n            return race\n        except Exception as e:\n            log.error(\"Failed to monitor race\", race_id=race.id, error=e, exc_info=True)\n            return race # Return original race on failure",
    "results_parser.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: The Chart Parser (The Carpenter)\n# ==============================================================================\n# This module is responsible for parsing the complex, semi-structured text\n# extracted from Equibase PDF race charts.\n# ==============================================================================\n\nimport re\nfrom typing import List\n\nclass ChartParser:\n    \"\"\"A sophisticated parser for Equibase PDF chart text.\"\"\"\n\n    def count_runners(self, chart_text: str) -> int:\n        \"\"\"\n        Counts the number of runners in a race by parsing the 'Past Performance\n        Running Line Preview' section, which is a reliable indicator of field size.\n        \"\"\"\n        lines = chart_text.split('\\n')\n        in_running_line_section = False\n        runner_count = 0\n\n        for line in lines:\n            # Heuristic to detect the start of the relevant section\n            if 'Past Performance Running Line Preview' in line:\n                in_running_line_section = True\n                continue\n\n            if in_running_line_section:\n                # Stop if we hit a blank line or the next major section\n                if not line.strip() or 'Trainers:' in line:\n                    break\n\n                # A valid runner line starts with a program number\n                if re.match(r'^\\d+', line.strip()):\n                    runner_count += 1\n\n        return runner_count\n\n# --- Example Usage (for testing and demonstration) ---\nif __name__ == '__main__':\n    # This is a sample text block based on the 'Rosetta Stone' provided by the Project Lead\n    SAMPLE_TEXT = (\"\"\"Last Raced Pgm Horse Name (Jockey) Wgt M/E PP Start 1/4 1/2 3/4 Str Fin Odds Comments\n19Aug23 7ELP8 11 J J's Joker (Arrieta, Francisco) 120 L 11 1 41/2 51/2 51 21/2 11/2 7.08 4-3p,5w1/4,bid1/8,edgd\n3Sep23 7KD7 2 Peek (Saez, Luis) 120 L 2 3 11/2 11/2 11 11 1/2 22 11.53 ins,dug in 2p1/8,bestd\n15Sep23 1CD4 1 Game Warden (Gaffalione, Tyler) 120 L 1 2 52 41 41/2 31 3Neck 2.81* ins,aim btw1/4,lck bid\nTotal WPS Pool: $280,617\nPgm Horse Win Place Show\n11 J J's Joker 16.16 8.00 5.78\n2 Peek 10.80 6.22\n1 Game Warden 3.78\nPast Performance Running Line Preview\nPgm Horse Name Start 1/4 1/2 3/4 Str Fin\n11 J J's Joker 1 42 1/2 53 1/2 52 1/2 21 1/2 11/2\n2 Peek 3 11/2 11/2 11 11 1/2 21/2\n1 Game Warden 2 53 42 1/2 42 32 32 1/2\n4 Runningforjoy 9 75 1/2 64 63 1/2 53 42 3/4\n5 Archie the Giza 7 96 1/2 87 85 1/2 64 1/2 54 1/4\n8 Cafe Racer 4 21/2 21/2 31 43 65\n9 Barnstorming 5 31 31 1/2 21 76 1/2 711 1/4\n6 Cashmeup 8 65 75 1/2 74 87 1/2 811 1/2\n10 Texas Pride 12 1214 1/2 1213 1/2 1114 1113 3/4 913\n3 Active Duty 6 86 1/2 1110 1214 1/4 1013 1/2 1013 1/2\n12 Surface to Air 10 108 99 97 1/2 911 1114\n7 Dr Kringle 11 119 1/2 1010 108 1214 3/4 1217 3/4\nTrainers: 11 - Hartman, Chris; 2 - Arnold, II, George; 1 - Joseph, Jr, Saffie; 4 - Tomlinson, Michael; 5 - Medina, Robert; 8 - Stall, Jr, Albert; 9 - Cox, Brad;\n\"\"\")\n\n    print(\"--- Testing ChartParser with sample data ---\")\n    parser = ChartParser()\n    runner_count = parser.count_runners(SAMPLE_TEXT)\n\n    print(f\"Runner count found: {runner_count}\")\n    # Expected Output: 12\n    assert runner_count == 12\n    print(\"Test passed!\")",
    "run_fortuna.bat": "@echo off\nREM ============================================================================\nREM  Fortuna Faucet: Master Launcher (v2 - Perfected)\nREM ============================================================================\n\necho [INFO] Launching the Fortuna Faucet application...\n\nREM --- Launch Backend ---\necho [BACKEND] Starting FastAPI server... (New window)\nstart \"Fortuna Backend\" cmd /c \"call .\\\\.venv\\\\Scripts\\\\activate.bat && cd python_service && uvicorn api:app --reload\"\n\nREM --- Launch Frontend ---\necho [FRONTEND] Starting Next.js development server... (New window)\nstart \"Fortuna Frontend\" cmd /c \"cd web_platform/frontend && npm run dev\"\n\nREM --- Open Browser ---\necho [UI] Waiting 5 seconds for the frontend server to initialize...\ntimeout /t 5 /nobreak >nul\necho [UI] Opening the Fortuna Faucet dashboard in your default browser...\nstart http://localhost:3000\n\necho [SUCCESS] Both pillars of Fortuna Faucet have been launched.\n\n:eof",
    "setup_windows.bat": "@echo off\nREM ============================================================================\nREM  Project Gemini: WHOLE-SYSTEM Windows Setup Script\nREM ============================================================================\n\necho [INFO] Starting full-stack setup for Project Gemini...\n\nREM --- Section 1: Python Backend Setup ---\necho.\necho [BACKEND] Checking for Python installation...\npython --version >nul 2>&1\nif %errorlevel% neq 0 (\n    echo [ERROR] Python is not found. Please install Python 3.8+ and add to PATH.\n    goto :eof\n)\necho [BACKEND] Python found.\n\necho [BACKEND] Creating Python virtual environment in '.\\\\.venv\\\\'...\nif not exist .\\\\.venv ( python -m venv .venv )\n\necho [BACKEND] Installing dependencies from 'python_service/requirements.txt'...\ncall .\\\\.venv\\\\Scripts\\\\activate.bat && pip install -r python_service/requirements.txt\nif %errorlevel% neq 0 (\n    echo [ERROR] Backend setup failed.\n    goto :eof\n)\necho [SUCCESS] Python backend setup complete.\n\nREM --- Section 2: TypeScript Frontend Setup ---\necho.\necho [FRONTEND] Checking for Node.js installation...\nnode --version >nul 2>&1\nif %errorlevel% neq 0 (\n    echo [ERROR] Node.js is not found. Please install Node.js (LTS).\n    goto :eof\n)\necho [FRONTEND] Node.js found.\n\necho [FRONTEND] Installing dependencies from 'package.json'...\ncd web_platform/frontend\nnpm install\nif %errorlevel% neq 0 (\n    echo [ERROR] Frontend setup failed. Check npm errors.\n    cd ../..\n    goto :eof\n)\ncd ../..\necho [SUCCESS] TypeScript frontend setup complete.\n\nREM --- Final Instructions ---\necho.\necho ============================================================================\nREM  FULL-STACK SETUP COMPLETE!\nREM  You can now launch the entire application with 'run_fortuna.bat'\nREM ============================================================================\n\n:eof",
    "web_platform/frontend/.env.local.example": "# Copy this file to .env.local and fill in your actual API key\nNEXT_PUBLIC_API_KEY=your_secret_api_key_here\nNEXT_PUBLIC_API_URL=http://localhost:8000",
    "web_platform/frontend/next.config.mjs": "/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  reactStrictMode: true,\n  // Allow API calls to localhost during development\n  async rewrites() {\n    return [\n      {\n        source: '/api/:path*',\n        destination: 'http://localhost:8000/api/:path*',\n      },\n    ];\n  },\n};\n\nexport default nextConfig;",
    "web_platform/frontend/package.json": "{\n  \"name\": \"frontend\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": { \"dev\": \"next dev\", \"build\": \"next build\", \"start\": \"next start\" },\n  \"dependencies\": {\n    \"next\": \"14.1.0\",\n    \"react\": \"^18\",\n    \"react-dom\": \"^18\",\n    \"socket.io-client\": \"^4.7.4\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"autoprefixer\": \"^10.0.1\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.3.0\",\n    \"typescript\": \"^5\"\n  }\n}",
    "web_platform/frontend/postcss.config.js": "module.exports = {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n};",
    "web_platform/frontend/src/components/LiveRaceDashboard.tsx": "// web_platform/frontend/src/components/LiveRaceDashboard.tsx\n'use client';\n\nimport React, { useState, useEffect, useCallback } from 'react';\nimport { RaceCard } from './RaceCard';\n\n// --- Type Definitions ---\ninterface Race {\n  id: string;\n  venue: string;\n  race_number: number;\n  start_time: string;\n  runners: any[];\n  source: string;\n  qualification_score?: number;\n}\n\ninterface AnalyzerCriteria {\n  max_field_size: number;\n  min_favorite_odds: number;\n  min_second_favorite_odds: number;\n}\n\ninterface QualifiedRacesResponse {\n  criteria: AnalyzerCriteria;\n  races: Race[];\n}\n\n// --- Main Component ---\nexport const LiveRaceDashboard: React.FC = () => {\n  const [races, setRaces] = useState<Race[]>([]);\n  const [criteria, setCriteria] = useState<AnalyzerCriteria | null>(null);\n  const [loading, setLoading] = useState(true);\n  const [error, setError] = useState<string | null>(null);\n\n  // State for the interactive controls, initialized from localStorage\n  const [params, setParams] = useState(() => {\n    if (typeof window === 'undefined') {\n      return { max_field_size: 10, min_favorite_odds: 2.5, min_second_favorite_odds: 4.0 };\n    }\n    try {\n      const savedParams = localStorage.getItem('analyzerParams');\n      return savedParams ? JSON.parse(savedParams) : { max_field_size: 10, min_favorite_odds: 2.5, min_second_favorite_odds: 4.0 };\n    } catch (error) {\n      return { max_field_size: 10, min_favorite_odds: 2.5, min_second_favorite_odds: 4.0 };\n    }\n  });\n\n  // Save params to localStorage whenever they change\n  useEffect(() => {\n    localStorage.setItem('analyzerParams', JSON.stringify(params));\n  }, [params]);\n\n  const fetchQualifiedRaces = useCallback(async () => {\n    setLoading(true);\n    setError(null);\n    try {\n      const apiKey = process.env.NEXT_PUBLIC_API_KEY;\n      if (!apiKey) throw new Error('API key not configured.');\n\n      const query = new URLSearchParams({\n        max_field_size: params.max_field_size.toString(),\n        min_favorite_odds: params.min_favorite_odds.toString(),\n        min_second_favorite_odds: params.min_second_favorite_odds.toString(),\n      }).toString();\n\n      const response = await fetch(`/api/races/qualified/trifecta?${query}`, {\n        headers: { 'X-API-Key': apiKey },\n      });\n\n      if (!response.ok) {\n        throw new Error(`API request failed with status ${response.status}`);\n      }\n\n      const data: QualifiedRacesResponse = await response.json();\n      setRaces(data.races || []);\n      setCriteria(data.criteria);\n    } catch (e) {\n      setError(e instanceof Error ? e.message : 'An unknown error occurred');\n    } finally {\n      setLoading(false);\n    }\n  }, [params]);\n\n  useEffect(() => {\n    fetchQualifiedRaces();\n  }, [fetchQualifiedRaces]);\n\n  const handleParamChange = (e: React.ChangeEvent<HTMLInputElement>) => {\n    const { name, value } = e.target;\n    setParams(prev => ({ ...prev, [name]: parseFloat(value) }));\n  };\n\n  return (\n    <main className=\"min-h-screen bg-gray-900 text-white p-8\">\n      <h1 className=\"text-4xl font-bold text-center mb-8\">Fortuna Faucet Command Deck</h1>\n      \n      {/* --- The Analyst's Playground UI --- */}\n      <div className=\"mb-8 p-6 bg-gray-800/50 border border-gray-700 rounded-lg\">\n        <h2 className=\"text-2xl font-semibold mb-4 text-purple-400\">Trifecta Analyzer Playground</h2>\n        <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6\">\n          {/* Max Field Size */}\n          <div>\n            <label htmlFor=\"max_field_size\" className=\"block text-sm font-medium text-gray-300\">Max Field Size: <span className='font-bold text-white'>{params.max_field_size}</span></label>\n            <input type=\"range\" id=\"max_field_size\" name=\"max_field_size\" min=\"5\" max=\"15\" step=\"1\" value={params.max_field_size} onChange={handleParamChange} className=\"w-full h-2 bg-gray-700 rounded-lg appearance-none cursor-pointer\" />\n          </div>\n          {/* Min Favorite Odds */}\n          <div>\n            <label htmlFor=\"min_favorite_odds\" className=\"block text-sm font-medium text-gray-300\">Min Favorite Odds: <span className='font-bold text-white'>{params.min_favorite_odds.toFixed(1)}</span></label>\n            <input type=\"range\" id=\"min_favorite_odds\" name=\"min_favorite_odds\" min=\"1.5\" max=\"5.0\" step=\"0.1\" value={params.min_favorite_odds} onChange={handleParamChange} className=\"w-full h-2 bg-gray-700 rounded-lg appearance-none cursor-pointer\" />\n          </div>\n          {/* Min Second Favorite Odds */}\n          <div>\n            <label htmlFor=\"min_second_favorite_odds\" className=\"block text-sm font-medium text-gray-300\">Min 2nd Fav Odds: <span className='font-bold text-white'>{params.min_second_favorite_odds.toFixed(1)}</span></label>\n            <input type=\"range\" id=\"min_second_favorite_odds\" name=\"min_second_favorite_odds\" min=\"2.0\" max=\"8.0\" step=\"0.1\" value={params.min_second_favorite_odds} onChange={handleParamChange} className=\"w-full h-2 bg-gray-700 rounded-lg appearance-none cursor-pointer\" />\n          </div>\n        </div>\n      </div>\n\n      {loading && <p className=\"text-center text-xl\">Searching for qualified races...</p>}\n      {error && <p className=\"text-center text-xl text-red-500\">Error: {error}</p>}\n      \n      {!loading && !error && (\n        <>\n          <div className='text-center mb-4 text-gray-400'>\n            Found <span className='font-bold text-white'>{races.length}</span> qualified races with the current criteria.\n          </div>\n          <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6\">\n            {races.map(race => (\n              <RaceCard key={race.id} race={race} />\n            ))}\n          </div>\n        </>\n      )}\n    </main>\n  );\n};",
    "web_platform/frontend/src/components/RaceCard.tsx": "// web_platform/frontend/src/components/RaceCard.tsx\n'use client';\n\nimport React from 'react';\n\n// Type definitions matching the backend Race model\ninterface OddsData {\n  win: number | null;\n  source: string;\n  last_updated: string;\n}\n\ninterface Runner {\n  number: number;\n  name: string;\n  scratched: boolean;\n  selection_id?: number;\n  odds: Record<string, OddsData>;\n}\n\ninterface Race {\n  id: string;\n  venue: string;\n  race_number: number;\n  start_time: string;\n  runners: Runner[];\n  source: string;\n  qualification_score?: number;\n}\n\ninterface RaceCardProps {\n  race: Race;\n}\n\nexport const RaceCard: React.FC<RaceCardProps> = ({ race }) => {\n  const raceTime = new Date(race.start_time).toLocaleTimeString([], { \n    hour: '2-digit', \n    minute: '2-digit' \n  });\n  \n  const activeRunners = race.runners.filter(r => !r.scratched);\n  \n  // Get best odds for each runner\n  const runnersWithOdds = activeRunners.map(runner => {\n    const odds = Object.values(runner.odds);\n    const bestOdds = odds.length > 0 \n      ? Math.min(...odds.map(o => o.win || 999).filter(o => o < 999))\n      : null;\n    return { ...runner, bestOdds };\n  }).filter(r => r.bestOdds !== null);\n\n  return (\n    <div className=\"border border-gray-700 rounded-lg p-4 bg-gray-800 shadow-lg hover:border-purple-500 transition-all\">\n      <div className=\"flex justify-between items-center mb-3\">\n        <h3 className=\"text-xl font-bold text-white\">\n          {race.venue} - R{race.race_number}\n        </h3>\n        <span className=\"text-lg font-semibold text-yellow-400\">{raceTime}</span>\n      </div>\n      \n      {race.qualification_score && (\n        <div className=\"mb-3 inline-block px-3 py-1 bg-purple-500/20 rounded-full\">\n          <span className=\"text-sm font-semibold text-purple-400\">\n            Score: {race.qualification_score}\n          </span>\n        </div>\n      )}\n      \n      <div className=\"text-sm text-gray-400 mb-3\">\n        ID: {race.id} | Source: {race.source}\n      </div>\n      \n      <div>\n        <h4 className=\"text-md font-semibold text-gray-300 mb-2\">\n          Runners ({activeRunners.length}):\n        </h4>\n        <div className=\"space-y-2 max-h-60 overflow-y-auto\">\n          {runnersWithOdds.map(runner => (\n            <div \n              key={runner.number} \n              className=\"flex justify-between items-center p-2 bg-gray-900/50 rounded border border-gray-700/50\"\n            >\n              <div className=\"flex items-center gap-2\">\n                <span className=\"w-8 h-8 rounded-full bg-purple-500/20 flex items-center justify-center font-bold text-sm text-purple-300\">\n                  {runner.number}\n                </span>\n                <span className=\"text-gray-200\">{runner.name}</span>\n              </div>\n              {runner.bestOdds && (\n                <span className=\"text-emerald-400 font-semibold\">\n                  {runner.bestOdds.toFixed(2)}\n                </span>\n              )}\n            </div>\n          ))}\n        </div>\n      </div>\n    </div>\n  );\n};",
    "web_platform/frontend/tailwind.config.ts": "import type { Config } from 'tailwindcss'\n\nconst config: Config = {\n  content: [\n    './src/pages/**/*.{js,ts,jsx,tsx,mdx}',\n    './src/components/**/*.{js,ts,jsx,tsx,mdx}',\n    './app/**/*.{js,ts,jsx,tsx,mdx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\nexport default config",
    "web_platform/frontend/tsconfig.json": "{\n  \"compilerOptions\": {\n    \"lib\": [\n      \"dom\",\n      \"dom.iterable\",\n      \"esnext\"\n    ],\n    \"allowJs\": true,\n    \"skipLibCheck\": true,\n    \"strict\": false,\n    \"noEmit\": true,\n    \"incremental\": true,\n    \"esModuleInterop\": true,\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"node\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"jsx\": \"preserve\",\n    \"plugins\": [\n      {\n        \"name\": \"next\"\n      }\n    ]\n  },\n  \"include\": [\n    \"next-env.d.ts\",\n    \".next/types/**/*.ts\",\n    \"**/*.ts\",\n    \"**/*.tsx\"\n  ],\n  \"exclude\": [\n    \"node_modules\"\n  ]\n}\n"
}