{
    "python_service/__init__.py": "# This file makes the python_service directory a Python package.",
    "python_service/analyzer.py": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Type, Optional, Any\nimport structlog\nfrom decimal import Decimal\n\nfrom python_service.models import Race, Runner\n\nlog = structlog.get_logger(__name__)\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    # Filter out invalid or placeholder odds (e.g., > 999)\n    valid_odds = [o.win for o in runner.odds.values() \n                  if o.win is not None and o.win > 0 and o.win < 999]\n\n    if not valid_odds:\n        return None\n\n    return min(valid_odds)\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n    def __init__(self, max_field_size: int = 10, min_favorite_odds: float = 2.5, min_second_favorite_odds: float = 4.0):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        scored_races = []\n        for race in races:\n            # The _evaluate_race method now always returns a float score.\n            race.qualification_score = self._evaluate_race(race)\n            scored_races.append(race)\n\n        scored_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds)\n        }\n\n        log.info(\"Universal scoring complete\", total_races_scored=len(scored_races), criteria=criteria)\n        return {\"criteria\": criteria, \"races\": scored_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply a penalty if hard filters are not met, instead of returning None ---\n        if (len(active_runners) > self.max_field_size or\n            favorite_odds < self.min_favorite_odds or\n            second_favorite_odds < self.min_second_favorite_odds):\n            # Assign a score of 0 to races that would have been filtered out\n            return 0.0\n\n        return round(final_score * 100, 2)\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer('trifecta', TrifectaAnalyzer)\n        log.info(\"AnalyzerEngine discovered plugins\", available_analyzers=list(self.analyzers.keys()))\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)",
    "python_service/api.py": "# python_service/api.py\n\nimport asyncio\nfrom datetime import date\nfrom typing import List\n\nimport aiosqlite\nfrom fastapi import Depends, FastAPI, HTTPException, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\n\n# CRITICAL FIX (BUG #2): Add all required slowapi imports\nfrom slowapi import Limiter\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.util import get_remote_address\n\nfrom python_service.models import AggregatedResponse, Race, TipsheetRace\nfrom python_service.engine import FortunaEngine\n\n# --- Configuration & Initialization ---\nDB_PATH = 'fortuna.db'\n\n# CRITICAL FIX (BUG #2): Correctly instantiate the limiter\nlimiter = Limiter(key_func=get_remote_address)\n\napp = FastAPI(\n    title=\"Fortuna Faucet API\",\n    description=\"Provides access to aggregated and analyzed horse racing data.\",\n    version=\"1.0.0\",\n)\n\n# CRITICAL FIX (BUG #2): Add limiter state and exception handler to the app\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, lambda request, exc: JSONResponse(\n    status_code=429,\n    content={\"error\": f\"Rate limit exceeded: {exc.detail}\"}\n))\n\n# Allow all origins for simplicity in this context\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# --- Dependencies ---\ndef get_current_date() -> date:\n    return date.today()\n\n# --- API Endpoints ---\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"10/minute\")\nasync def get_races_endpoint(request: Request, current_date: date = Depends(get_current_date)):\n    fortuna_engine = FortunaEngine()\n    background_tasks = request.app.state.background_tasks\n    response = await fortuna_engine.get_races(date=current_date.isoformat(), background_tasks=background_tasks)\n    return response\n\n@app.get(\"/api/tipsheet\", response_model=List[TipsheetRace])\n@limiter.limit(\"30/minute\")\nasync def get_tipsheet_endpoint(request: Request, date: date = Depends(get_current_date)):\n    \"\"\"Fetches the generated tipsheet from the database asynchronously.\"\"\"\n    results = []\n    try:\n        async with aiosqlite.connect(DB_PATH) as db:\n            db.row_factory = aiosqlite.Row\n            query = 'SELECT * FROM tipsheet WHERE date(post_time) = ? ORDER BY post_time ASC'\n            async with db.execute(query, (date.isoformat(),)) as cursor:\n                async for row in cursor:\n                    results.append(dict(row))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n    \n    return results\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    app.state.background_tasks = set()\n",
    "python_service/config.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Centralized Configuration\n# =================================\u00e1=============================================\n# This module, restored by the Great Correction, provides a centralized and\n# validated source for all application settings using pydantic-settings.\n# ==============================================================================\n\nfrom pydantic_settings import BaseSettings\nfrom functools import lru_cache\nfrom typing import List, Optional\n\nclass Settings(BaseSettings):\n    # --- Application Security ---\n    API_KEY: str\n\n    # --- Betfair API Credentials ---\n    BETFAIR_APP_KEY: str = \"\"\n    BETFAIR_USERNAME: str = \"\"\n    BETFAIR_PASSWORD: str = \"\"\n\n    # --- Other Adapter Keys ---\n    TVG_API_KEY: str = \"\"\n    RACING_AND_SPORTS_TOKEN: str = \"\"\n    POINTSBET_API_KEY: str = \"\"\n    GREYHOUND_API_URL: Optional[str] = None\n    THE_RACING_API_KEY: Optional[str] = None\n\n    # --- Placeholder keys for restored scrapers (not currently used but good practice) ---\n    AT_THE_RACES_KEY: Optional[str] = None\n    SPORTING_LIFE_KEY: Optional[str] = None\n    TIMEFORM_KEY: Optional[str] = None\n\n    # --- Caching Configuration ---\n    REDIS_URL: str = \"redis://localhost\"\n\n    # --- CORS Configuration ---\n    ALLOWED_ORIGINS: List[str] = [\"http://localhost:3000\", \"http://localhost:3001\"]\n\n    model_config = {\n        \"env_file\": \".env\",\n        \"case_sensitive\": True\n    }\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"Returns a cached instance of the application settings.\"\"\"\n    return Settings()",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport structlog\nimport httpx\nimport redis.asyncio as redis\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Tuple\n\nfrom .models import Race, AggregatedResponse\nfrom .adapters.base import BaseAdapter\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\n\nlog = structlog.get_logger(__name__)\n\nclass FortunaEngine:\n    def __init__(self, config=None):\n        from .config import get_settings\n        self.config = config or get_settings()\n        self.log = structlog.get_logger(self.__class__.__name__)\n        self.adapters: List[BaseAdapter] = [\n            BetfairAdapter(config=self.config),\n            BetfairGreyhoundAdapter(config=self.config),\n            TVGAdapter(config=self.config),\n            RacingAndSportsAdapter(config=self.config),\n            RacingAndSportsGreyhoundAdapter(config=self.config),\n            AtTheRacesAdapter(config=self.config),\n            SportingLifeAdapter(config=self.config),\n            TimeformAdapter(config=self.config),\n            TheRacingApiAdapter(config=self.config),\n            GbgbApiAdapter(config=self.config),\n            HarnessAdapter(config=self.config)\n        ]\n        self.http_client = httpx.AsyncClient()\n        self.redis_client = redis.from_url(self.config.REDIS_URL, decode_responses=True)\n        self.log.info(\"Redis client initialized\", redis_url=self.config.REDIS_URL)\n\n    async def close(self):\n        await self.http_client.aclose()\n        await self.redis_client.aclose() # Use aclose() for async client\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapter, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"Wraps an adapter's fetch call, catches all exceptions, and returns a consistent payload.\"\"\"\n        start_time = datetime.now()\n        try:\n            result = await adapter.fetch_races(date, self.http_client)\n            duration = (datetime.now() - start_time).total_seconds()\n            return (adapter.source_name, result, duration)\n        except Exception as e:\n            duration = (datetime.now() - start_time).total_seconds()\n            log.error(\"Adapter raised an unhandled exception\", adapter=adapter.source_name, error=str(e), exc_info=True)\n            failed_result = {\n                'races': [],\n                'source_info': {\n                    'name': adapter.source_name,\n                    'status': 'FAILED',\n                    'races_fetched': 0,\n                    'error_message': str(e),\n                    'fetch_duration': duration\n                }\n            }\n            return (adapter.source_name, failed_result, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        runner_map[new_runner.number].odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n        return list(race_map.values())\n\n    async def get_races(self, date: str, background_tasks: set, source_filter: str = None) -> Dict[str, Any]:\n        cache_key = f\"fortuna:races:{date}\"\n        if not source_filter: # Only use cache for 'all sources' requests\n            try:\n                cached_data = await self.redis_client.get(cache_key)\n                if cached_data:\n                    self.log.info(\"CACHE HIT\", key=cache_key)\n                    # Pydantic can validate directly from the JSON string\n                    return AggregatedResponse.model_validate_json(cached_data).model_dump()\n            except Exception as e:\n                self.log.error(\"Redis GET failed\", error=str(e))\n\n        self.log.info(\"CACHE MISS\", key=cache_key)\n        target_adapters = self.adapters\n        if source_filter:\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n        for result in results:\n            if isinstance(result, Exception):\n                self.log.error(\"Adapter fetch failed\", error=result, exc_info=False)\n                continue\n            adapter_name, adapter_result, duration = result\n            source_info = adapter_result.get('source_info', {})\n            source_info['fetch_duration'] = round(duration, 2)\n            source_infos.append(source_info)\n            if source_info.get('status') == 'SUCCESS':\n                all_races.extend(adapter_result.get('races', []))\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, '%Y-%m-%d').date(),\n            races=deduped_races,\n            sources=source_infos,\n            metadata={\n                'fetch_time': datetime.now(),\n                'sources_queried': [a.source_name for a in target_adapters],\n                'sources_successful': len([s for s in source_infos if s['status'] == 'SUCCESS']),\n                'total_races': len(deduped_races)\n            }\n        )\n\n        if not source_filter: # Only use cache for 'all sources' requests\n            try:\n                # Cache the result for 5 minutes (300 seconds)\n                await self.redis_client.set(cache_key, response_obj.model_dump_json(), ex=300)\n                self.log.info(\"CACHE SET\", key=cache_key, expiry=300)\n            except Exception as e:\n                self.log.error(\"Redis SET failed\", error=str(e))\n\n        return response_obj.model_dump()",
    "python_service/etl.py": "# python_service/etl.py\n# This module contains the ETL logic for the PostgreSQL data warehouse.\n# Restored based on the 'Code Archaeology Report'.\n\nimport os\nfrom typing import List\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nfrom .models import Race\n\nclass PostgresETL:\n    \"\"\"Data Warehouse ETL\"\"\"\n    def __init__(self):\n        db_url = os.getenv(\"POSTGES_URL\", \"postgresql://user:password@localhost/fortuna_dw\")\n        self.engine = create_engine(db_url)\n\n    def process_and_load(self, analyzed_races: List[Race]):\n        valid_for_historical = []\n        quarantined = []\n        for race in analyzed_races:\n            errors = []\n            if not race.venue: errors.append(\"Missing venue\")\n            if race.race_number is None: errors.append(\"Missing race_number\")\n            if not errors:\n                valid_for_historical.append({\n                    \"race_id\": race.id,\n                    \"track_name\": race.venue,\n                    \"race_number\": race.race_number,\n                    \"post_time\": race.start_time,\n                    \"qualification_score\": race.qualification_score\n                })\n            else:\n                quarantined.append({\n                    \"race_id\": race.id,\n                    \"quarantine_reason\": \", \".join(errors),\n                    \"raw_data\": race.json()\n                })\n        if valid_for_historical:\n            pd.DataFrame(valid_for_historical).to_sql('historical_races', self.engine, if_exists='append', index=False)\n        if quarantined:\n            pd.DataFrame(quarantined).to_sql('quarantine_races', self.engine, if_exists='append', index=False)",
    "python_service/logging_config.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Logging Configuration\n# ==============================================================================\n# This module, restored by the Great Correction, provides a centralized\n# configuration for structured logging using structlog.\n# ==============================================================================\n\nimport logging\nimport sys\nimport structlog\n\ndef configure_logging():\n    \"\"\"Configures structlog for JSON-based structured logging.\"\"\"\n    logging.basicConfig(\n        format=\"%(message)s\",\n        stream=sys.stdout,\n        level=logging.INFO,\n    )\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.add_logger_name,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )",
    "python_service/models.py": "# python_service/models.py\n\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime, date\nfrom decimal import Decimal\n\nclass OddsData(BaseModel):\n    win: Optional[Decimal] = None\n    source: str\n    last_updated: datetime\n\n    @field_validator('win')\n    def win_must_be_positive(cls, v):\n        if v is not None and v <= Decimal(\"1.0\"):\n            raise ValueError('Odds must be greater than 1.0')\n        return v\n\nclass Runner(BaseModel):\n    number: int = Field(..., gt=0, lt=100)\n    name: str = Field(..., max_length=100)\n    scratched: bool = False\n    selection_id: Optional[int] = None # For Betfair Exchange integration\n    odds: Dict[str, OddsData] = Field(default_factory=dict)\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\nclass Race(BaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., gt=0, lt=21)\n    start_time: datetime\n    runners: List[Runner]\n    source: str\n    qualification_score: Optional[float] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n\n    @field_validator('runners')\n    def runner_numbers_must_be_unique(cls, v):\n        numbers = [r.number for r in v]\n        if len(numbers) != len(set(numbers)):\n            raise ValueError('Runner numbers must be unique within a race')\n        return v\n\nclass SourceInfo(BaseModel):\n    name: str\n    status: str\n    races_fetched: int\n    error_message: Optional[str] = None\n    fetch_duration: float\n\nclass FetchMetadata(BaseModel):\n    fetch_time: datetime\n    sources_queried: List[str]\n    sources_successful: int\n    total_races: int\n\nclass AggregatedResponse(BaseModel):\n    date: date\n    races: List[Race]\n    sources: List[SourceInfo]\n    metadata: FetchMetadata\n\nclass QualifiedRacesResponse(BaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\nclass TipsheetRace(BaseModel):\n    \"\"\"Represents a single race in the daily tipsheet.\"\"\"\n    race_id: str\n    track_name: str\n    race_number: int\n    post_time: datetime\n    score: float\n    factors: str # Storing as JSON string\n    track_name: str\n    race_number: int\n    post_time: datetime\n    score: float\n    factors: str # Storing as JSON string",
    "python_service/requirements.txt": "requests==2.31.0\npython-dotenv==1.0.0\npydantic==2.5.2\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\naiohttp==3.9.0\nhttpx==0.24.1\npytest==8.4.2\n\nstructlog\nrespx\npytest-asyncio\nstreamlit\npandas\ntabula-py\nlxml\nbeautifulsoup4\npikepdf\npydantic-settings==2.1.0\nslowapi==0.1.8\n\ntenacity\n\n# Caching Layer\nredis==5.0.1\nfakeredis==2.20.0 # For testing\n\n# Code Quality & Linting\nruff\n\naiosqlite",
    "python_service/security.py": "# python_service/security.py\n\nimport secrets\nfrom fastapi import Security, HTTPException, status, Depends\nfrom fastapi.security import APIKeyHeader\n\nfrom .config import Settings, get_settings\n\nAPI_KEY_NAME = \"X-API-Key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)\n\nasync def verify_api_key(\n    key: str = Security(api_key_header),\n    settings: Settings = Depends(get_settings)\n):\n    \"\"\"\n    Verifies the provided API key against the one in settings using a\n    timing-attack resistant comparison.\n    \"\"\"\n    if secrets.compare_digest(key, settings.API_KEY):\n        return True\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Invalid or missing API Key\"\n        )"
}