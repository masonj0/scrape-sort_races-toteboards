{
    "python_service/analyzer.py": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Type, Optional, Any\nimport structlog\nfrom decimal import Decimal\nimport os\nfrom pathlib import Path\n\nfrom python_service.models import Race, Runner\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept ImportError:\n    winsound = None\ntry:\n    from win10toast_py3 import ToastNotifier\nexcept (ImportError, RuntimeError):\n    # Fails gracefully on non-Windows systems\n    ToastNotifier = None\n\nlog = structlog.get_logger(__name__)\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    # Filter out invalid or placeholder odds (e.g., > 999)\n    valid_odds = [o.win for o in runner.odds.values()\n                  if o.win is not None and o.win > 0 and o.win < 999]\n\n    if not valid_odds:\n        return None\n\n    return min(valid_odds)\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n    def __init__(self, max_field_size: int = 10, min_favorite_odds: float = 2.5, min_second_favorite_odds: float = 4.0):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        scored_races = []\n        for race in races:\n            # The _evaluate_race method now always returns a float score.\n            race.qualification_score = self._evaluate_race(race)\n            scored_races.append(race)\n\n        scored_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds)\n        }\n\n        log.info(\"Universal scoring complete\", total_races_scored=len(scored_races), criteria=criteria)\n        \n        for race in scored_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": scored_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply a penalty if hard filters are not met, instead of returning None ---\n        if (len(active_runners) > self.max_field_size or\n            favorite_odds < self.min_favorite_odds or\n            second_favorite_odds < self.min_second_favorite_odds):\n            # Assign a score of 0 to races that would have been filtered out\n            return 0.0\n\n        return round(final_score * 100, 2)\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer('trifecta', TrifectaAnalyzer)\n        log.info(\"AnalyzerEngine discovered plugins\", available_analyzers=list(self.analyzers.keys()))\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n    def __init__(self):\n        self.sounds = {\n            'high_value': Path(__file__).parent.parent.parent / 'assets' / 'sounds' / 'alert_premium.wav',\n        }\n\n    def play(self, sound_type: str):\n        if not winsound:\n            return\n            \n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\nclass RaceNotifier:\n    \"\"\"Handles sending native Windows notifications and audio alerts for high-value races.\"\"\"\n    def __init__(self):\n        self.toaster = ToastNotifier() if ToastNotifier else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n\n    def notify_qualified_race(self, race):\n        if not self.toaster or race.id in self.notified_races:\n            return\n\n        title = f\"\ud83c\udfc7 High-Value Opportunity!\"\n        message = f\\\"\\\"\\\"{race.venue} - Race {race.race_number}\nScore: {race.qualification_score:.0f}%\nPost Time: {race.start_time.strftime('%I:%M %p')}\\\"\\\"\\\"\n        \n        try:\n            # The `threaded=True` argument is crucial to prevent blocking the main application thread.\n            self.toaster.show_toast(title, message, duration=10, threaded=True)\n            self.notified_races.add(race.id)\n            self.audio_system.play('high_value')\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            # Catch potential exceptions from the notification library itself\n            log.error(\"Failed to send notification\", error=str(e), exc_info=True)\n",
    "python_service/api.py": "# python_service/api.py\n\nimport structlog\nfrom datetime import datetime, date\nfrom typing import Optional\nfrom fastapi import FastAPI, HTTPException, Request, Depends, Query\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom python_service.middleware.error_handler import ErrorRecoveryMiddleware\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nimport aiosqlite\nfrom typing import List\nfrom contextlib import asynccontextmanager\n\nfrom .config import get_settings\nfrom .engine import FortunaEngine\nfrom .models import AggregatedResponse, QualifiedRacesResponse, TipsheetRace\nfrom .security import verify_api_key\nfrom .logging_config import configure_logging\nfrom .analyzer import AnalyzerEngine\n\nlog = structlog.get_logger()\n\n# Define the lifespan context manager for robust startup/shutdown\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Manage the application's lifespan. On startup, it initializes the OddsEngine\n    with validated settings and attaches it to the app state. On shutdown, it\n    properly closes the engine's resources.\n    \"\"\"\n    configure_logging()\n    settings = get_settings()\n    app.state.engine = FortunaEngine(config=settings)\n    app.state.analyzer_engine = AnalyzerEngine()\n    log.info(\"Server startup: Configuration validated and FortunaEngine initialized.\")\n    yield\n    # Clean up the engine resources\n    await app.state.engine.close()\n    log.info(\"Server shutdown: HTTP client resources closed.\")\n\nlimiter = Limiter(key_func=get_remote_address)\n\n# Pass the lifespan manager to the FastAPI app\napp = FastAPI(title=\"Checkmate Ultimate Solo API\", version=\"2.1\", lifespan=lifespan)\napp.add_middleware(SlowAPIMiddleware)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\nsettings = get_settings()\n\n# Add middlewares (order can be important)\napp.add_middleware(ErrorRecoveryMiddleware)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True, allow_methods=[\"GET\"], allow_headers=[\"*\"]\n)\n\n# Dependency function to get the engine instance from the app state\ndef get_engine(request: Request) -> FortunaEngine:\n    return request.app.state.engine\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(request: Request, engine: FortunaEngine = Depends(get_engine), _=Depends(verify_api_key)):\n    \"\"\"Provides a list of health statuses for all adapters, required by the new frontend blueprint.\"\"\"\n    try:\n        statuses = engine.get_all_adapter_statuses()\n        return statuses\n    except Exception:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races/qualified/{analyzer_name}\", response_model=QualifiedRacesResponse)\n@limiter.limit(\"30/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[date] = None,\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    # --- Dynamic Analyzer Parameters ---\n    max_field_size: Optional[int] = Query(None, description=\"Override the max field size for the analyzer.\"),\n    min_favorite_odds: Optional[float] = Query(None, description=\"Override the min favorite odds.\"),\n    min_second_favorite_odds: Optional[float] = Query(None, description=\"Override the min second favorite odds.\")\n):\n    \"\"\"\n    Gets all races for a given date, filters them for qualified betting\n    opportunities, and returns the qualified races.\n    \"\"\"\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime('%Y-%m-%d')\n        background_tasks = set() # Dummy background tasks\n        aggregated_data = await engine.get_races(date_str, background_tasks)\n\n        races = aggregated_data.get('races', [])\n\n        analyzer_engine = request.app.state.analyzer_engine\n        analyzer_params = {\n            \"max_field_size\": max_field_size,\n            \"min_favorite_odds\": min_favorite_odds,\n            \"min_second_favorite_odds\": min_second_favorite_odds\n        }\n        custom_params = {k: v for k, v in analyzer_params.items() if v is not None}\n\n        analyzer = analyzer_engine.get_analyzer(analyzer_name, **custom_params)\n        result = analyzer.qualify_races(races)\n        return QualifiedRacesResponse(**result)\n    except ValueError as e:\n        log.warning(\"Requested analyzer not found\", analyzer_name=analyzer_name)\n        raise HTTPException(status_code=404, detail=str(e))\n    except Exception as e:\n        log.error(\"Error in /api/races/qualified\", error=str(e), exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[date] = None,\n    source: Optional[str] = None,\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key)\n):\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime('%Y-%m-%d')\n        background_tasks = set() # Dummy background tasks\n        aggregated_data = await engine.get_races(date_str, background_tasks, source)\n        return aggregated_data\n    except Exception:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\nDB_PATH = 'fortuna.db'\n\ndef get_current_date() -> date:\n    return datetime.now().date()\n\n@app.get(\"/api/tipsheet\", response_model=List[TipsheetRace])\n@limiter.limit(\"30/minute\")\nasync def get_tipsheet_endpoint(request: Request, date: date = Depends(get_current_date)):\n    \"\"\"Fetches the generated tipsheet from the database asynchronously.\"\"\"\n    results = []\n    try:\n        async with aiosqlite.connect(DB_PATH) as db:\n            db.row_factory = aiosqlite.Row\n            query = 'SELECT * FROM tipsheet WHERE date(post_time) = ? ORDER BY post_time ASC'\n            async with db.execute(query, (date.isoformat(),)) as cursor:\n                async for row in cursor:\n                    results.append(dict(row))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n    return results\n",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport structlog\nimport httpx\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Tuple\nfrom decimal import Decimal\nfrom datetime import timezone\n\nfrom .models import Race, Runner, OddsData, AggregatedResponse\nfrom .models_v3 import NormalizedRace, NormalizedRunner\nfrom .cache_manager import cache_async_result\nfrom .adapters.base import BaseAdapter\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\n\nlog = structlog.get_logger(__name__)\n\nclass FortunaEngine:\n    def __init__(self, config=None):\n        from .config import get_settings\n        self.config = config or get_settings()\n        self.log = structlog.get_logger(self.__class__.__name__)\n        self.adapters: List[BaseAdapter] = [\n            BetfairAdapter(config=self.config),\n            BetfairGreyhoundAdapter(config=self.config),\n            TVGAdapter(config=self.config),\n            RacingAndSportsAdapter(config=self.config),\n            RacingAndSportsGreyhoundAdapter(config=self.config),\n            AtTheRacesAdapter(config=self.config),\n            SportingLifeAdapter(config=self.config),\n            TimeformAdapter(config=self.config),\n            TheRacingApiAdapter(config=self.config),\n            GbgbApiAdapter(config=self.config),\n            HarnessAdapter(config=self.config)\n        ]\n        # V3 ADAPTERS\n        self.v3_adapters = [\n            BetfairDataScientistAdapter(\n                model_name=\"ThoroughbredModel\",\n                url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets?id=thoroughbred-model&date=\"\n            )\n        ]\n        self.http_client = httpx.AsyncClient()\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapter, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"Wraps an adapter's fetch call, catches all exceptions, and returns a consistent payload.\"\"\"\n        start_time = datetime.now()\n        try:\n            result = await adapter.fetch_races(date, self.http_client)\n            duration = (datetime.now() - start_time).total_seconds()\n            return (adapter.source_name, result, duration)\n        except Exception as e:\n            duration = (datetime.now() - start_time).total_seconds()\n            log.error(\"Adapter raised an unhandled exception\", adapter=adapter.source_name, error=str(e), exc_info=True)\n            failed_result = {\n                'races': [],\n                'source_info': {\n                    'name': adapter.source_name,\n                    'status': 'FAILED',\n                    'races_fetched': 0,\n                    'error_message': str(e),\n                    'fetch_duration': duration\n                }\n            }\n            return (adapter.source_name, failed_result, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            key = self._race_key(race)\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        runner_map[new_runner.number].odds.update(new_runner.odds)\n                    else:\n                        existing_race.runners.append(new_runner)\n        return list(race_map.values())\n\n    async def get_races(self, date: str, background_tasks: set, source_filter: str = None) -> Dict[str, Any]:\n        if source_filter:\n            self.log.info(\"Bypassing cache for source-specific request\", source=source_filter)\n            return await self._fetch_races_from_sources(date, source_filter=source_filter)\n\n        return await self._get_all_races_cached(date, background_tasks=background_tasks)\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"fortuna_engine_races\")\n    async def _get_all_races_cached(self, date: str, background_tasks: set) -> Dict[str, Any]:\n        \"\"\"This method fetches races for all sources and its result is cached.\"\"\"\n        self.log.info(\"CACHE MISS: Fetching all races from sources.\", date=date)\n        return await self._fetch_races_from_sources(date)\n\n    def _convert_v3_race_to_v2(self, v3_race: NormalizedRace) -> Race:\n        \"\"\"Converts a V3 NormalizedRace object to a V2 Race object.\"\"\"\n        import re\n        race_number = 0\n        match = re.search(r'\\d+', v3_race.race_name)\n        if match:\n            race_number = int(match.group())\n\n        runners = []\n        for v3_runner in v3_race.runners:\n            odds_data = OddsData(\n                win=Decimal(str(v3_runner.odds_decimal)),\n                source=v3_race.source_ids[0],\n                last_updated=datetime.now(timezone.utc)\n            )\n            runner = Runner(\n                id=v3_runner.runner_id,\n                name=v3_runner.name,\n                number=int(v3_runner.saddle_cloth) if v3_runner.saddle_cloth and v3_runner.saddle_cloth.isdigit() else 99,\n                odds={v3_race.source_ids[0]: odds_data}\n            )\n            runners.append(runner)\n\n        return Race(\n            id=v3_race.race_key,\n            venue=v3_race.track_key,\n            race_number=race_number,\n            start_time=datetime.fromisoformat(v3_race.start_time_iso),\n            runners=runners,\n            source=v3_race.source_ids[0],\n            race_name=v3_race.race_name\n        )\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"odds_engine_fetch\")\n    async def _fetch_races_from_sources(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"Helper method to contain the logic for fetching and aggregating races.\"\"\"\n        target_adapters = self.adapters\n        if source_filter:\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n\n        # Run V3 (synchronous) adapters in a thread pool\n        v3_tasks = [asyncio.to_thread(adapter.fetch_and_normalize) for adapter in self.v3_adapters]\n\n        # Gather results from both V2 (async) and V3 (sync) adapters\n        all_tasks = tasks + v3_tasks\n        results = await asyncio.gather(*all_tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n\n        v3_start_time = datetime.now() # Approximate start time for all V3 adapters\n\n        for result in results:\n            if isinstance(result, Exception):\n                self.log.error(\"Adapter fetch failed\", error=result, exc_info=False)\n                continue\n\n            # Correctly differentiate between V2 and V3 results\n            if isinstance(result, tuple) and len(result) == 3:  # V2 Adapter Result\n                adapter_name, adapter_result, duration = result\n                source_info = adapter_result.get('source_info', {})\n                source_info['fetch_duration'] = round(duration, 2)\n                source_infos.append(source_info)\n                if source_info.get('status') == 'SUCCESS':\n                    all_races.extend(adapter_result.get('races', []))\n            elif isinstance(result, list) and all(isinstance(r, NormalizedRace) for r in result):  # V3 Adapter Result\n                if result:\n                    v3_races = result\n                    adapter_name = v3_races[0].source_ids[0]\n                    translated_races = [self._translate_v3_race_to_v2(nr) for nr in result]\n                    all_races.extend(translated_races)\n\n                    v3_duration = (datetime.now() - v3_start_time).total_seconds()\n                    source_infos.append({\n                        'name': adapter_name,\n                        'status': 'SUCCESS',\n                        'races_fetched': len(translated_races),\n                        'error_message': None,\n                        'fetch_duration': round(v3_duration, 2)\n                    })\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, '%Y-%m-%d').date(),\n            races=deduped_races,\n            sources=source_infos,\n            metadata={\n                'fetch_time': datetime.now(),\n                'sources_queried': [a.source_name for a in target_adapters],\n                'sources_successful': len([s for s in source_infos if s['status'] == 'SUCCESS']),\n                'total_races': len(deduped_races)\n            }\n        )\n\n        # --- Windows Operator Experience Enhancement ---\n        try:\n            from win10toast import ToastNotifier\n            toaster = ToastNotifier()\n            toaster.show_toast(\n                \"Fortuna Faucet\",\n                f\"Initial data load complete. {len(deduped_races)} races are ready for analysis.\",\n                duration=10,\n                threaded=True\n            )\n        except (ImportError, RuntimeError):\n            # Fails gracefully if not on Windows or library is missing\n            pass\n\n        return response_obj.model_dump()\n\n\n    def _translate_v3_race_to_v2(self, norm_race: NormalizedRace) -> Race:\n        \"\"\"Translates a V3 NormalizedRace into a V2 Race object.\"\"\"\n        runners = []\n        for norm_runner in norm_race.runners:\n            adapter_name = norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\"\n            odds_data = OddsData(\n                win=Decimal(str(norm_runner.odds_decimal)),\n                source=adapter_name,\n                last_updated=datetime.now(timezone.utc)\n            )\n\n            try:\n                runner_number = int(norm_runner.saddle_cloth)\n            except (ValueError, TypeError):\n                runner_number = None\n\n            runner = Runner(\n                id=norm_runner.runner_id,\n                name=norm_runner.name,\n                number=runner_number,\n                odds={adapter_name: odds_data}\n            )\n            runners.append(runner)\n\n        return Race(\n            id=norm_race.race_key,\n            venue=norm_race.track_key,\n            start_time=datetime.fromisoformat(norm_race.start_time_iso),\n            race_number=0, # V3 model does not have race_number, placeholder\n            runners=runners,\n            source=norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\",\n            # Store extra V3 data in metadata for future use\n            metadata={\"v3_race_name\": norm_race.race_name}\n        )\n",
    "python_service/models.py": "# python_service/models.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Dict, List, Optional, Any\nfrom pydantic import BaseModel, Field\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    class Config:\n        populate_by_name = True\n        arbitrary_types_allowed = True\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[Decimal] = None\n    place: Optional[Decimal] = None\n    show: Optional[Decimal] = None\n    source: str\n    last_updated: datetime\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias='saddleClothNumber')\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias='raceNumber')\n    start_time: datetime = Field(..., alias='startTime')\n    runners: List[Runner]\n    source: str\n    qualification_score: Optional[float] = Field(None, alias='qualificationScore')\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias='racesFetched')\n    fetch_duration: float = Field(..., alias='fetchDuration')\n    error_message: Optional[str] = Field(None, alias='errorMessage')\n\nclass AggregatedResponse(FortunaBaseModel):\n    races: List[Race]\n    source_info: List[SourceInfo] = Field(..., alias='sourceInfo')\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias='raceId')\n    track_name: str = Field(..., alias='trackName')\n    race_number: int = Field(..., alias='raceNumber')\n    post_time: str = Field(..., alias='postTime')\n    score: float\n    factors: Any # JSON string stored as Any\n"
}