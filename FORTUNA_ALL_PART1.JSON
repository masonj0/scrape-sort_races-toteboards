{
    "python_service/etl.py": "# python_service/etl.py\n# ETL pipeline for populating the historical data warehouse\n\nimport json\nimport logging\nimport os\nfrom datetime import date\n\nimport requests\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ScribesArchivesETL:\n    def __init__(self):\n        self.postgres_url = os.getenv(\"POSTGRES_URL\")\n        self.api_key = os.getenv(\"API_KEY\")\n        self.api_base_url = \"http://localhost:8000\"\n        self.engine = self._get_db_engine()\n\n    def _get_db_engine(self):\n        if not self.postgres_url:\n            logger.warning(\"POSTGRES_URL not set. ETL will be skipped.\")\n            return None\n        try:\n            return create_engine(self.postgres_url)\n        except Exception as e:\n            logger.error(f\"Failed to create database engine: {e}\", exc_info=True)\n            return None\n\n    def _fetch_race_data(self, target_date: date) -> list:\n        \"\"\"Fetches aggregated race data from the local API.\"\"\"\n        if not self.api_key:\n            raise ValueError(\"API_KEY not found in environment.\")\n\n        url = f\"{self.api_base_url}/api/races?race_date={target_date.isoformat()}\"\n        headers = {\"X-API-KEY\": self.api_key}\n        response = requests.get(url, headers=headers, timeout=120)\n        response.raise_for_status()\n        return response.json().get(\"races\", [])\n\n    def _validate_and_transform(self, race: dict) -> tuple:\n        \"\"\"Validates a race dictionary and transforms it for insertion.\"\"\"\n        if not all(k in race for k in [\"id\", \"venue\", \"race_number\", \"start_time\", \"runners\"]):\n            return None, \"Missing core fields (id, venue, race_number, start_time, runners)\"\n\n        active_runners = [r for r in race.get(\"runners\", []) if not r.get(\"scratched\")]\n\n        transformed = {\n            \"race_id\": race[\"id\"],\n            \"venue\": race[\"venue\"],\n            \"race_number\": race[\"race_number\"],\n            \"start_time\": race[\"start_time\"],\n            \"source\": race.get(\"source\"),\n            \"qualification_score\": race.get(\"qualification_score\"),\n            \"field_size\": len(active_runners),\n        }\n        return transformed, None\n\n    def run(self, target_date: date):\n        if not self.engine:\n            return\n\n        logger.info(f\"Starting ETL process for {target_date.isoformat()}...\")\n        try:\n            races = self._fetch_race_data(target_date)\n        except (requests.RequestException, ValueError) as e:\n            logger.error(f\"Failed to fetch race data: {e}\", exc_info=True)\n            return\n\n        clean_records = []\n        quarantined_records = []\n\n        for race in races:\n            transformed, reason = self._validate_and_transform(race)\n            if transformed:\n                clean_records.append(transformed)\n            else:\n                quarantined_records.append(\n                    {\n                        \"race_id\": race.get(\"id\"),\n                        \"source\": race.get(\"source\"),\n                        \"payload\": json.dumps(race),\n                        \"reason\": reason,\n                    }\n                )\n\n        with self.engine.connect() as connection:\n            try:\n                with connection.begin():  # Transaction block\n                    if clean_records:\n                        # Using ON CONFLICT to prevent duplicates\n                        stmt = text(\n                            \"\"\"\n                            INSERT INTO historical_races (\n                                race_id, venue, race_number, start_time, source,\n                                qualification_score, field_size\n                            )\n                            VALUES (\n                                :race_id, :venue, :race_number, :start_time, :source,\n                                :qualification_score, :field_size\n                            )\n                            ON CONFLICT (race_id) DO NOTHING;\n                        \"\"\"\n                        )\n                        connection.execute(stmt, clean_records)\n                        logger.info(f\"Inserted/updated {len(clean_records)} records into historical_races.\")\n\n                    if quarantined_records:\n                        stmt = text(\"\"\"\n                            INSERT INTO quarantined_races (race_id, source, payload, reason)\n                            VALUES (:race_id, :source, :payload::jsonb, :reason);\n                        \"\"\")\n                        connection.execute(stmt, quarantined_records)\n                        logger.warning(f\"Moved {len(quarantined_records)} records to quarantine.\")\n            except SQLAlchemyError as e:\n                logger.error(f\"Database transaction failed: {e}\", exc_info=True)\n\n        logger.info(\"ETL process finished.\")\n\n\ndef run_etl_for_yesterday():\n    from datetime import timedelta\n\n    yesterday = date.today() - timedelta(days=1)\n    etl = ScribesArchivesETL()\n    etl.run(yesterday)\n",
    "python_service/config.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Centralized Configuration\n# =================================\u00e1=============================================\n# This module, restored by the Great Correction, provides a centralized and\n# validated source for all application settings using pydantic-settings.\n# ==============================================================================\n\nfrom functools import lru_cache\nfrom typing import List\nfrom typing import Optional\n\nfrom pydantic_settings import BaseSettings\n\n\nclass Settings(BaseSettings):\n    # --- Core Settings ---\n    API_KEY: str\n\n    # --- Optional Betfair Credentials ---\n    BETFAIR_APP_KEY: Optional[str] = None\n    BETFAIR_USERNAME: Optional[str] = None\n    BETFAIR_PASSWORD: Optional[str] = None\n\n    # --- Caching & Performance ---\n    REDIS_URL: str = \"redis://localhost:6379\"\n    CACHE_TTL_SECONDS: int = 1800  # 30 minutes\n    MAX_CONCURRENT_REQUESTS: int = 10\n    HTTP_POOL_CONNECTIONS: int = 100\n    HTTP_POOL_MAXSIZE: int = 100\n    HTTP_MAX_KEEPALIVE: int = 50\n    DEFAULT_TIMEOUT: int = 30\n    ADAPTER_TIMEOUT: int = 20\n\n    # --- Logging ---\n    LOG_LEVEL: str = \"INFO\"\n\n    # --- Optional Adapter Keys ---\n    TVG_API_KEY: Optional[str] = None\n    RACING_AND_SPORTS_TOKEN: Optional[str] = None\n    POINTSBET_API_KEY: Optional[str] = None\n    GREYHOUND_API_URL: Optional[str] = None\n    THE_RACING_API_KEY: Optional[str] = None\n\n    # --- CORS Configuration ---\n    ALLOWED_ORIGINS: List[str] = [\"http://localhost:3000\", \"http://localhost:3001\"]\n\n    model_config = {\"env_file\": \".env\", \"case_sensitive\": True}\n\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"Returns a cached instance of the application settings.\"\"\"\n    return Settings()\n",
    "python_service/models.py": "# python_service/models.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    class Config:\n        populate_by_name = True\n        arbitrary_types_allowed = True\n\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[Decimal] = None\n    place: Optional[Decimal] = None\n    show: Optional[Decimal] = None\n    source: str\n    last_updated: datetime\n\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias=\"saddleClothNumber\")\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias=\"raceNumber\")\n    start_time: datetime = Field(..., alias=\"startTime\")\n    runners: List[Runner]\n    source: str\n    qualification_score: Optional[float] = Field(None, alias=\"qualificationScore\")\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias=\"racesFetched\")\n    fetch_duration: float = Field(..., alias=\"fetchDuration\")\n    error_message: Optional[str] = Field(None, alias=\"errorMessage\")\n\n\nclass AggregatedResponse(FortunaBaseModel):\n    races: List[Race]\n    source_info: List[SourceInfo] = Field(..., alias=\"sourceInfo\")\n\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias=\"raceId\")\n    track_name: str = Field(..., alias=\"trackName\")\n    race_number: int = Field(..., alias=\"raceNumber\")\n    post_time: str = Field(..., alias=\"postTime\")\n    score: float\n    factors: Any  # JSON string stored as Any\n",
    "python_service/minimal_service.py": "# python_service/minimal_service.py\n# This is the minimal, sanctioned Flask backend for Checkmate Solo.\n\nimport random\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom flask import Flask\nfrom flask import jsonify\nfrom flask_cors import CORS\n\napp = Flask(__name__)\n# This enables CORS for all domains on all routes.\n# For a production environment, you would want to restrict this\n# to the domain of your frontend application.\nCORS(app)\n\n\ndef generate_mock_race(race_id: int):\n    \"\"\"Generates a single mock race with randomized data, mirroring the frontend's mock generator.\"\"\"\n    tracks = [\"Belmont Park\", \"Churchill Downs\", \"Santa Anita\", \"Keeneland\", \"Del Mar\"]\n    horses = [\n        \"Thunder Strike\",\n        \"Lightning Bolt\",\n        \"Swift Arrow\",\n        \"Golden Dream\",\n        \"Storm Chaser\",\n        \"Midnight Runner\",\n        \"Royal Flash\",\n        \"Desert Wind\",\n    ]\n\n    race_horses = []\n    for i in range(8):\n        betfair = round(2 + random.random() * 15, 2)\n        pointsbet = round(betfair * (0.9 + random.random() * 0.2), 2)\n        tvg = round(betfair * (0.85 + random.random() * 0.3), 2)\n\n        odds_values = {\"Betfair\": betfair, \"PointsBet\": pointsbet, \"TVG\": tvg}\n        best_source = min(odds_values, key=odds_values.get)\n        best_odds = odds_values[best_source]\n\n        avg_odds = (betfair + pointsbet + tvg) / 3\n        value_score = ((avg_odds - best_odds) / best_odds) * 100 if best_odds > 0 else 0\n\n        race_horses.append(\n            {\n                \"number\": i + 1,\n                \"name\": random.choice(horses),\n                \"odds\": {\n                    \"betfair\": f\"{betfair:.2f}\",\n                    \"pointsbet\": f\"{pointsbet:.2f}\",\n                    \"tvg\": f\"{tvg:.2f}\",\n                    \"best\": f\"{best_odds:.2f}\",\n                    \"best_source\": best_source,\n                },\n                \"value_score\": f\"{value_score:.1f}\",\n                \"trend\": random.choice([\"up\", \"down\"]),\n            }\n        )\n\n    race_horses.sort(key=lambda x: float(x[\"value_score\"]), reverse=True)\n\n    return {\n        \"id\": race_id,\n        \"track\": random.choice(tracks),\n        \"race_number\": random.randint(1, 10),\n        \"post_time\": (datetime.now() + timedelta(minutes=random.randint(5, 120))).isoformat(),\n        \"horses\": race_horses,\n    }\n\n\n@app.route(\"/api/races/live\", methods=[\"GET\"])\ndef get_live_races():\n    \"\"\"\n    This endpoint provides a list of live mock race data for the frontend.\n    It mimics the data structure the CheckmateSolo component expects.\n    \"\"\"\n    mock_races = [generate_mock_race(i) for i in range(5)]\n    return jsonify(mock_races)\n\n\nif __name__ == \"__main__\":\n    # The frontend component's TODO comment specifies port 8000.\n    print(\"Starting Checkmate Solo minimal backend service...\")\n    print(\"API available at http://localhost:8000/api/races/live\")\n    app.run(host=\"0.0.0.0\", port=8000, debug=False)\n",
    "python_service/logging_config.py": "# python_service/logging_config.py\nimport logging\nimport sys\n\nimport structlog\n\ndef configure_logging(log_level: str = \"INFO\"):\n    \"\"\"Configures structlog for structured, JSON-formatted logging.\"\"\"\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(message)s\",\n        stream=sys.stdout,\n    )\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.add_log_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer()\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        wrapper_class=structlog.stdlib.BoundLogger,\n        cache_logger_on_first_use=True,\n    )\n",
    "python_service/__init__.py": "# This file makes the python_service directory a Python package.\n",
    "python_service/models_v3.py": "# python_service/models_v3.py\n# Defines the data structures for the V3 adapter architecture.\n\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom typing import List\n\n\n@dataclass\nclass NormalizedRunner:\n    runner_id: str\n    name: str\n    saddle_cloth: str\n    odds_decimal: float\n\n\n@dataclass\nclass NormalizedRace:\n    race_key: str\n    track_key: str\n    start_time_iso: str\n    race_name: str\n    runners: List[NormalizedRunner] = field(default_factory=list)\n    source_ids: List[str] = field(default_factory=list)\n",
    "python_service/checkmate_service.py": "# checkmate_service.py\n# The main service runner, upgraded to the final Endgame architecture.\n\nimport json\nimport logging\nimport os\nimport sqlite3\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom .engine import EnhancedTrifectaAnalyzer\nfrom .engine import Race\nfrom .engine import Settings\nfrom .engine import SuperchargedOrchestrator\n\n\nclass DatabaseHandler:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._setup_database()\n\n    def _get_connection(self):\n        return sqlite3.connect(self.db_path, timeout=10)\n\n    def _setup_database(self):\n        try:\n            # Correctly resolve paths from the service's location\n            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n            schema_path = os.path.join(base_dir, \"shared_database\", \"schema.sql\")\n            web_schema_path = os.path.join(base_dir, \"shared_database\", \"web_schema.sql\")\n\n            # Read both schema files\n            with open(schema_path, \"r\") as f:\n                schema = f.read()\n            with open(web_schema_path, \"r\") as f:\n                web_schema = f.read()\n\n            # Apply both schemas in a single transaction\n            with self._get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.executescript(schema)\n                cursor.executescript(web_schema)\n                conn.commit()\n            self.logger.info(\"CRITICAL SUCCESS: All database schemas (base + web) applied successfully.\")\n        except Exception as e:\n            self.logger.critical(f\"FATAL: Database setup failed. Other platforms will fail. Error: {e}\", exc_info=True)\n            raise\n\n    def update_races_and_status(self, races: List[Race], statuses: List[dict]):\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            for race in races:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO live_races (\n                        race_id, track_name, race_number, post_time, raw_data_json,\n                        checkmate_score, qualified, trifecta_factors_json, updated_at\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        race.race_id,\n                        race.track_name,\n                        race.race_number,\n                        race.post_time,\n                        race.model_dump_json(),\n                        race.checkmate_score,\n                        race.is_qualified,\n                        race.trifecta_factors_json,\n                        datetime.now(),\n                    ),\n                )\n            for status in statuses:\n                cursor.execute(\n                    \"\"\"\n                    INSERT OR REPLACE INTO adapter_status (\n                        adapter_name, status, last_run, races_found, error_message,\n                        execution_time_ms\n                    )\n                    VALUES (?, ?, ?, ?, ?, ?)\n                \"\"\",\n                    (\n                        status.get(\"adapter_id\"),\n                        status.get(\"status\"),\n                        status.get(\"timestamp\"),\n                        status.get(\"races_found\"),\n                        status.get(\"error_message\"),\n                        int(status.get(\"response_time\", 0) * 1000),\n                    ),\n                )\n\n            if races or statuses:\n                cursor.execute(\n                    \"INSERT INTO events (event_type, payload) VALUES (?, ?)\",\n                    (\"RACES_UPDATED\", json.dumps({\"race_count\": len(races)})),\n                )\n\n            conn.commit()\n        self.logger.info(f\"Database updated with {len(races)} races and {len(statuses)} adapter statuses.\")\n\n\nclass CheckmateBackgroundService:\n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        from dotenv import load_dotenv\n\n        dotenv_path = os.path.join(os.path.dirname(__file__), \"..\", \".env\")\n        load_dotenv(dotenv_path=dotenv_path)\n\n        db_path = os.getenv(\"CHECKMATE_DB_PATH\")\n        if not db_path:\n            self.logger.critical(\"FATAL: CHECKMATE_DB_PATH environment variable not set. Service cannot start.\")\n            raise ValueError(\"CHECKMATE_DB_PATH is not configured.\")\n\n        self.logger.info(f\"Database path loaded from environment: {db_path}\")\n\n        self.settings = Settings()\n        self.db_handler = DatabaseHandler(db_path)\n        self.orchestrator = SuperchargedOrchestrator(self.settings)\n        self.python_analyzer = EnhancedTrifectaAnalyzer(self.settings)\n        self.stop_event = threading.Event()\n        self.rust_engine_path = os.path.join(\n            os.path.dirname(__file__), \"..\", \"rust_engine\", \"target\", \"release\", \"checkmate_engine.exe\"\n        )\n\n    def _analyze_with_rust(self, races: List[Race]) -> Optional[List[Race]]:\n        self.logger.info(\"Attempting analysis with external Rust engine.\")\n        try:\n            race_data_json = json.dumps([r.model_dump() for r in races])\n            result = subprocess.run(\n                [self.rust_engine_path], input=race_data_json, capture_output=True, text=True, check=True, timeout=30\n            )\n            results_data = json.loads(result.stdout)\n            results_map = {res[\"race_id\"]: res for res in results_data}\n\n            for race in races:\n                if race.race_id in results_map:\n                    res = results_map[race.race_id]\n                    race.checkmate_score = res.get(\"checkmate_score\")\n                    race.is_qualified = res.get(\"qualified\")\n                    race.trifecta_factors_json = json.dumps(res.get(\"trifecta_factors\"))\n            return races\n        except FileNotFoundError:\n            self.logger.warning(\"Rust engine not found. Falling back to Python analyzer.\")\n            return None\n        except (subprocess.CalledProcessError, json.JSONDecodeError, subprocess.TimeoutExpired) as e:\n            self.logger.error(f\"Rust engine execution failed: {e}. Falling back to Python analyzer.\")\n            return None\n\n    def _analyze_with_python(self, races: List[Race]) -> List[Race]:\n        self.logger.info(\"Performing analysis with internal Python engine.\")\n        return [self.python_analyzer.analyze_race_advanced(race) for race in races]\n\n    def run_continuously(self, interval_seconds: int = 60):\n        self.logger.info(\"Background service thread starting continuous run.\")\n\n        while not self.stop_event.is_set():\n            try:\n                self.logger.info(\"Starting data collection and analysis cycle.\")\n                races, statuses = self.orchestrator.get_races_parallel()\n\n                analyzed_races = None\n                if os.path.exists(self.rust_engine_path):\n                    analyzed_races = self._analyze_with_rust(races)\n\n                if analyzed_races is None:  # Fallback condition\n                    analyzed_races = self._analyze_with_python(races)\n\n                if analyzed_races:  # Ensure we have something to update\n                    self.db_handler.update_races_and_status(analyzed_races, statuses)\n\n            except Exception as e:\n                self.logger.critical(f\"Unhandled exception in service loop: {e}\", exc_info=True)\n\n            self.logger.info(f\"Cycle complete. Sleeping for {interval_seconds} seconds.\")\n            self.stop_event.wait(interval_seconds)\n        self.logger.info(\"Background service run loop has terminated.\")\n\n    def start(self):\n        self.stop_event.clear()\n        self.thread = threading.Thread(target=self.run_continuously)\n        self.thread.daemon = True\n        self.thread.start()\n        self.logger.info(\"CheckmateBackgroundService started.\")\n\n    def stop(self):\n        self.stop_event.set()\n        if hasattr(self, \"thread\") and self.thread.is_alive():\n            self.thread.join(timeout=10)\n        self.logger.info(\"CheckmateBackgroundService stopped.\")\n",
    "python_service/windows_service_wrapper.py": "# windows_service_wrapper.py\n\nimport logging\nimport os\nimport sys\n\nimport servicemanager\nimport win32event\nimport win32service\nimport win32serviceutil\n\n# Add the service's directory to the Python path\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom checkmate_service import CheckmateBackgroundService\n\n\nclass CheckmateWindowsService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"CheckmateV8Service\"\n    _svc_display_name_ = \"Checkmate V8 Racing Analysis Service\"\n    _svc_description_ = \"Continuously fetches and analyzes horse racing data.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.checkmate_service = CheckmateBackgroundService()\n        # Configure logging to use the Windows Event Log\n        logging.basicConfig(\n            level=logging.INFO, format=\"%(name)s - %(levelname)s - %(message)s\", handlers=[servicemanager.LogHandler()]\n        )\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        self.checkmate_service.stop()\n        win32event.SetEvent(self.hWaitStop)\n        self.ReportServiceStatus(win32service.SERVICE_STOPPED)\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(\n            servicemanager.EVENTLOG_INFORMATION_TYPE, servicemanager.PYS_SERVICE_STARTED, (self._svc_name_, \"\")\n        )\n        self.main()\n\n    def main(self):\n        self.checkmate_service.start()\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(CheckmateWindowsService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(CheckmateWindowsService)\n",
    "python_service/cache_manager.py": "# python_service/cache_manager.py\nimport hashlib\nimport json\nimport os\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom functools import wraps\nfrom typing import Any\nfrom typing import Callable\n\nimport structlog\n\ntry:\n    import redis\n\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n\nlog = structlog.get_logger(__name__)\n\n\nclass CacheManager:\n    def __init__(self, redis_url: str = None):\n        self.redis_client = None\n        self.memory_cache = {}\n        if REDIS_AVAILABLE and redis_url:\n            try:\n                self.redis_client = redis.from_url(redis_url, decode_responses=True)\n                log.info(\"Redis cache connected successfully.\")\n            except Exception as e:\n                log.warning(f\"Failed to connect to Redis: {e}. Falling back to in-memory cache.\")\n\n    def _generate_key(self, prefix: str, *args, **kwargs) -> str:\n        key_data = f\"{prefix}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    def get(self, key: str) -> Any | None:\n        if self.redis_client:\n            try:\n                value = self.redis_client.get(key)\n                return json.loads(value) if value else None\n            except Exception as e:\n                log.warning(f\"Redis GET failed: {e}\")\n\n        entry = self.memory_cache.get(key)\n        if entry and entry[\"expires_at\"] > datetime.now():\n            return entry[\"value\"]\n        return None\n\n    def set(self, key: str, value: Any, ttl_seconds: int = 300):\n        serialized = json.dumps(value, default=str)\n        if self.redis_client:\n            try:\n                self.redis_client.setex(key, ttl_seconds, serialized)\n                return\n            except Exception as e:\n                log.warning(f\"Redis SET failed: {e}\")\n\n        self.memory_cache[key] = {\"value\": value, \"expires_at\": datetime.now() + timedelta(seconds=ttl_seconds)}\n\n\n# --- Singleton Instance & Decorator ---\ncache_manager = CacheManager(redis_url=os.getenv(\"REDIS_URL\"))\n\n\ndef cache_async_result(ttl_seconds: int = 300, key_prefix: str = \"cache\"):\n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            instance_args = args[1:] if args and hasattr(args[0], func.__name__) else args\n            cache_key = cache_manager._generate_key(f\"{key_prefix}:{func.__name__}\", *instance_args, **kwargs)\n\n            cached_result = cache_manager.get(cache_key)\n            if cached_result is not None:\n                log.debug(\"Cache hit\", function=func.__name__)\n                return cached_result\n\n            log.debug(\"Cache miss\", function=func.__name__)\n            result = await func(*args, **kwargs)\n            cache_manager.set(cache_key, result, ttl_seconds)\n            return result\n\n        return wrapper\n\n    return decorator\n",
    "python_service/requirements.txt": "# Fortuna Faucet - Python Dependencies (Windows Optimized v2)\n# This is the 'golden' manifest, combining the blueprint's pinned versions with all feature requirements.\n\n# --- Core Backend (FastAPI & Async) ---\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\npydantic==2.5.0\npydantic-settings==2.1.0\n\n# --- HTTP & Web Scraping ---\nhttpx==0.25.1\ntenacity==8.2.3\nrequests\n\n# --- HTML & Data Parsing ---\nselectolax\n\n# --- Logging & Configuration ---\nstructlog==23.2.0\npython-dotenv==1.0.0\n\n# --- Caching ---\nredis==5.0.1\nslowapi==0.1.9\n\n# --- Database & ETL ---\nSQLAlchemy\npsycopg2-binary\n\n# --- Monitoring & GUI ---\nmatplotlib==3.8.2\n\n# --- UI & Visualization (for utility scripts) ---\nrich\nstreamlit\npikepdf\ntabula-py\n\n# --- Windows Specific (DO NOT REMOVE MARKERS) ---\npsutil==5.9.6; sys_platform == 'win32'\npywin32==306; sys_platform == 'win32'\nwin10toast-click==1.0.1; sys_platform == 'win32'\n\n# --- Testing ---\npytest==7.4.3\npytest-asyncio==0.21.1\n\n# --- Development ---\nblack==23.11.0\n",
    "python_service/api.py": "# python_service/api.py\n\nfrom contextlib import asynccontextmanager\nfrom .logging_config import configure_logging\nfrom datetime import date\nfrom datetime import datetime\nfrom typing import List\nfrom typing import Optional\n\nimport aiosqlite\nimport structlog\nfrom fastapi import Depends\nfrom fastapi import FastAPI\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom fastapi import Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom slowapi import Limiter\nfrom slowapi import _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom .middleware.error_handler import ErrorHandlingMiddleware\nfrom slowapi.util import get_remote_address\n\n\nfrom .analyzer import AnalyzerEngine\nfrom .config import get_settings\nfrom .engine import FortunaEngine\nfrom .health import router as health_router\nfrom .logging_config import configure_logging\nfrom .models import AggregatedResponse\nfrom .models import QualifiedRacesResponse\nfrom .models import TipsheetRace\nfrom .security import verify_api_key\n\nlog = structlog.get_logger()\n\n\n# Define the lifespan context manager for robust startup/shutdown\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    configure_logging()\n    \"\"\"\n    Manage the application's lifespan. On startup, it initializes the OddsEngine\n    with validated settings and attaches it to the app state. On shutdown, it\n    properly closes the engine's resources.\n    \"\"\"\n    settings = get_settings()\n    app.state.engine = FortunaEngine(config=settings)\n    app.state.analyzer_engine = AnalyzerEngine()\n    log.info(\"Server startup: Configuration validated and FortunaEngine initialized.\")\n    yield\n    # Clean up the engine resources\n    await app.state.engine.close()\n    log.info(\"Server shutdown: HTTP client resources closed.\")\n\n\nlimiter = Limiter(key_func=get_remote_address)\n\n# Pass the lifespan manager to the FastAPI app\napp = FastAPI(title=\"Checkmate Ultimate Solo API\", version=\"2.1\", lifespan=lifespan)\n\n# Add the new error handling middleware FIRST, to catch exceptions from all other middleware\napp.add_middleware(ErrorHandlingMiddleware)\napp.add_middleware(SlowAPIMiddleware)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\nsettings = get_settings()\n\n# Add middlewares (order can be important)\napp.include_router(health_router)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"GET\"],\n    allow_headers=[\"*\"],\n)\n\n\n# Dependency function to get the engine instance from the app state\ndef get_engine(request: Request) -> FortunaEngine:\n    return request.app.state.engine\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(\n    request: Request, engine: FortunaEngine = Depends(get_engine), _=Depends(verify_api_key)\n):\n    \"\"\"Provides a list of health statuses for all adapters, required by the new frontend blueprint.\"\"\"\n    try:\n        statuses = engine.get_all_adapter_statuses()\n        return statuses\n    except Exception:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\n    \"/api/races/qualified/{analyzer_name}\",\n    response_model=QualifiedRacesResponse,\n    description=(\n        \"Fetch and analyze races from all configured data sources, returning a list of races \"\n        \"that meet the specified analyzer's criteria.\"\n    ),\n    responses={\n        200: {\n            \"description\": \"A list of qualified races with their scores.\",\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\n                        \"races\": [\n                            {\n                                \"id\": \"12345_2025-10-14_1\",\n                                \"venue\": \"Santa Anita\",\n                                \"race_number\": 1,\n                                \"start_time\": \"2025-10-14T20:30:00Z\",\n                                \"runners\": [{\"number\": 1, \"name\": \"Speedy Gonzalez\", \"odds\": \"5/2\"}],\n                                \"source\": \"TVG\",\n                                \"qualification_score\": 95.5,\n                            }\n                        ],\n                        \"analyzer\": \"trifecta_analyzer\",\n                    }\n                }\n            },\n        },\n        404: {\"description\": \"The specified analyzer was not found.\"},\n    },\n)\n@limiter.limit(\"30/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[date] = None,\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    # --- Dynamic Analyzer Parameters ---\n    max_field_size: Optional[int] = Query(None, description=\"Override the max field size for the analyzer.\"),\n    min_favorite_odds: Optional[float] = Query(None, description=\"Override the min favorite odds.\"),\n    min_second_favorite_odds: Optional[float] = Query(None, description=\"Override the min second favorite odds.\"),\n):\n    \"\"\"\n    Gets all races for a given date, filters them for qualified betting\n    opportunities, and returns the qualified races.\n    \"\"\"\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime(\"%Y-%m-%d\")\n        background_tasks = set()  # Dummy background tasks\n        aggregated_data = await engine.get_races(date_str, background_tasks)\n\n        races = aggregated_data.get(\"races\", [])\n\n        analyzer_engine = request.app.state.analyzer_engine\n        analyzer_params = {\n            \"max_field_size\": max_field_size,\n            \"min_favorite_odds\": min_favorite_odds,\n            \"min_second_favorite_odds\": min_second_favorite_odds,\n        }\n        custom_params = {k: v for k, v in analyzer_params.items() if v is not None}\n\n        analyzer = analyzer_engine.get_analyzer(analyzer_name, **custom_params)\n        result = analyzer.qualify_races(races)\n        return QualifiedRacesResponse(**result)\n    except ValueError as e:\n        log.warning(\"Requested analyzer not found\", analyzer_name=analyzer_name)\n        raise HTTPException(status_code=404, detail=str(e))\n    except Exception as e:\n        log.error(\"Error in /api/races/qualified\", error=str(e), exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[date] = None,\n    source: Optional[str] = None,\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n):\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime(\"%Y-%m-%d\")\n        background_tasks = set()  # Dummy background tasks\n        aggregated_data = await engine.get_races(date_str, background_tasks, source)\n        return aggregated_data\n    except Exception:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\nDB_PATH = \"fortuna.db\"\n\n\ndef get_current_date() -> date:\n    return datetime.now().date()\n\n\n@app.get(\"/api/tipsheet\", response_model=List[TipsheetRace])\n@limiter.limit(\"30/minute\")\nasync def get_tipsheet_endpoint(request: Request, date: date = Depends(get_current_date)):\n    \"\"\"Fetches the generated tipsheet from the database asynchronously.\"\"\"\n    results = []\n    try:\n        async with aiosqlite.connect(DB_PATH) as db:\n            db.row_factory = aiosqlite.Row\n            query = \"SELECT * FROM tipsheet WHERE date(post_time) = ? ORDER BY post_time ASC\"\n            async with db.execute(query, (date.isoformat(),)) as cursor:\n                async for row in cursor:\n                    results.append(dict(row))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n    return results\n",
    "python_service/analyzer.py": "from abc import ABC\nfrom abc import abstractmethod\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\n\nimport structlog\n\nfrom python_service.models import Race\nfrom python_service.models import Runner\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept ImportError:\n    winsound = None\ntry:\n    from win10toast_py3 import ToastNotifier\nexcept (ImportError, RuntimeError):\n    # Fails gracefully on non-Windows systems\n    ToastNotifier = None\n\nlog = structlog.get_logger(__name__)\n\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    # Filter out invalid or placeholder odds (e.g., > 999)\n    valid_odds = [o.win for o in runner.odds.values() if o.win is not None and o.win > 0 and o.win < 999]\n\n    if not valid_odds:\n        return None\n\n    return min(valid_odds)\n\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(self, max_field_size: int = 10, min_favorite_odds: float = 2.5, min_second_favorite_odds: float = 4.0):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        scored_races = []\n        for race in races:\n            # The _evaluate_race method now always returns a float score.\n            race.qualification_score = self._evaluate_race(race)\n            scored_races.append(race)\n\n        scored_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds),\n        }\n\n        log.info(\"Universal scoring complete\", total_races_scored=len(scored_races), criteria=criteria)\n\n        for race in scored_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": scored_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply a penalty if hard filters are not met, instead of returning None ---\n        if (\n            len(active_runners) > self.max_field_size\n            or favorite_odds < self.min_favorite_odds\n            or second_favorite_odds < self.min_second_favorite_odds\n        ):\n            # Assign a score of 0 to races that would have been filtered out\n            return 0.0\n\n        return round(final_score * 100, 2)\n\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer(\"trifecta\", TrifectaAnalyzer)\n        log.info(\"AnalyzerEngine discovered plugins\", available_analyzers=list(self.analyzers.keys()))\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)\n\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n\n    def __init__(self):\n        self.sounds = {\n            \"high_value\": Path(__file__).parent.parent.parent / \"assets\" / \"sounds\" / \"alert_premium.wav\",\n        }\n\n    def play(self, sound_type: str):\n        if not winsound:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\n\nclass RaceNotifier:\n    \"\"\"Handles sending native Windows notifications and audio alerts for high-value races.\"\"\"\n\n    def __init__(self):\n        self.toaster = ToastNotifier() if ToastNotifier else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n\n    def notify_qualified_race(self, race):\n        if not self.toaster or race.id in self.notified_races:\n            return\n\n        title = \"\ud83c\udfc7 High-Value Opportunity!\"\n        message = f\"\"\"{race.venue} - Race {race.race_number}\nScore: {race.qualification_score:.0f}%\nPost Time: {race.start_time.strftime(\"%I:%M %p\")}\"\"\"\n\n        try:\n            # The `threaded=True` argument is crucial to prevent blocking the main application thread.\n            self.toaster.show_toast(title, message, duration=10, threaded=True)\n            self.notified_races.add(race.id)\n            self.audio_system.play(\"high_value\")\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            # Catch potential exceptions from the notification library itself\n            log.error(\"Failed to send notification\", error=str(e), exc_info=True)\n",
    "python_service/health.py": "# python_service/health.py\nfrom datetime import datetime\nfrom typing import Dict\nfrom typing import List\n\nimport psutil\nimport structlog\nfrom fastapi import APIRouter\n\nrouter = APIRouter()\nlog = structlog.get_logger(__name__)\n\n\nclass HealthMonitor:\n    def __init__(self):\n        self.adapter_health: Dict[str, Dict] = {}\n        self.system_metrics: List[Dict] = []\n        self.max_metrics_history = 100\n\n    def record_adapter_response(self, adapter_name: str, success: bool, duration: float):\n        if adapter_name not in self.adapter_health:\n            self.adapter_health[adapter_name] = {\n                \"total_requests\": 0,\n                \"successful_requests\": 0,\n                \"failed_requests\": 0,\n                \"avg_response_time\": 0.0,\n                \"last_success\": None,\n                \"last_failure\": None,\n            }\n\n        health = self.adapter_health[adapter_name]\n        health[\"total_requests\"] += 1\n\n        if success:\n            health[\"successful_requests\"] += 1\n            health[\"last_success\"] = datetime.now().isoformat()\n        else:\n            health[\"failed_requests\"] += 1\n            health[\"last_failure\"] = datetime.now().isoformat()\n\n        health[\"avg_response_time\"] = (\n            health[\"avg_response_time\"] * (health[\"total_requests\"] - 1) + duration\n        ) / health[\"total_requests\"]\n\n    def get_system_metrics(self) -> Dict:\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory = psutil.virtual_memory()\n        disk = psutil.disk_usage(\"/\")\n\n        metrics = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"cpu_percent\": cpu_percent,\n            \"memory_percent\": memory.percent,\n            \"memory_available_gb\": round(memory.available / (1024**3), 2),\n            \"disk_percent\": disk.percent,\n            \"disk_free_gb\": round(disk.free / (1024**3), 2),\n        }\n\n        self.system_metrics.append(metrics)\n        if len(self.system_metrics) > self.max_metrics_history:\n            self.system_metrics.pop(0)\n\n        return metrics\n\n    def get_health_report(self) -> Dict:\n        system_metrics = self.get_system_metrics()\n        return {\n            \"status\": \"healthy\" if self.is_system_healthy() else \"degraded\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"system\": system_metrics,\n            \"adapters\": self.adapter_health,\n            \"metrics_history\": self.system_metrics[-10:],\n        }\n\n    def is_system_healthy(self) -> bool:\n        if not self.system_metrics:\n            return True\n        latest = self.system_metrics[-1]\n        return latest[\"cpu_percent\"] < 80 and latest[\"memory_percent\"] < 85 and latest[\"disk_percent\"] < 90\n\n\n# Global instance for the application to use\nhealth_monitor = HealthMonitor()\n\n\n@router.get(\"/health/detailed\", tags=[\"Health\"])\nasync def get_detailed_health():\n    \"\"\"Provides a comprehensive health check of the system.\"\"\"\n    return health_monitor.get_health_report()\n\n\n@router.get(\"/health\", tags=[\"Health\"])\nasync def get_basic_health():\n    \"\"\"Provides a basic health check for load balancers and uptime monitoring.\"\"\"\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n",
    "python_service/security.py": "# python_service/security.py\n\nimport secrets\n\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Security\nfrom fastapi import status\nfrom fastapi.security import APIKeyHeader\n\nfrom .config import Settings\nfrom .config import get_settings\n\nAPI_KEY_NAME = \"X-API-Key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)\n\n\nasync def verify_api_key(key: str = Security(api_key_header), settings: Settings = Depends(get_settings)):\n    \"\"\"\n    Verifies the provided API key against the one in settings using a\n    timing-attack resistant comparison.\n    \"\"\"\n    if secrets.compare_digest(key, settings.API_KEY):\n        return True\n    else:\n        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Invalid or missing API Key\")\n",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timezone\nfrom decimal import Decimal\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\nimport asyncio\nimport httpx\nimport structlog\n\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.base import BaseAdapter\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.equibase_adapter import EquibaseAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.racingpost_adapter import RacingPostAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .cache_manager import cache_async_result\nfrom .health import health_monitor\nfrom .models import AggregatedResponse\nfrom .models import OddsData\nfrom .models import Race\nfrom .models import Runner\nfrom .models_v3 import NormalizedRace\n\nlog = structlog.get_logger(__name__)\n\n\nclass FortunaEngine:\n    def __init__(self, config=None):\n        from .config import get_settings\n\n        self.config = config or get_settings()\n        self.logger = structlog.get_logger(__name__)\n        self.adapters: List[BaseAdapter] = [\n            BetfairAdapter(config=self.config),\n            BetfairGreyhoundAdapter(config=self.config),\n            RacingAndSportsAdapter(config=self.config),\n            RacingAndSportsGreyhoundAdapter(config=self.config),\n            AtTheRacesAdapter(config=self.config),\n            RacingPostAdapter(config=self.config),\n            HarnessAdapter(config=self.config),\n            EquibaseAdapter(config=self.config),\n            SportingLifeAdapter(config=self.config),\n            TimeformAdapter(config=self.config),\n            TheRacingApiAdapter(config=self.config),\n            GbgbApiAdapter(config=self.config),\n        ]\n        # V3 ADAPTERS\n        self.v3_adapters = [\n            BetfairDataScientistAdapter(\n                model_name=\"ThoroughbredModel\",\n                url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets?id=thoroughbred-model&date=\",\n            ),\n            TVGAdapter(config=self.config),\n        ]\n        self.http_limits = httpx.Limits(\n            max_connections=config.HTTP_POOL_CONNECTIONS, max_keepalive_connections=config.HTTP_MAX_KEEPALIVE\n        )\n        self.http_client = httpx.AsyncClient(limits=self.http_limits, http2=True)\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapter, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"Wraps an adapter's fetch call, catches all exceptions, and returns a consistent payload.\"\"\"\n        start_time = datetime.now()\n        try:\n            result = await adapter.fetch_races(date, self.http_client)\n            duration = (datetime.now() - start_time).total_seconds()\n            health_monitor.record_adapter_response(adapter.source_name, success=True, duration=duration)\n            return (adapter.source_name, result, duration)\n        except Exception as e:\n            duration = (datetime.now() - start_time).total_seconds()\n            log.error(\"Adapter raised an unhandled exception\", adapter=adapter.source_name, error=str(e), exc_info=True)\n            health_monitor.record_adapter_response(adapter.source_name, success=False, duration=duration)\n            failed_result = {\n                \"races\": [],\n                \"source_info\": {\n                    \"name\": adapter.source_name,\n                    \"status\": \"FAILED\",\n                    \"races_fetched\": 0,\n                    \"error_message\": str(e),\n                    \"fetch_duration\": duration,\n                },\n            }\n            return (adapter.source_name, failed_result, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races from multiple sources and reconciles odds.\"\"\"\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            # Use a robust key: venue, date, and race number\n            key = f\"{race.venue.upper()}-{race.start_time.strftime('%Y-%m-%d')}-{race.race_number}\"\n\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                # Merge runners and odds into the existing race object\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        # Runner exists, reconcile odds\n                        existing_runner = runner_map[new_runner.number]\n                        updated_odds = existing_runner.odds.copy()\n                        updated_odds.update(new_runner.odds)\n                        existing_runner.odds = updated_odds\n                    else:\n                        # New runner, add to the existing race\n                        existing_race.runners.append(new_runner)\n\n                # Update source count\n                existing_race.source += f\", {race.source}\"\n\n        return list(race_map.values())\n\n    async def get_races(self, date: str, background_tasks: set, source_filter: str = None) -> Dict[str, Any]:\n        if source_filter:\n            self.log.info(\"Bypassing cache for source-specific request\", source=source_filter)\n            return await self._fetch_races_from_sources(date, source_filter=source_filter)\n\n        return await self._get_all_races_cached(date, background_tasks=background_tasks)\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"fortuna_engine_races\")\n    async def _get_all_races_cached(self, date: str, background_tasks: set) -> Dict[str, Any]:\n        \"\"\"This method fetches races for all sources and its result is cached.\"\"\"\n        self.log.info(\"CACHE MISS: Fetching all races from sources.\", date=date)\n        return await self._fetch_races_from_sources(date)\n\n    def _convert_v3_race_to_v2(self, v3_race: NormalizedRace) -> Race:\n        \"\"\"Converts a V3 NormalizedRace object to a V2 Race object.\"\"\"\n        import re\n\n        race_number = 0\n        match = re.search(r\"\\d+\", v3_race.race_name)\n        if match:\n            race_number = int(match.group())\n\n        runners = []\n        for v3_runner in v3_race.runners:\n            odds_data = OddsData(\n                win=Decimal(str(v3_runner.odds_decimal)),\n                source=v3_race.source_ids[0],\n                last_updated=datetime.now(timezone.utc),\n            )\n            runner = Runner(\n                id=v3_runner.runner_id,\n                name=v3_runner.name,\n                number=int(v3_runner.saddle_cloth)\n                if v3_runner.saddle_cloth and v3_runner.saddle_cloth.isdigit()\n                else 99,\n                odds={v3_race.source_ids[0]: odds_data},\n            )\n            runners.append(runner)\n\n        return Race(\n            id=v3_race.race_key,\n            venue=v3_race.track_key,\n            race_number=race_number,\n            start_time=datetime.fromisoformat(v3_race.start_time_iso),\n            runners=runners,\n            source=v3_race.source_ids[0],\n            race_name=v3_race.race_name,\n        )\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"odds_engine_fetch\")\n    async def _fetch_races_from_sources(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"Helper method to contain the logic for fetching and aggregating races.\"\"\"\n        target_adapters = self.adapters\n        if source_filter:\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n\n        # Run V3 adapters\n        for adapter in self.v3_adapters:\n            if hasattr(adapter, 'fetch_and_normalize'):\n                # Handle synchronous V3 adapters\n                v3_task = asyncio.to_thread(adapter.fetch_and_normalize)\n                tasks.append(v3_task)\n            elif hasattr(adapter, 'get_races'):\n                # Handle asynchronous V3 adapters\n                v3_task = adapter.get_races(date)\n                tasks.append(v3_task)\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n\n        v3_start_time = datetime.now()  # Approximate start time for all V3 adapters\n\n        for result in results:\n            try:\n                if isinstance(result, Exception):\n                    self.log.error(\"Adapter fetch failed\", error=result, exc_info=False)\n                    continue\n\n                # Correctly differentiate between V2 and V3 results\n                if isinstance(result, tuple) and len(result) == 3:  # V2 Adapter Result\n                    adapter_name, adapter_result, duration = result\n                    source_info = adapter_result.get(\"source_info\", {})\n                    source_info[\"fetch_duration\"] = round(duration, 2)\n                    source_infos.append(source_info)\n                    if source_info.get(\"status\") == \"SUCCESS\":\n                        all_races.extend(adapter_result.get(\"races\", []))\n                elif isinstance(result, list) and all(\n                    isinstance(r, NormalizedRace) for r in result\n                ):  # V3 Adapter Result\n                    if result:\n                        v3_races = result\n                        adapter_name = v3_races[0].source_ids[0]\n                        translated_races = [self._translate_v3_race_to_v2(nr) for nr in result]\n                        all_races.extend(translated_races)\n\n                        v3_duration = (datetime.now() - v3_start_time).total_seconds()\n                        source_infos.append(\n                            {\n                                \"name\": adapter_name,\n                                \"status\": \"SUCCESS\",\n                                \"races_fetched\": len(translated_races),\n                                \"error_message\": None,\n                                \"fetch_duration\": round(v3_duration, 2),\n                            }\n                        )\n            except Exception:\n                self.log.error(\"Failed to process result from an adapter.\", exc_info=True)\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, \"%Y-%m-%d\").date(),\n            races=deduped_races,\n            sources=source_infos,\n            metadata={\n                \"fetch_time\": datetime.now(),\n                \"sources_queried\": [a.source_name for a in target_adapters],\n                \"sources_successful\": len([s for s in source_infos if s[\"status\"] == \"SUCCESS\"]),\n                \"total_races\": len(deduped_races),\n            },\n        )\n\n        # --- Add Success Notification ---\n        try:\n            from windows_toasts import Toast, WindowsToaster\n            toaster = WindowsToaster(\"Fortuna Faucet Data Refresh\")\n            new_toast = Toast()\n            new_toast.text_fields = [f\"Successfully fetched {len(deduped_races)} races from {len(source_infos)} sources.\"]\n            toaster.show_toast(new_toast)\n        except (ImportError, RuntimeError):\n            pass\n\n        return response_obj.model_dump()\n\n    def _translate_v3_race_to_v2(self, norm_race: NormalizedRace) -> Race:\n        \"\"\"Translates a V3 NormalizedRace into a V2 Race object.\"\"\"\n        import re\n\n        race_number = 0\n        match = re.search(r\"R(\\d+)\", norm_race.race_name)\n        if match:\n            race_number = int(match.group(1))\n\n        runners = []\n        for norm_runner in norm_race.runners:\n            adapter_name = norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\"\n            odds_data = OddsData(\n                win=Decimal(str(norm_runner.odds_decimal)), source=adapter_name, last_updated=datetime.now(timezone.utc)\n            )\n\n            try:\n                runner_number = int(norm_runner.saddle_cloth)\n            except (ValueError, TypeError):\n                runner_number = None\n\n            runner = Runner(\n                id=norm_runner.runner_id, name=norm_runner.name, number=runner_number, odds={adapter_name: odds_data}\n            )\n            runners.append(runner)\n\n        return Race(\n            id=norm_race.race_key,\n            venue=norm_race.track_key,\n            start_time=datetime.fromisoformat(norm_race.start_time_iso),\n            race_number=race_number,\n            runners=runners,\n            source=norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\",\n            # Store extra V3 data in metadata for future use\n            metadata={\"v3_race_name\": norm_race.race_name},\n        )\n",
    "python_service/utils/odds.py": "# Centralized odds parsing utility, created by Operation: The A+ Trifecta\nfrom decimal import Decimal\nfrom decimal import InvalidOperation\nfrom typing import Optional\nfrom typing import Union\n\n\ndef parse_odds_to_decimal(odds: Union[str, int, float, None]) -> Optional[Decimal]:\n    \"\"\"\n    Parse various odds formats to Decimal for precise financial calculations.\n    Handles fractional, decimal, and special cases ('EVS', 'SP', etc.).\n    Returns None for unparseable or invalid values.\n    \"\"\"\n    if odds is None:\n        return None\n\n    if isinstance(odds, (int, float)):\n        return Decimal(str(odds))\n\n    odds_str = str(odds).strip().upper()\n\n    SPECIAL_CASES = {\n        \"EVS\": Decimal(\"2.0\"),\n        \"EVENS\": Decimal(\"2.0\"),\n        \"SP\": None,\n        \"SCRATCHED\": None,\n        \"SCR\": None,\n        \"\": None,\n    }\n\n    if odds_str in SPECIAL_CASES:\n        return SPECIAL_CASES[odds_str]\n\n    if \"/\" in odds_str:\n        try:\n            parts = odds_str.split(\"/\")\n            if len(parts) != 2:\n                return None\n            num, den = map(Decimal, parts)\n            if den <= 0:\n                return None\n            return Decimal(\"1.0\") + (num / den)\n        except (ValueError, InvalidOperation):\n            return None\n\n    try:\n        return Decimal(odds_str)\n    except (ValueError, InvalidOperation):\n        return None\n",
    "python_service/utils/__init__.py": "",
    "python_service/utils/text.py": "# python_service/utils/text.py\n# Centralized text and name normalization utilities\nimport re\nfrom typing import Optional\n\n\ndef clean_text(text: Optional[str]) -> Optional[str]:\n    \"\"\"Strips leading/trailing whitespace and collapses internal whitespace.\"\"\"\n    if not text:\n        return None\n    return \" \".join(text.strip().split())\n\n\ndef normalize_venue_name(name: Optional[str]) -> Optional[str]:\n    \"\"\"\n    Normalizes a UK or Irish racecourse name to a standard format.\n    Handles common abbreviations and variations.\n    \"\"\"\n    if not name:\n        return None\n\n    # Use a temporary variable for matching, but return the properly cased name\n    cleaned_name_upper = clean_text(name).upper()\n\n    VENUE_MAP = {\n        \"ASCOT\": \"Ascot\",\n        \"AYR\": \"Ayr\",\n        \"BANGOR-ON-DEE\": \"Bangor-on-Dee\",\n        \"CATTERICK BRIDGE\": \"Catterick\",\n        \"CHELMSFORD CITY\": \"Chelmsford\",\n        \"EPSOM DOWNS\": \"Epsom\",\n        \"FONTWELL\": \"Fontwell Park\",\n        \"HAYDOCK\": \"Haydock Park\",\n        \"KEMPTON\": \"Kempton Park\",\n        \"LINGFIELD\": \"Lingfield Park\",\n        \"NEWMARKET (ROWLEY)\": \"Newmarket\",\n        \"NEWMARKET (JULY)\": \"Newmarket\",\n        \"SANDOWN\": \"Sandown Park\",\n        \"STRATFORD\": \"Stratford-on-Avon\",\n        \"YARMOUTH\": \"Great Yarmouth\",\n        \"CURRAGH\": \"Curragh\",\n        \"DOWN ROYAL\": \"Down Royal\",\n    }\n\n    # Check primary map first\n    if cleaned_name_upper in VENUE_MAP:\n        return VENUE_MAP[cleaned_name_upper]\n\n    # Handle cases where the key is the desired output but needs to be mapped from a variation\n    # e.g. CHELMSFORD maps to Chelmsford\n    # Title case the cleaned name for a sensible default\n    title_cased_name = clean_text(name).title()\n    if title_cased_name in VENUE_MAP.values():\n        return title_cased_name\n\n    # Return the title-cased cleaned name as a fallback\n    return title_cased_name\n\n\ndef normalize_course_name(name: str) -> str:\n    if not name:\n        return \"\"\n    name = name.lower().strip()\n    name = re.sub(r\"[^a-z0-9\\s-]\", \"\", name)\n    name = re.sub(r\"[\\s-]+\", \"_\", name)\n    return name\n",
    "python_service/middleware/__init__.py": "",
    "python_service/middleware/error_handler.py": "# python_service/middleware/error_handler.py\nimport structlog\nimport httpx\nfrom fastapi import Request, Response\nfrom fastapi.responses import JSONResponse\nfrom starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint\nfrom pydantic import ValidationError\nimport json\n\nfrom ..core.errors import ErrorCategory\n\nlogger = structlog.get_logger(__name__)\n\ndef _get_error_category(exc: Exception) -> ErrorCategory:\n    \"\"\"Maps an exception type to a defined ErrorCategory.\"\"\"\n    if isinstance(exc, httpx.RequestError):\n        return ErrorCategory.NETWORK_ERROR\n    if isinstance(exc, (ValidationError, json.JSONDecodeError)):\n        return ErrorCategory.PARSING_ERROR\n    # In a more complex system, we could check for specific config errors\n    # if isinstance(exc, ConfigNotFoundError): return ErrorCategory.CONFIGURATION_ERROR\n    return ErrorCategory.UNEXPECTED_ERROR\n\nclass ErrorHandlingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next: RequestResponseEndpoint) -> Response:\n        try:\n            return await call_next(request)\n        except Exception as exc:\n            category = _get_error_category(exc)\n\n            logger.error(\n                \"unhandled_exception_caught\",\n                error_category=category.name,\n                error_message=str(exc),\n                path=request.url.path,\n                method=request.method,\n                exc_info=True,\n            )\n\n            return JSONResponse(\n                status_code=500,\n                content={\n                    \"error\": {\n                        \"category\": category.name,\n                        \"message\": category.value,\n                        \"detail\": str(exc),\n                    }\n                },\n            )",
    "python_service/core/errors.py": "# python_service/core/errors.py\nfrom enum import Enum\n\nclass ErrorCategory(Enum):\n    CONFIGURATION_ERROR = \"Configuration missing or invalid\"\n    NETWORK_ERROR = \"HTTP/Network request failed\"\n    PARSING_ERROR = \"Data parsing or validation unsuccessful\"\n    UNEXPECTED_ERROR = \"An unhandled exception occurred\"",
    "python_service/core/__init__.py": "",
    "tests/test_engine.py": "import pytest\nfrom unittest.mock import AsyncMock, patch\nfrom datetime import datetime, date\nfrom decimal import Decimal\nimport fakeredis.aioredis\n\nfrom python_service.models import Race, Runner, OddsData\nfrom python_service.engine import OddsEngine\nfrom python_service.config import get_settings\nfrom python_service.adapters.base import BaseAdapter\n\ndef create_mock_race(source: str, venue: str, race_number: int, start_time: datetime, runners_data: list) -> Race:\n    \"\"\"Helper function to create a Race object for testing.\"\"\"\n    runners = []\n    for r_data in runners_data:\n        odds = {source: OddsData(win=Decimal(r_data[\"odds\"]), source=source, last_updated=datetime.now())}\n        runners.append(Runner(number=r_data[\"number\"], name=r_data[\"name\"], odds=odds))\n\n    return Race(\n        id=f\"test_{source}_{race_number}\",\n        venue=venue,\n        race_number=race_number,\n        start_time=start_time,\n        runners=runners,\n        source=source\n    )\n\n@pytest.fixture\ndef mock_engine() -> OddsEngine:\n    \"\"\"Provides an OddsEngine instance with a mock config.\"\"\"\n    return OddsEngine(config=get_settings())\n\n@pytest.mark.asyncio\n@patch('python_service.engine.OddsEngine._time_adapter_fetch', new_callable=AsyncMock)\nasync def test_engine_deduplicates_races_and_merges_odds(mock_time_adapter_fetch, mock_engine):\n    \"\"\"\n    SPEC: The OddsEngine's fetch_all_odds method should identify duplicate races\n    from different sources and merge their runner data, stacking the odds.\n    \"\"\"\n    # ARRANGE\n    test_time = datetime(2025, 10, 9, 14, 30)\n\n    source_a_race = create_mock_race(\"SourceA\", \"Test Park\", 1, test_time, [\n        {\"number\": 1, \"name\": \"Speedy\", \"odds\": \"5.0\"},\n        {\"number\": 2, \"name\": \"Steady\", \"odds\": \"10.0\"},\n    ])\n    source_b_race = create_mock_race(\"SourceB\", \"Test Park\", 1, test_time, [\n        {\"number\": 1, \"name\": \"Speedy\", \"odds\": \"5.5\"},\n        {\"number\": 3, \"name\": \"Newcomer\", \"odds\": \"15.0\"},\n    ])\n    other_race = create_mock_race(\"SourceC\", \"Another Place\", 2, test_time, [\n        {\"number\": 1, \"name\": \"Solo\", \"odds\": \"3.0\"}\n    ])\n\n    mock_time_adapter_fetch.side_effect = [\n        (\"SourceA\", {'races': [source_a_race], 'source_info': {'name': 'SourceA', 'status': 'SUCCESS', 'races_fetched': 1}}, 1.0),\n        (\"SourceB\", {'races': [source_b_race], 'source_info': {'name': 'SourceB', 'status': 'SUCCESS', 'races_fetched': 1}}, 1.0),\n        (\"SourceC\", {'races': [other_race], 'source_info': {'name': 'SourceC', 'status': 'SUCCESS', 'races_fetched': 1}}, 1.0),\n    ]\n\n    # ACT\n    today_str = date.today().strftime('%Y-%m-%d')\n    result = await mock_engine.fetch_all_odds(today_str)\n\n    # ASSERT\n    assert len(result['races']) == 2, \"Engine should have de-duplicated the races.\"\n\n    merged_race = next((r for r in result['races'] if r['venue'] == \"Test Park\"), None)\n    assert merged_race is not None, \"Merged race should be present in the results.\"\n    assert len(merged_race['runners']) == 3, \"Merged race should contain all unique runners.\"\n\n    runner1 = next((r for r in merged_race['runners'] if r['number'] == 1), None)\n    assert runner1 is not None\n    assert \"SourceA\" in runner1['odds']\n    assert \"SourceB\" in runner1['odds']\n    assert runner1['odds']['SourceA']['win'] == Decimal(\"5.0\")\n    assert runner1['odds']['SourceB']['win'] == Decimal(\"5.5\")\n\n    runner2 = next((r for r in merged_race['runners'] if r['number'] == 2), None)\n    assert runner2 is not None\n    assert \"SourceA\" in runner2['odds'] and \"SourceB\" not in runner2['odds']\n\n    runner3 = next((r for r in merged_race['runners'] if r['number'] == 3), None)\n    assert runner3 is not None\n    assert \"SourceB\" in runner3['odds'] and \"SourceA\" not in runner3['odds']\n\n\n@pytest.mark.asyncio\n@patch('python_service.engine.redis.from_url')\nasync def test_engine_caching_logic(mock_redis_from_url):\n    \"\"\"\n    SPEC: The OddsEngine should cache results in Redis.\n    1. On a cache miss, it should fetch from adapters and set the cache.\n    2. On a cache hit, it should return data from the cache without fetching from adapters.\n    \"\"\"\n    # ARRANGE\n    mock_redis_client = fakeredis.aioredis.FakeRedis(decode_responses=True)\n    mock_redis_from_url.return_value = mock_redis_client\n    await mock_redis_client.flushall()\n\n    engine = OddsEngine(config=get_settings())\n\n    today_str = date.today().strftime('%Y-%m-%d')\n    cache_key = f\"fortuna:races:{today_str}\"\n    test_time = datetime(2025, 10, 9, 15, 0)\n\n    mock_race = create_mock_race(\"TestSource\", \"Cache Park\", 1, test_time, [\n        {\"number\": 1, \"name\": \"Cachedy\", \"odds\": \"4.0\"}\n    ])\n\n    # Replace the engine's adapters with a single mock to isolate the test\n    mock_adapter = AsyncMock(spec=BaseAdapter)\n    mock_adapter.source_name = \"TestSource\"\n    mock_adapter.fetch_races.return_value = {\n        'races': [mock_race],\n        'source_info': {'name': 'TestSource', 'status': 'SUCCESS', 'races_fetched': 1}\n    }\n    engine.adapters = [mock_adapter]\n\n\n    # --- ACT 1: Cache Miss ---\n    result_miss = await engine.fetch_all_odds(today_str)\n\n    # --- ASSERT 1: Cache Miss ---\n    mock_adapter.fetch_races.assert_called_once()\n    cached_value = await mock_redis_client.get(cache_key)\n    assert cached_value is not None\n    assert len(result_miss['races']) == 1\n    assert result_miss['races'][0]['venue'] == \"Cache Park\"\n\n\n    # --- ACT 2: Cache Hit ---\n    mock_adapter.fetch_races.reset_mock()\n    result_hit = await engine.fetch_all_odds(today_str)\n\n    # --- ASSERT 2: Cache Hit ---\n    mock_adapter.fetch_races.assert_not_called()\n    assert len(result_hit['races']) == 1\n    assert result_hit['races'][0]['venue'] == \"Cache Park\"\n\n    assert result_hit['races'] == result_miss['races']\n    assert result_hit['sources'] == result_miss['sources']\n\n    await engine.close()",
    "tests/test_api.py": "# tests/test_api.py\nimport pytest\nimport aiosqlite\nfrom unittest.mock import patch, AsyncMock\nfrom datetime import datetime, date\nfrom decimal import Decimal\n\nfrom python_service.models import Race, Runner, OddsData, TipsheetRace\n\n# Note: The 'client' fixture is automatically available from tests/conftest.py\n\n@pytest.mark.asyncio\n@patch('python_service.engine.FortunaEngine.get_races', new_callable=AsyncMock)\nasync def test_get_races_endpoint_success(mock_get_races, client):\n    \"\"\"\n    SPEC: The /api/races endpoint should return data with a valid API key.\n    \"\"\"\n    # ARRANGE\n    today = date.today()\n    now = datetime.now()\n    mock_response_data = {\n        \"races\": [],\n        \"source_info\": []\n    }\n    mock_get_races.return_value = mock_response_data\n    headers = {\"X-API-Key\": \"test_api_key\"}\n\n    # ACT\n    response = client.get(f\"/api/races?date={today.isoformat()}\", headers=headers)\n\n    # ASSERT\n    assert response.status_code == 200\n    mock_get_races.assert_awaited_once()\n\nfrom fastapi.testclient import TestClient\n\n@pytest.mark.asyncio\nasync def test_get_tipsheet_endpoint_success(tmp_path):\n    \"\"\"\n    SPEC: The /api/tipsheet endpoint should return a list of tipsheet races from the database.\n    \"\"\"\n    db_path = tmp_path / \"test.db\"\n    post_time = datetime.now()\n\n    with patch('python_service.api.DB_PATH', db_path):\n        from python_service.api import app\n        with TestClient(app) as client:\n            async with aiosqlite.connect(db_path) as db:\n                await db.execute(\"\"\"\n                    CREATE TABLE tipsheet (\n                        race_id TEXT PRIMARY KEY,\n                        track_name TEXT,\n                        race_number INTEGER,\n                        post_time TEXT,\n                        score REAL,\n                        factors TEXT\n                    )\n                \"\"\")\n                await db.execute(\n                    \"INSERT INTO tipsheet VALUES (?, ?, ?, ?, ?, ?)\",\n                    (\"test_race_1\", \"Test Park\", 1, post_time.isoformat(), 85.5, \"{}\")\n                )\n                await db.commit()\n\n            # ACT\n            response = client.get(f\"/api/tipsheet?date={post_time.date().isoformat()}\")\n\n            # ASSERT\n            assert response.status_code == 200\n            response_data = response.json()\n            assert len(response_data) == 1\n            assert response_data[0][\"raceId\"] == \"test_race_1\"\n            assert response_data[0][\"score\"] == 85.5\n\n# --- Tests resurrected by Operation: The Great Resurrection ---\n\nfrom fastapi.testclient import TestClient\nfrom python_service.api import app\n\nclient = TestClient(app)\n\ndef test_health_check_unauthenticated():\n    \"\"\"Ensures the /health endpoint is accessible without an API key.\"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    json_response = response.json()\n    assert json_response[\"status\"] == \"ok\"\n    assert \"timestamp\" in json_response\n\ndef test_api_key_authentication_failure():\n    \"\"\"Ensures that endpoints are protected and fail with an invalid API key.\"\"\"\n    response = client.get(\"/api/races/qualified/trifecta\", headers={\"X-API-KEY\": \"invalid_key\"})\n    assert response.status_code == 403\n    assert response.json() == {\"detail\": \"Invalid or missing API Key\"}\n\ndef test_api_key_authentication_missing():\n    \"\"\"Ensures that endpoints are protected and fail with a missing API key.\"\"\"\n    response = client.get(\"/api/races/qualified/trifecta\")\n    assert response.status_code == 403\n    assert response.json() == {\"detail\": \"Not authenticated\"}\n",
    "tests/test_analyzer.py": "import pytest\nfrom decimal import Decimal\nfrom datetime import datetime\nfrom python_service.models import Race, Runner, OddsData\nfrom python_service.analyzer import AnalyzerEngine, TrifectaAnalyzer, _get_best_win_odds\n\n# Helper to create runners for tests\ndef create_runner(number, odds_val=None, scratched=False):\n    odds_data = {}\n    if odds_val:\n        odds_data[\"TestOdds\"] = OddsData(win=Decimal(str(odds_val)), source=\"TestOdds\", last_updated=datetime.now())\n    return Runner(number=number, name=f\"Runner {number}\", odds=odds_data, scratched=scratched)\n\n@pytest.fixture\ndef sample_races_for_true_trifecta():\n    \"\"\"Provides a list of sample Race objects for the new 'True Trifecta' logic.\"\"\"\n    return [\n        # Race 1: Should PASS all criteria, will have a lower score\n        Race(\n            id=\"race_pass_1\", venue=\"Test Park\", race_number=1, start_time=datetime.now(), source=\"Test\",\n            runners=[\n                create_runner(1, 3.0), # Favorite\n                create_runner(2, 4.5), # Second Favorite\n                create_runner(3, 5.0),\n            ]\n        ),\n        # Race 2: Should FAIL (Field size too large)\n        Race(\n            id=\"race_fail_field_size\", venue=\"Test Park\", race_number=2, start_time=datetime.now(), source=\"Test\",\n            runners=[create_runner(i, 5.0 + i) for i in range(1, 12)] # 11 runners\n        ),\n        # Race 3: Should FAIL (Favorite odds too low)\n        Race(\n            id=\"race_fail_fav_odds\", venue=\"Test Park\", race_number=3, start_time=datetime.now(), source=\"Test\",\n            runners=[create_runner(1, 2.0), create_runner(2, 4.5)]\n        ),\n        # Race 4: Should FAIL (Second favorite odds too low)\n        Race(\n            id=\"race_fail_2nd_fav_odds\", venue=\"Test Park\", race_number=4, start_time=datetime.now(), source=\"Test\",\n            runners=[create_runner(1, 3.0), create_runner(2, 3.5)]\n        ),\n        # Race 5: Should also PASS and have a higher score than race_pass_1\n        Race(\n            id=\"race_pass_2\", venue=\"Test Park\", race_number=5, start_time=datetime.now(), source=\"Test\",\n            runners=[\n                create_runner(1, 4.0), # Favorite\n                create_runner(2, 6.0), # Second Favorite\n                create_runner(3, 8.0),\n                create_runner(4, 12.0),\n                create_runner(5, 15.0),\n            ]\n        ),\n    ]\n\ndef test_analyzer_engine_discovery():\n    \"\"\"Tests that the AnalyzerEngine correctly discovers the TrifectaAnalyzer.\"\"\"\n    engine = AnalyzerEngine()\n    assert 'trifecta' in engine.analyzers\n    assert engine.analyzers['trifecta'] == TrifectaAnalyzer\n\ndef test_analyzer_engine_get_analyzer():\n    \"\"\"Tests that the AnalyzerEngine can instantiate a specific analyzer.\"\"\"\n    engine = AnalyzerEngine()\n    analyzer = engine.get_analyzer('trifecta', max_field_size=8)\n    assert isinstance(analyzer, TrifectaAnalyzer)\n    assert analyzer.max_field_size == 8\n\ndef test_analyzer_engine_get_nonexistent_analyzer():\n    \"\"\"Tests that requesting a non-existent analyzer raises a ValueError.\"\"\"\n    engine = AnalyzerEngine()\n    with pytest.raises(ValueError, match=\"Analyzer 'nonexistent' not found.\"):\n        engine.get_analyzer('nonexistent')\n\ndef test_trifecta_analyzer_plugin_logic(sample_races_for_true_trifecta):\n    \"\"\"\n    Tests the TrifectaAnalyzer's scoring, sorting, and new response structure.\n    \"\"\"\n    engine = AnalyzerEngine()\n    analyzer = engine.get_analyzer('trifecta')  # Use default criteria\n\n    result = analyzer.qualify_races(sample_races_for_true_trifecta)\n\n    # 1. Verify the new response structure\n    assert isinstance(result, dict)\n    assert \"criteria\" in result\n    assert \"races\" in result\n    assert result['criteria']['max_field_size'] == 10\n\n    qualified_races = result['races']\n\n    # 2. Check that the correct number of races were qualified\n    assert len(qualified_races) == 2\n\n    # 3. Check that the scores have been assigned and are valid numbers\n    assert qualified_races[0].qualification_score is not None\n    assert qualified_races[1].qualification_score is not None\n    assert isinstance(qualified_races[0].qualification_score, float)\n\n    # 4. Check that the races are sorted by score in descending order\n    assert qualified_races[0].qualification_score > qualified_races[1].qualification_score\n    assert qualified_races[0].id == \"race_pass_2\"  # This race should have the higher score\n    assert qualified_races[1].id == \"race_pass_1\"\n\ndef test_get_best_win_odds_helper():\n    \"\"\"Tests the helper function for finding the best odds.\"\"\"\n    runner_with_odds = create_runner(1)\n    runner_with_odds.odds = {\n        \"SourceA\": OddsData(win=Decimal(\"3.0\"), source=\"A\", last_updated=datetime.now()),\n        \"SourceB\": OddsData(win=Decimal(\"2.5\"), source=\"B\", last_updated=datetime.now()),\n    }\n    assert _get_best_win_odds(runner_with_odds) == Decimal(\"2.5\")\n\n    runner_no_odds = create_runner(2)\n    assert _get_best_win_odds(runner_no_odds) is None\n\n    runner_no_win = create_runner(3)\n    runner_no_win.odds = {\"SourceA\": OddsData(win=None, source=\"A\", last_updated=datetime.now())}\n    assert _get_best_win_odds(runner_no_win) is None\n\n# Test case added by Operation: Resurrect and Modernize\nfrom python_service.models import Race, Runner\nimport datetime\n\ndef test_trifecta_analyzer_rejects_races_with_too_few_runners(trifecta_analyzer):\n    \"\"\"Ensure analyzer rejects races with < 3 runners for a trifecta.\"\"\"\n    race_with_two_runners = Race(\n        id='test_race_123',\n        venue='TEST',\n        race_number=1,\n        start_time=datetime.datetime.now(),\n        runners=[\n            Runner(number=1, name='Horse A', odds='2/1', scratched=False),\n            Runner(number=2, name='Horse B', odds='3/1', scratched=False)\n        ],\n        source='test'\n    )\n\n    is_qualified = trifecta_analyzer.is_race_qualified(race_with_two_runners)\n    assert not is_qualified, 'Trifecta analyzer should not qualify a race with only two runners.'\n",
    "tests/conftest.py": "import pytest\nfrom fastapi.testclient import TestClient\nfrom unittest.mock import patch\nfrom python_service.config import Settings\n\n@pytest.fixture(autouse=True)\ndef override_settings_for_tests():\n    \"\"\"\n    Patches the Settings class for all tests to prevent loading .env files\n    and to provide a consistent, mock configuration. This runs automatically.\n    \"\"\"\n    class TestSettings(Settings):\n        class Config:\n            env_file = None\n\n    mock_settings = TestSettings(\n        BETFAIR_APP_KEY=\"test_key\",\n        BETFAIR_USERNAME=\"test_user\",\n        BETFAIR_PASSWORD=\"test_password\",\n        API_KEY=\"test_api_key\"\n    )\n    with patch('python_service.config.Settings', return_value=mock_settings):\n        yield\n\n@pytest.fixture\ndef client():\n    \"\"\"\n    Creates a TestClient for the API. The app is imported *inside* this\n    fixture to ensure the settings patch is active before initialization.\n    \"\"\"\n    from python_service.api import app\n    with TestClient(app) as c:\n        yield c",
    "tests/test_models.py": "# Test suite for Pydantic models, resurrected from attic/legacy_tests_pre_triage/checkmate_v7/test_models.py\nimport pytest\nfrom pydantic import ValidationError\nfrom python_service.models import Race, Runner\nimport datetime\n\ndef test_runner_model_creation():\n    \"\"\"Tests basic successful creation of the Runner model.\"\"\"\n    runner = Runner(number=5, name='Test Horse', odds='5/1', scratched=False)\n    assert runner.number == 5\n    assert runner.name == 'Test Horse'\n    assert not runner.scratched\n\ndef test_race_model_with_valid_runners():\n    \"\"\"Tests basic successful creation of the Race model.\"\"\"\n    runner1 = Runner(number=1, name='A', odds='2/1', scratched=False)\n    runner2 = Runner(number=2, name='B', odds='3/1', scratched=False)\n    race = Race(\n        id='test-race-1',\n        venue='TEST',\n        race_number=1,\n        start_time=datetime.datetime.now(),\n        runners=[runner1, runner2],\n        source='test'\n    )\n    assert race.venue == 'TEST'\n    assert len(race.runners) == 2\n\ndef test_model_validation_fails_on_missing_required_field():\n    \"\"\"Ensures Pydantic's validation fires for missing required fields.\"\"\"\n    with pytest.raises(ValidationError):\n        # 'name' is a required field for a Runner\n        Runner(number=3, odds='3/1', scratched=False)\n\n    with pytest.raises(ValidationError):\n        # 'venue' is a required field for a Race\n        Race(\n            id='test-race-2',\n            race_number=2,\n            start_time=datetime.datetime.now(),\n            runners=[],\n            source='test'\n        )",
    "tests/utils/test_odds.py": "# tests/utils/test_odds.py\nimport pytest\nfrom decimal import Decimal\nfrom python_service.utils.odds import parse_odds_to_decimal\n\n@pytest.mark.parametrize(\"input_odds, expected_decimal\", [\n    (\"5/2\", Decimal(\"3.5\")),      # 2.5 + 1 stake\n    (\"10/1\", Decimal(\"11.0\")),     # 10.0 + 1 stake\n    (\"EVENS\", Decimal(\"2.0\")),     # 1.0 + 1 stake\n    (\"EVS\", Decimal(\"2.0\")),       # 1.0 + 1 stake\n    (\"1/2\", Decimal(\"1.5\")),       # 0.5 + 1 stake\n    (\"2.5\", Decimal(\"2.5\")),       # Handles decimal strings\n    (3.5, Decimal(\"3.5\")),         # Handles float input\n    (10, Decimal(\"10\")),           # Handles int input\n    (\"SP\", None),                  # Should handle non-fractional odds gracefully\n    (\"SCR\", None),                 # Should handle scratched runners\n    (\"REF\", None),                 # Should handle other non-numeric values\n    (\"\", None),\n    (None, None)\n])\ndef test_parse_odds_to_decimal(input_odds, expected_decimal):\n    \"\"\"Tests the new centralized odds parsing utility with various formats.\"\"\"\n    assert parse_odds_to_decimal(input_odds) == expected_decimal\n",
    "tests/adapters/test_timeform_adapter_modernized.py": "# Modernized test resurrected from attic/legacy_tests_pre_triage/adapters/test_timeform_adapter.py\nimport pytest\nfrom unittest.mock import MagicMock, patch\nimport httpx\nfrom decimal import Decimal\nfrom python_service.adapters.timeform_adapter import TimeformAdapter\nfrom python_service.models import Race, Runner\n\n@pytest.fixture\ndef timeform_adapter():\n    mock_config = MagicMock()\n    return TimeformAdapter(config=mock_config)\n\ndef read_fixture(file_path):\n    with open(file_path, 'r') as f:\n        return f.read()\n\n@pytest.mark.asyncio\nasync def test_timeform_adapter_parses_html_correctly(timeform_adapter):\n    \"\"\"Verify adapter correctly parses a known HTML fixture.\"\"\"\n    mock_html = read_fixture('tests/fixtures/timeform_modern_sample.html')\n\n    # Directly test the parsing of runners from the correct HTML structure\n    from bs4 import BeautifulSoup\n    soup = BeautifulSoup(mock_html, \"html.parser\")\n    runners = [timeform_adapter._parse_runner(row) for row in soup.select(\"div.rp-horseTable_mainRow\")]\n\n    assert len(runners) == 3, 'Should parse three runners'\n\n    braveheart = next((r for r in runners if r.name == 'Braveheart'), None)\n    assert braveheart is not None\n    assert braveheart.odds['Timeform'].win == Decimal('3.5')\n\n    steady_eddy = next((r for r in runners if r.name == 'Steady Eddy'), None)\n    assert steady_eddy is not None\n    assert steady_eddy.odds['Timeform'].win == Decimal('2.0')",
    "tests/adapters/test_gbgb_api_adapter.py": "# tests/adapters/test_gbgb_api_adapter.py\n\nimport pytest\nimport respx\nimport httpx\nfrom datetime import date\nfrom decimal import Decimal\n\nfrom python_service.config import get_settings\nfrom python_service.adapters.gbgb_api_adapter import GbgbApiAdapter\n\n@pytest.fixture\ndef gbgb_adapter():\n    \"\"\"Returns a GbgbApiAdapter instance for testing.\"\"\"\n    return GbgbApiAdapter(config=get_settings())\n\n@pytest.mark.asyncio\n@respx.mock\nasync def test_fetch_gbgb_races_successfully(gbgb_adapter):\n    \"\"\"\n    SPEC: The GbgbApiAdapter should correctly parse a standard API response,\n    creating Race and Runner objects with the correct data, including fractional odds.\n    \"\"\"\n    # ARRANGE\n    mock_date = date.today().strftime('%Y-%m-%d')\n    mock_url = f\"{gbgb_adapter.base_url}results/meeting/{mock_date}\"\n\n    mock_api_response = [\n        {\n            \"trackName\": \"Towcester\",\n            \"races\": [\n                {\n                    \"raceId\": 12345,\n                    \"raceNumber\": 1,\n                    \"raceTime\": \"2025-10-09T18:00:00Z\",\n                    \"raceTitle\": \"The October Sprint\",\n                    \"raceDistance\": 500,\n                    \"traps\": [\n                        {\n                            \"trapNumber\": 1,\n                            \"dogName\": \"Rapid Rover\",\n                            \"sp\": \"5/2\"\n                        },\n                        {\n                            \"trapNumber\": 2,\n                            \"dogName\": \"Speedy Sue\",\n                            \"sp\": \"EVS\" # Test even money\n                        },\n                        {\n                            \"trapNumber\": 3,\n                            \"dogName\": \"Lazy Larry\",\n                            \"sp\": \"10/1\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n\n    respx.get(mock_url).mock(return_value=httpx.Response(200, json=mock_api_response))\n\n    # ACT\n    async with httpx.AsyncClient() as client:\n        result = await gbgb_adapter.fetch_races(mock_date, client)\n\n    # ASSERT\n    assert result['source_info']['status'] == 'SUCCESS'\n    assert len(result['races']) == 1\n\n    race = result['races'][0]\n    assert race.venue == \"Towcester\"\n    assert race.race_number == 1\n    assert race.race_name == \"The October Sprint\"\n    assert race.distance == \"500m\"\n    assert len(race.runners) == 3\n\n    runner1 = next(r for r in race.runners if r.number == 1)\n    assert runner1.name == \"Rapid Rover\"\n    assert runner1.odds['GBGB'].win == Decimal(\"3.5\")\n\n    runner2 = next(r for r in race.runners if r.number == 2)\n    assert runner2.name == \"Speedy Sue\"\n    assert runner2.odds['GBGB'].win == Decimal(\"2.0\")\n\n    runner3 = next(r for r in race.runners if r.number == 3)\n    assert runner3.name == \"Lazy Larry\"\n    assert runner3.odds['GBGB'].win == Decimal(\"11.0\")",
    "tests/adapters/test_base_v3.py": "# tests/adapters/test_base_v3.py\nimport pytest\nfrom typing import Any, List\nfrom python_service.adapters.base_v3 import BaseAdapterV3\nfrom python_service.models import Race\n\n# A concrete implementation for testing the abstract class\nclass ConcreteAdapter(BaseAdapterV3):\n    def __init__(self, source_name: str, base_url: str):\n        super().__init__(source_name, base_url)\n\n    SOURCE_NAME = \"TestAdapter\"\n    async def _fetch_data(self, date: str) -> Any:\n        if date == \"good_data\":\n            return [{\"id\": 1}, {\"id\": 2}]\n        if date == \"no_data\":\n            return None\n        if date == \"bad_data\":\n            raise ValueError(\"Simulated fetch error\")\n        return None\n\n    def _parse_races(self, raw_data: Any) -> List[Race]:\n        # In a real test, you'd mock Race objects\n        return raw_data # Simple passthrough for testing orchestration\n\n    async def fetch_races(self, date: str, http_client):\n        pass\n\n@pytest.mark.asyncio\nasync def test_get_races_orchestration_success():\n    \"\"\"Tests that _fetch_data and _parse_races are called in order on success.\"\"\"\n    adapter = ConcreteAdapter(source_name=\"TestAdapter\", base_url=\"http://test.com\")\n    races = [race async for race in adapter.get_races(\"good_data\")]\n    assert len(races) == 2\n    assert races == [{\"id\": 1}, {\"id\": 2}]\n    assert adapter.circuit_breaker_failure_count == 0\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_no_data_from_fetch():\n    \"\"\"Tests that the pipeline gracefully exits if _fetch_data returns None.\"\"\"\n    adapter = ConcreteAdapter(source_name=\"TestAdapter\", base_url=\"http://test.com\")\n    races = [race async for race in adapter.get_races(\"no_data\")]\n    assert len(races) == 0\n\n@pytest.mark.asyncio\nasync def test_get_races_handles_fetch_exception_and_trips_breaker():\n    \"\"\"Tests that an exception in _fetch_data is caught and trips the circuit breaker.\"\"\"\n    adapter = ConcreteAdapter(source_name=\"TestAdapter\", base_url=\"http://test.com\")\n    assert not adapter.circuit_breaker_tripped\n\n    # Fail 3 times to trip the breaker\n    for _ in range(3):\n        races = [race async for race in adapter.get_races(\"bad_data\")]\n        assert len(races) == 0\n\n    assert adapter.circuit_breaker_tripped\n    assert adapter.circuit_breaker_failure_count == 3\n\n    # On the 4th attempt, it should not even try to fetch\n    races = [race async for race in adapter.get_races(\"good_data\")]\n    assert len(races) == 0",
    "tests/adapters/test_the_racing_api_adapter.py": "import pytest\nfrom unittest.mock import AsyncMock, Mock, patch\nfrom datetime import date, datetime\nfrom decimal import Decimal\n\nfrom python_service.adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom python_service.config import Settings\n\n@pytest.fixture\ndef mock_config():\n    \"\"\"\n    Provides a mock config object for the adapter, ensuring it doesn't\n    load from any .env files and provides the necessary API key.\n    \"\"\"\n    class TestSettings(Settings):\n        class Config:\n            env_file = None\n\n    return TestSettings(\n        API_KEY=\"test_api_key\",\n        THE_RACING_API_KEY=\"test_racing_api_key\"\n    )\n\n@pytest.fixture\ndef mock_config_no_key():\n    \"\"\"Provides a mock config with the API key explicitly set to None.\"\"\"\n    class TestSettings(Settings):\n        class Config:\n            env_file = None\n\n    return TestSettings(\n        API_KEY=\"test_api_key\",\n        THE_RACING_API_KEY=None\n    )\n\n@pytest.mark.asyncio\n@patch('python_service.adapters.the_racing_api_adapter.TheRacingApiAdapter.make_request', new_callable=AsyncMock)\nasync def test_fetch_races_parses_correctly(mock_make_request, mock_config):\n    \"\"\"\n    Tests that TheRacingApiAdapter correctly parses a valid API response.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n    off_time_str = datetime.utcnow().isoformat() + \"Z\"\n\n\n    mock_api_response = {\n        \"racecards\": [\n            {\n                \"race_id\": \"12345\",\n                \"course\": \"Newbury\",\n                \"race_no\": 3,\n                \"off_time\": off_time_str,\n                \"race_name\": \"The Great Race\",\n                \"distance_f\": \"1m 2f\",\n                \"runners\": [\n                    {\n                        \"horse\": \"Speedy Steed\",\n                        \"number\": 1,\n                        \"jockey\": \"T. Rider\",\n                        \"trainer\": \"A. Trainer\",\n                        \"odds\": [{\"odds_decimal\": \"5.50\"}]\n                    },\n                    {\n                        \"horse\": \"Gallant Gus\",\n                        \"number\": 2,\n                        \"jockey\": \"J. Jockey\",\n                        \"trainer\": \"B. Builder\",\n                        \"odds\": [{\"odds_decimal\": \"3.25\"}]\n                    }\n                ]\n            }\n        ]\n    }\n    mock_response = Mock()\n    mock_response.json.return_value = mock_api_response\n    mock_make_request.return_value = mock_response\n\n    # ACT\n    result = await adapter.fetch_races(today, AsyncMock())\n\n    # ASSERT\n    assert result is not None\n    assert result['source_info']['status'] == 'SUCCESS'\n    assert result['source_info']['races_fetched'] == 1\n\n    races = result['races']\n    assert len(races) == 1\n\n    race = races[0]\n    assert race.id == 'tra_12345'\n    assert race.venue == \"Newbury\"\n    assert race.race_number == 3\n    assert race.race_name == \"The Great Race\"\n    assert race.distance == \"1m 2f\"\n\n    assert len(race.runners) == 2\n\n    runner1 = race.runners[0]\n    assert runner1.name == \"Speedy Steed\"\n    assert runner1.number == 1\n    assert runner1.jockey == \"T. Rider\"\n    assert runner1.trainer == \"A. Trainer\"\n    assert runner1.odds[adapter.source_name].win == Decimal(\"5.50\")\n\n@pytest.mark.asyncio\n@patch('python_service.adapters.the_racing_api_adapter.TheRacingApiAdapter.make_request', new_callable=AsyncMock)\nasync def test_fetch_races_handles_empty_response(mock_make_request, mock_config):\n    \"\"\"\n    Tests that the adapter handles an API response with no racecards.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n    mock_response = Mock()\n    mock_response.json.return_value = {\"racecards\": []}\n    mock_make_request.return_value = mock_response\n\n    # ACT\n    result = await adapter.fetch_races(today, AsyncMock())\n\n    # ASSERT\n    assert result is not None\n    assert result['source_info']['status'] == 'SUCCESS'\n    assert result['source_info']['races_fetched'] == 0\n    assert result['source_info']['error_message'] == \"No racecards found in API response.\"\n    assert len(result['races']) == 0\n\n@pytest.mark.asyncio\nasync def test_fetch_races_handles_auth_failure(mock_config_no_key):\n    \"\"\"\n    Tests that the adapter returns a configuration error if the API key is not set.\n    \"\"\"\n    # ARRANGE\n    adapter = TheRacingApiAdapter(config=mock_config_no_key)\n    today = date.today().strftime('%Y-%m-%d')\n\n    # ACT\n    result = await adapter.fetch_races(today, AsyncMock())\n\n    # ASSERT\n    assert result is not None\n    assert result['source_info']['status'] == 'FAILED'\n    assert result['source_info']['races_fetched'] == 0\n    assert result['source_info']['error_message'] == \"ConfigurationError: THE_RACING_API_KEY not set\"\n    assert len(result['races']) == 0",
    "tests/adapters/test_greyhound_adapter.py": "import pytest\nfrom unittest.mock import AsyncMock, Mock, patch\nfrom datetime import date, datetime\nfrom python_service.adapters.greyhound_adapter import GreyhoundAdapter\nfrom python_service.config import Settings\n\n@pytest.fixture\ndef mock_config():\n    \"\"\"\n    Provides a mock config object for the adapter, ensuring it doesn't\n    load from any .env files, which prevents test pollution.\n    \"\"\"\n    class TestSettings(Settings):\n        class Config:\n            env_file = None\n\n    return TestSettings(\n        BETFAIR_APP_KEY=\"test_key\",\n        BETFAIR_USERNAME=\"test_user\",\n        BETFAIR_PASSWORD=\"test_password\",\n        API_KEY=\"test_api_key\",\n        GREYHOUND_API_URL=\"https://api.example.com\"\n    )\n\n@pytest.mark.asyncio\n@patch('python_service.adapters.greyhound_adapter.GreyhoundAdapter.make_request', new_callable=AsyncMock)\nasync def test_fetch_races_parses_correctly(mock_make_request, mock_config):\n    \"\"\"\n    Tests that the GreyhoundAdapter correctly parses a valid API response.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n\n    mock_api_response = {\n        \"cards\": [\n            {\n                \"track_name\": \"Test Track\",\n                \"races\": [\n                    {\n                        \"race_id\": \"test_race_123\",\n                        \"race_number\": 1,\n                        \"start_time\": int(datetime.now().timestamp()),\n                        \"runners\": [\n                            {\n                                \"dog_name\": \"Rapid Rover\",\n                                \"trap_number\": 1,\n                                \"odds\": {\"win\": \"2.5\"}\n                            },\n                            {\n                                \"dog_name\": \"Swift Sprint\",\n                                \"trap_number\": 2,\n                                \"scratched\": True\n                            },\n                            {\n                                \"dog_name\": \"Lazy Larry\",\n                                \"trap_number\": 3,\n                                \"odds\": {\"win\": \"10.0\"}\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\n    mock_response = Mock()\n    mock_response.json.return_value = mock_api_response\n    mock_make_request.return_value = mock_response\n\n    # ACT\n    result = await adapter.fetch_races(today, AsyncMock())\n\n    # ASSERT\n    assert result is not None\n    assert result['source_info']['status'] == 'SUCCESS'\n    assert result['source_info']['races_fetched'] == 1\n\n    races = result['races']\n    assert len(races) == 1\n\n    race = races[0]\n    assert race.id == 'greyhound_test_race_123'\n    assert race.venue == 'Test Track'\n    assert len(race.runners) == 2 # One was scratched\n\n    runner1 = race.runners[0]\n    assert runner1.name == 'Rapid Rover'\n    assert runner1.number == 1\n    assert runner1.odds['Greyhound Racing'].win == 2.5\n\n@pytest.mark.asyncio\n@patch('python_service.adapters.greyhound_adapter.GreyhoundAdapter.make_request', new_callable=AsyncMock)\nasync def test_fetch_races_handles_empty_response(mock_make_request, mock_config):\n    \"\"\"\n    Tests that the GreyhoundAdapter handles an empty or invalid API response gracefully.\n    \"\"\"\n    # ARRANGE\n    adapter = GreyhoundAdapter(config=mock_config)\n    today = date.today().strftime('%Y-%m-%d')\n    mock_response = Mock()\n    mock_response.json.return_value = {\"cards\": []}\n    mock_make_request.return_value = mock_response\n\n    # ACT\n    result = await adapter.fetch_races(today, AsyncMock())\n\n    # ASSERT\n    assert result is not None\n    assert result['source_info']['status'] == 'SUCCESS'\n    assert result['source_info']['races_fetched'] == 0\n    assert result['source_info']['error_message'] == \"No race cards found for date.\"\n    assert len(result['races']) == 0",
    "tests/fixtures/timeform_legacy_sample.html": "<!DOCTYPE html><html><body><div class='race-card'><div class='runner'><span class='runner-name'>Braveheart</span><span class='runner-odds'>5/2</span></div><div class='runner'><span class='runner-name'>Speedster</span><span class='runner-odds'>10/1</span></div><div class='runner'><span class='runner-name'>Steady Eddy</span><span class='runner-odds'>EVENS</span></div></div></body></html>",
    "tests/fixtures/timeform_modern_sample.html": "<div class=\"rp-horseTable_mainRow\">\n  <a class=\"rp-horseTable_horse-name\">Braveheart</a>\n  <span class=\"rp-horseTable_horse-number\">(1)</span>\n  <button class=\"rp-bet-placer-btn__odds\">5/2</button>\n</div>\n<div class=\"rp-horseTable_mainRow\">\n  <a class=\"rp-horseTable_horse-name\">Speedster</a>\n  <span class=\"rp-horseTable_horse-number\">(2)</span>\n  <button class=\"rp-bet-placer-btn__odds\">10/1</button>\n</div>\n<div class=\"rp-horseTable_mainRow\">\n  <a class=\"rp-horseTable_horse-name\">Steady Eddy</a>\n  <span class=\"rp-horseTable_horse-number\">(3)</span>\n  <button class=\"rp-bet-placer-btn__odds\">EVENS</button>\n</div>\n",
    "tests/analyzers/test_trifecta_analyzer.py": "# Dedicated test suite for the TrifectaAnalyzer, resurrected and expanded.\nimport pytest\nimport datetime\nfrom python_service.analyzer import TrifectaAnalyzer\nfrom python_service.models import Race, Runner\n\n@pytest.fixture\ndef analyzer():\n    return TrifectaAnalyzer()\n\n@pytest.fixture\ndef create_race(runners):\n    return Race(\n        id='test-race',\n        venue='TEST',\n        race_number=1,\n        start_time=datetime.datetime.now(),\n        runners=runners,\n        source='test'\n    )\n\ndef test_analyzer_name(analyzer):\n    assert analyzer.name == \"trifecta_analyzer\"\n\n# Test cases resurrected from legacy scorer and logic tests\ndef test_qualifies_with_exactly_three_runners(analyzer, create_race):\n    runners = [\n        Runner(number=1, name='A', odds='2/1', scratched=False),\n        Runner(number=2, name='B', odds='3/1', scratched=False),\n        Runner(number=3, name='C', odds='4/1', scratched=False)\n    ]\n    race = create_race(runners)\n    assert analyzer.is_race_qualified(race) is True\n\ndef test_qualifies_with_more_than_three_runners(analyzer, create_race):\n    runners = [\n        Runner(number=1, name='A', odds='2/1', scratched=False),\n        Runner(number=2, name='B', odds='3/1', scratched=False),\n        Runner(number=3, name='C', odds='4/1', scratched=False),\n        Runner(number=4, name='D', odds='5/1', scratched=False)\n    ]\n    race = create_race(runners)\n    assert analyzer.is_race_qualified(race) is True\n\n# New test cases for edge-case hardening\ndef test_rejects_with_fewer_than_three_runners(analyzer, create_race):\n    runners = [\n        Runner(number=1, name='A', odds='2/1', scratched=False),\n        Runner(number=2, name='B', odds='3/1', scratched=False)\n    ]\n    race = create_race(runners)\n    assert analyzer.is_race_qualified(race) is False\n\ndef test_rejects_if_scratched_runners_reduce_field_below_three(analyzer, create_race):\n    runners = [\n        Runner(number=1, name='A', odds='2/1', scratched=False),\n        Runner(number=2, name='B', odds='3/1', scratched=False),\n        Runner(number=3, name='C', odds='4/1', scratched=True) # Scratched\n    ]\n    race = create_race(runners)\n    assert analyzer.is_race_qualified(race) is False\n\ndef test_handles_empty_runner_list(analyzer, create_race):\n    race = create_race([])\n    assert analyzer.is_race_qualified(race) is False\n\ndef test_handles_none_race_object(analyzer):\n    assert analyzer.is_race_qualified(None) is False",
    "pg_schemas/quarantine_races.sql": "CREATE TABLE IF NOT EXISTS quarantine_races (\n    quarantine_id SERIAL PRIMARY KEY,\n    race_id VARCHAR(100),\n    track_name VARCHAR(100),\n    race_number INT,\n    post_time TIMESTAMP WITH TIME ZONE,\n    source VARCHAR(50),\n    raw_data_json JSONB, -- Store the original raw data for inspection\n    quarantine_reason TEXT, -- Reason for failing validation\n    collection_timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);",
    "pg_schemas/quarantined_races.sql": "-- Schema for storing race data that fails validation\nCREATE TABLE IF NOT EXISTS quarantined_races (\n    quarantine_id SERIAL PRIMARY KEY,\n    race_id VARCHAR(255),\n    source VARCHAR(50),\n    payload JSONB NOT NULL,\n    reason VARCHAR(255) NOT NULL,\n    quarantined_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n",
    "pg_schemas/historical_races.sql": "-- Schema for the main historical races data warehouse table\nCREATE TABLE IF NOT EXISTS historical_races (\n    race_id VARCHAR(255) PRIMARY KEY,\n    venue VARCHAR(100) NOT NULL,\n    race_number INTEGER NOT NULL,\n    start_time TIMESTAMP WITH TIME ZONE NOT NULL,\n    source VARCHAR(50),\n    qualification_score NUMERIC(5, 2),\n    field_size INTEGER,\n    extracted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n"
}