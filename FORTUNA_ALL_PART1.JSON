{
    "python_service/analyzer.py": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Type, Optional, Any\nimport structlog\nfrom decimal import Decimal\nimport os\nfrom pathlib import Path\n\nfrom python_service.models import Race, Runner\n\ntry:\n    # winsound is a built-in Windows library\n    import winsound\nexcept ImportError:\n    winsound = None\ntry:\n    from win10toast_py3 import ToastNotifier\nexcept (ImportError, RuntimeError):\n    # Fails gracefully on non-Windows systems\n    ToastNotifier = None\n\nlog = structlog.get_logger(__name__)\n\ndef _get_best_win_odds(runner: Runner) -> Optional[Decimal]:\n    \"\"\"Gets the best win odds for a runner, filtering out invalid or placeholder values.\"\"\"\n    if not runner.odds:\n        return None\n\n    # Filter out invalid or placeholder odds (e.g., > 999)\n    valid_odds = [o.win for o in runner.odds.values()\n                  if o.win is not None and o.win > 0 and o.win < 999]\n\n    if not valid_odds:\n        return None\n\n    return min(valid_odds)\n\nclass BaseAnalyzer(ABC):\n    \"\"\"The abstract interface for all future analyzer plugins.\"\"\"\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"The core method every analyzer must implement.\"\"\"\n        pass\n\nclass TrifectaAnalyzer(BaseAnalyzer):\n    \"\"\"Analyzes races and assigns a qualification score based on the 'Trifecta of Factors'.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return \"trifecta_analyzer\"\n\n    def __init__(self, max_field_size: int = 10, min_favorite_odds: float = 2.5, min_second_favorite_odds: float = 4.0):\n        self.max_field_size = max_field_size\n        self.min_favorite_odds = Decimal(str(min_favorite_odds))\n        self.min_second_favorite_odds = Decimal(str(min_second_favorite_odds))\n        self.notifier = RaceNotifier()\n\n    def is_race_qualified(self, race: Race) -> bool:\n        \"\"\"A race is qualified for a trifecta if it has at least 3 non-scratched runners.\"\"\"\n        if not race or not race.runners:\n            return False\n\n        active_runners = sum(1 for r in race.runners if not r.scratched)\n        return active_runners >= 3\n\n    def qualify_races(self, races: List[Race]) -> Dict[str, Any]:\n        \"\"\"Scores all races and returns a dictionary with criteria and a sorted list.\"\"\"\n        scored_races = []\n        for race in races:\n            # The _evaluate_race method now always returns a float score.\n            race.qualification_score = self._evaluate_race(race)\n            scored_races.append(race)\n\n        scored_races.sort(key=lambda r: r.qualification_score, reverse=True)\n\n        criteria = {\n            \"max_field_size\": self.max_field_size,\n            \"min_favorite_odds\": float(self.min_favorite_odds),\n            \"min_second_favorite_odds\": float(self.min_second_favorite_odds)\n        }\n\n        log.info(\"Universal scoring complete\", total_races_scored=len(scored_races), criteria=criteria)\n\n        for race in scored_races:\n            if race.qualification_score and race.qualification_score >= 85:\n                self.notifier.notify_qualified_race(race)\n\n        return {\"criteria\": criteria, \"races\": scored_races}\n\n    def _evaluate_race(self, race: Race) -> float:\n        \"\"\"Evaluates a single race and returns a qualification score.\"\"\"\n        # --- Constants for Scoring Logic ---\n        FAV_ODDS_NORMALIZATION = 10.0\n        SEC_FAV_ODDS_NORMALIZATION = 15.0\n        FAV_ODDS_WEIGHT = 0.6\n        SEC_FAV_ODDS_WEIGHT = 0.4\n        FIELD_SIZE_SCORE_WEIGHT = 0.3\n        ODDS_SCORE_WEIGHT = 0.7\n\n        active_runners = [r for r in race.runners if not r.scratched]\n\n        runners_with_odds = []\n        for runner in active_runners:\n            best_odds = _get_best_win_odds(runner)\n            if best_odds is not None:\n                runners_with_odds.append((runner, best_odds))\n\n        if len(runners_with_odds) < 2:\n            return 0.0\n\n        runners_with_odds.sort(key=lambda x: x[1])\n        favorite_odds = runners_with_odds[0][1]\n        second_favorite_odds = runners_with_odds[1][1]\n\n        # --- Calculate Qualification Score (as inspired by the TypeScript Genesis) ---\n        field_score = (self.max_field_size - len(active_runners)) / self.max_field_size\n\n        # Normalize odds scores - cap influence of extremely high odds\n        fav_odds_score = min(float(favorite_odds) / FAV_ODDS_NORMALIZATION, 1.0)\n        sec_fav_odds_score = min(float(second_favorite_odds) / SEC_FAV_ODDS_NORMALIZATION, 1.0)\n\n        # Weighted average\n        odds_score = (fav_odds_score * FAV_ODDS_WEIGHT) + (sec_fav_odds_score * SEC_FAV_ODDS_WEIGHT)\n        final_score = (field_score * FIELD_SIZE_SCORE_WEIGHT) + (odds_score * ODDS_SCORE_WEIGHT)\n\n        # --- Apply a penalty if hard filters are not met, instead of returning None ---\n        if (len(active_runners) > self.max_field_size or\n            favorite_odds < self.min_favorite_odds or\n            second_favorite_odds < self.min_second_favorite_odds):\n            # Assign a score of 0 to races that would have been filtered out\n            return 0.0\n\n        return round(final_score * 100, 2)\n\nclass AnalyzerEngine:\n    \"\"\"Discovers and manages all available analyzer plugins.\"\"\"\n    def __init__(self):\n        self.analyzers: Dict[str, Type[BaseAnalyzer]] = {}\n        self._discover_analyzers()\n\n    def _discover_analyzers(self):\n        # In a real plugin system, this would inspect a folder.\n        # For now, we register them manually.\n        self.register_analyzer('trifecta', TrifectaAnalyzer)\n        log.info(\"AnalyzerEngine discovered plugins\", available_analyzers=list(self.analyzers.keys()))\n\n    def register_analyzer(self, name: str, analyzer_class: Type[BaseAnalyzer]):\n        self.analyzers[name] = analyzer_class\n\n    def get_analyzer(self, name: str, **kwargs) -> BaseAnalyzer:\n        analyzer_class = self.analyzers.get(name)\n        if not analyzer_class:\n            log.error(\"Requested analyzer not found\", requested_analyzer=name)\n            raise ValueError(f\"Analyzer '{name}' not found.\")\n        return analyzer_class(**kwargs)\n\nclass AudioAlertSystem:\n    \"\"\"Plays sound alerts for important events.\"\"\"\n    def __init__(self):\n        self.sounds = {\n            'high_value': Path(__file__).parent.parent.parent / 'assets' / 'sounds' / 'alert_premium.wav',\n        }\n\n    def play(self, sound_type: str):\n        if not winsound:\n            return\n\n        sound_file = self.sounds.get(sound_type)\n        if sound_file and sound_file.exists():\n            try:\n                winsound.PlaySound(str(sound_file), winsound.SND_FILENAME | winsound.SND_ASYNC)\n            except Exception as e:\n                log.warning(\"Could not play sound\", file=sound_file, error=e)\n\nclass RaceNotifier:\n    \"\"\"Handles sending native Windows notifications and audio alerts for high-value races.\"\"\"\n    def __init__(self):\n        self.toaster = ToastNotifier() if ToastNotifier else None\n        self.audio_system = AudioAlertSystem()\n        self.notified_races = set()\n\n    def notify_qualified_race(self, race):\n        if not self.toaster or race.id in self.notified_races:\n            return\n\n        title = f\"\ud83c\udfc7 High-Value Opportunity!\"\n        message = f\"\"\"{race.venue} - Race {race.race_number}\nScore: {race.qualification_score:.0f}%\nPost Time: {race.start_time.strftime('%I:%M %p')}\"\"\"\n\n        try:\n            # The `threaded=True` argument is crucial to prevent blocking the main application thread.\n            self.toaster.show_toast(title, message, duration=10, threaded=True)\n            self.notified_races.add(race.id)\n            self.audio_system.play('high_value')\n            log.info(\"Notification and audio alert sent for high-value race\", race_id=race.id)\n        except Exception as e:\n            # Catch potential exceptions from the notification library itself\n            log.error(\"Failed to send notification\", error=str(e), exc_info=True)\n",
    "python_service/api.py": "# python_service/api.py\n\nimport structlog\nfrom datetime import datetime, date\nfrom typing import Optional\nfrom fastapi import FastAPI, HTTPException, Request, Depends, Query\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom python_service.middleware.error_handler import ErrorRecoveryMiddleware\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.middleware import SlowAPIMiddleware\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nimport aiosqlite\nfrom typing import List\nfrom contextlib import asynccontextmanager\n\nfrom .config import get_settings\nfrom .engine import FortunaEngine\nfrom .health import router as health_router\nfrom .models import AggregatedResponse, QualifiedRacesResponse, TipsheetRace\nfrom .security import verify_api_key\nfrom .logging_config import configure_logging\nfrom .analyzer import AnalyzerEngine\n\nlog = structlog.get_logger()\n\n# Define the lifespan context manager for robust startup/shutdown\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Manage the application's lifespan. On startup, it initializes the OddsEngine\n    with validated settings and attaches it to the app state. On shutdown, it\n    properly closes the engine's resources.\n    \"\"\"\n    configure_logging()\n    settings = get_settings()\n    app.state.engine = FortunaEngine(config=settings)\n    app.state.analyzer_engine = AnalyzerEngine()\n    log.info(\"Server startup: Configuration validated and FortunaEngine initialized.\")\n    yield\n    # Clean up the engine resources\n    await app.state.engine.close()\n    log.info(\"Server shutdown: HTTP client resources closed.\")\n\nlimiter = Limiter(key_func=get_remote_address)\n\n# Pass the lifespan manager to the FastAPI app\napp = FastAPI(title=\"Checkmate Ultimate Solo API\", version=\"2.1\", lifespan=lifespan)\napp.add_middleware(SlowAPIMiddleware)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\nsettings = get_settings()\n\n# Add middlewares (order can be important)\napp.add_middleware(ErrorRecoveryMiddleware)\napp.include_router(health_router)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True, allow_methods=[\"GET\"], allow_headers=[\"*\"]\n)\n\n# Dependency function to get the engine instance from the app state\ndef get_engine(request: Request) -> FortunaEngine:\n    return request.app.state.engine\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n\n@app.get(\"/api/adapters/status\")\n@limiter.limit(\"60/minute\")\nasync def get_all_adapter_statuses(request: Request, engine: FortunaEngine = Depends(get_engine), _=Depends(verify_api_key)):\n    \"\"\"Provides a list of health statuses for all adapters, required by the new frontend blueprint.\"\"\"\n    try:\n        statuses = engine.get_all_adapter_statuses()\n        return statuses\n    except Exception:\n        log.error(\"Error in /api/adapters/status\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races/qualified/{analyzer_name}\", \n         response_model=QualifiedRacesResponse,\n         description=\"Fetch and analyze races from all configured data sources, returning a list of races that meet the specified analyzer's criteria.\",\n         responses={\n             200: {\n                 \"description\": \"A list of qualified races with their scores.\",\n                 \"content\": {\n                     \"application/json\": {\n                         \"example\": {\n                             \"races\": [\n                                 {\n                                     \"id\": \"12345_2025-10-14_1\",\n                                     \"venue\": \"Santa Anita\",\n                                     \"race_number\": 1,\n                                     \"start_time\": \"2025-10-14T20:30:00Z\",\n                                     \"runners\": [\n                                         {\"number\": 1, \"name\": \"Speedy Gonzalez\", \"odds\": \"5/2\"}\n                                     ],\n                                     \"source\": \"TVG\",\n                                     \"qualification_score\": 95.5\n                                 }\n                             ],\n                             \"analyzer\": \"trifecta_analyzer\"\n                         }\n                     }\n                 }\n             },\n             404: {\n                 \"description\": \"The specified analyzer was not found.\"\n             }\n         }\n)\n@limiter.limit(\"30/minute\")\nasync def get_qualified_races(\n    analyzer_name: str,\n    request: Request,\n    race_date: Optional[date] = None,\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key),\n    # --- Dynamic Analyzer Parameters ---\n    max_field_size: Optional[int] = Query(None, description=\"Override the max field size for the analyzer.\"),\n    min_favorite_odds: Optional[float] = Query(None, description=\"Override the min favorite odds.\"),\n    min_second_favorite_odds: Optional[float] = Query(None, description=\"Override the min second favorite odds.\")\n):\n    \"\"\"\n    Gets all races for a given date, filters them for qualified betting\n    opportunities, and returns the qualified races.\n    \"\"\"\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime('%Y-%m-%d')\n        background_tasks = set() # Dummy background tasks\n        aggregated_data = await engine.get_races(date_str, background_tasks)\n\n        races = aggregated_data.get('races', [])\n\n        analyzer_engine = request.app.state.analyzer_engine\n        analyzer_params = {\n            \"max_field_size\": max_field_size,\n            \"min_favorite_odds\": min_favorite_odds,\n            \"min_second_favorite_odds\": min_second_favorite_odds\n        }\n        custom_params = {k: v for k, v in analyzer_params.items() if v is not None}\n\n        analyzer = analyzer_engine.get_analyzer(analyzer_name, **custom_params)\n        result = analyzer.qualify_races(races)\n        return QualifiedRacesResponse(**result)\n    except ValueError as e:\n        log.warning(\"Requested analyzer not found\", analyzer_name=analyzer_name)\n        raise HTTPException(status_code=404, detail=str(e))\n    except Exception as e:\n        log.error(\"Error in /api/races/qualified\", error=str(e), exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n@app.get(\"/api/races\", response_model=AggregatedResponse)\n@limiter.limit(\"30/minute\")\nasync def get_races(\n    request: Request,\n    race_date: Optional[date] = None,\n    source: Optional[str] = None,\n    engine: FortunaEngine = Depends(get_engine),\n    _=Depends(verify_api_key)\n):\n    try:\n        if race_date is None:\n            race_date = datetime.now().date()\n        date_str = race_date.strftime('%Y-%m-%d')\n        background_tasks = set() # Dummy background tasks\n        aggregated_data = await engine.get_races(date_str, background_tasks, source)\n        return aggregated_data\n    except Exception:\n        log.error(\"Error in /api/races\", exc_info=True)\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\nDB_PATH = 'fortuna.db'\n\ndef get_current_date() -> date:\n    return datetime.now().date()\n\n@app.get(\"/api/tipsheet\", response_model=List[TipsheetRace])\n@limiter.limit(\"30/minute\")\nasync def get_tipsheet_endpoint(request: Request, date: date = Depends(get_current_date)):\n    \"\"\"Fetches the generated tipsheet from the database asynchronously.\"\"\"\n    results = []\n    try:\n        async with aiosqlite.connect(DB_PATH) as db:\n            db.row_factory = aiosqlite.Row\n            query = 'SELECT * FROM tipsheet WHERE date(post_time) = ? ORDER BY post_time ASC'\n            async with db.execute(query, (date.isoformat(),)) as cursor:\n                async for row in cursor:\n                    results.append(dict(row))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n    return results\n",
    "python_service/cache_manager.py": "# python_service/cache_manager.py\nfrom functools import wraps\nimport json\nimport hashlib\nfrom datetime import datetime, timedelta\nfrom typing import Any, Callable\nimport os\nimport structlog\n\ntry:\n    import redis\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n\nlog = structlog.get_logger(__name__)\n\nclass CacheManager:\n    def __init__(self, redis_url: str = None):\n        self.redis_client = None\n        self.memory_cache = {}\n        if REDIS_AVAILABLE and redis_url:\n            try:\n                self.redis_client = redis.from_url(redis_url, decode_responses=True)\n                log.info(\"Redis cache connected successfully.\")\n            except Exception as e:\n                log.warning(f\"Failed to connect to Redis: {e}. Falling back to in-memory cache.\")\n\n    def _generate_key(self, prefix: str, *args, **kwargs) -> str:\n        key_data = f\"{prefix}:{args}:{sorted(kwargs.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    def get(self, key: str) -> Any | None:\n        if self.redis_client:\n            try:\n                value = self.redis_client.get(key)\n                return json.loads(value) if value else None\n            except Exception as e:\n                log.warning(f\"Redis GET failed: {e}\")\n\n        entry = self.memory_cache.get(key)\n        if entry and entry[\"expires_at\"] > datetime.now():\n            return entry[\"value\"]\n        return None\n\n    def set(self, key: str, value: Any, ttl_seconds: int = 300):\n        serialized = json.dumps(value, default=str)\n        if self.redis_client:\n            try:\n                self.redis_client.setex(key, ttl_seconds, serialized)\n                return\n            except Exception as e:\n                log.warning(f\"Redis SET failed: {e}\")\n\n        self.memory_cache[key] = {\n            \"value\": value,\n            \"expires_at\": datetime.now() + timedelta(seconds=ttl_seconds)\n        }\n\n# --- Singleton Instance & Decorator ---\ncache_manager = CacheManager(redis_url=os.getenv(\"REDIS_URL\"))\n\ndef cache_async_result(ttl_seconds: int = 300, key_prefix: str = \"cache\"):\n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            instance_args = args[1:] if args and hasattr(args[0], func.__name__) else args\n            cache_key = cache_manager._generate_key(f\"{key_prefix}:{func.__name__}\", *instance_args, **kwargs)\n\n            cached_result = cache_manager.get(cache_key)\n            if cached_result is not None:\n                log.debug(\"Cache hit\", function=func.__name__)\n                return cached_result\n\n            log.debug(\"Cache miss\", function=func.__name__)\n            result = await func(*args, **kwargs)\n            cache_manager.set(cache_key, result, ttl_seconds)\n            return result\n        return wrapper\n    return decorator\n",
    "python_service/checkmate_service.py": "# checkmate_service.py\n# The main service runner, upgraded to the final Endgame architecture.\n\nimport time\nimport logging\nimport sqlite3\nimport json\nimport os\nimport threading\nfrom datetime import datetime\nimport subprocess\nfrom .engine import SuperchargedOrchestrator, EnhancedTrifectaAnalyzer, Settings, Race\nfrom typing import List, Optional\n\nclass DatabaseHandler:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._setup_database()\n\n    def _get_connection(self):\n        return sqlite3.connect(self.db_path, timeout=10)\n\n    def _setup_database(self):\n        try:\n            # Correctly resolve paths from the service's location\n            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n            schema_path = os.path.join(base_dir, 'shared_database', 'schema.sql')\n            web_schema_path = os.path.join(base_dir, 'shared_database', 'web_schema.sql')\n\n            # Read both schema files\n            with open(schema_path, 'r') as f:\n                schema = f.read()\n            with open(web_schema_path, 'r') as f:\n                web_schema = f.read()\n\n            # Apply both schemas in a single transaction\n            with self._get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.executescript(schema)\n                cursor.executescript(web_schema)\n                conn.commit()\n            self.logger.info(\"CRITICAL SUCCESS: All database schemas (base + web) applied successfully.\")\n        except Exception as e:\n            self.logger.critical(f\"FATAL: Database setup failed. Other platforms will fail. Error: {e}\", exc_info=True)\n            raise\n\n    def update_races_and_status(self, races: List[Race], statuses: List[dict]):\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            for race in races:\n                cursor.execute(\"\"\"\n                    INSERT OR REPLACE INTO live_races\n                    (race_id, track_name, race_number, post_time, raw_data_json, checkmate_score, qualified, trifecta_factors_json, updated_at)\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\", (\n                    race.race_id, race.track_name, race.race_number, race.post_time,\n                    race.model_dump_json(), race.checkmate_score, race.is_qualified,\n                    race.trifecta_factors_json, datetime.now()\n                ))\n            for status in statuses:\n                cursor.execute(\"\"\"\n                    INSERT OR REPLACE INTO adapter_status (adapter_name, status, last_run, races_found, error_message, execution_time_ms)\n                    VALUES (?, ?, ?, ?, ?, ?)\n                \"\"\", (\n                    status.get('adapter_id'), status.get('status'), status.get('timestamp'),\n                    status.get('races_found'), status.get('error_message'), int(status.get('response_time', 0) * 1000)\n                ))\n\n            if races or statuses:\n                cursor.execute(\"INSERT INTO events (event_type, payload) VALUES (?, ?)\",\n                               ('RACES_UPDATED', json.dumps({'race_count': len(races)})))\n\n            conn.commit()\n        self.logger.info(f\"Database updated with {len(races)} races and {len(statuses)} adapter statuses.\")\n\nclass CheckmateBackgroundService:\n    def __init__(self):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        from dotenv import load_dotenv\n\n        dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')\n        load_dotenv(dotenv_path=dotenv_path)\n\n        db_path = os.getenv(\"CHECKMATE_DB_PATH\")\n        if not db_path:\n            self.logger.critical(\"FATAL: CHECKMATE_DB_PATH environment variable not set. Service cannot start.\")\n            raise ValueError(\"CHECKMATE_DB_PATH is not configured.\")\n\n        self.logger.info(f\"Database path loaded from environment: {db_path}\")\n\n        self.settings = Settings()\n        self.db_handler = DatabaseHandler(db_path)\n        self.orchestrator = SuperchargedOrchestrator(self.settings)\n        self.python_analyzer = EnhancedTrifectaAnalyzer(self.settings)\n        self.stop_event = threading.Event()\n        self.rust_engine_path = os.path.join(os.path.dirname(__file__), '..', 'rust_engine', 'target', 'release', 'checkmate_engine.exe')\n\n    def _analyze_with_rust(self, races: List[Race]) -> Optional[List[Race]]:\n        self.logger.info(\"Attempting analysis with external Rust engine.\")\n        try:\n            race_data_json = json.dumps([r.model_dump() for r in races])\n            result = subprocess.run(\n                [self.rust_engine_path],\n                input=race_data_json,\n                capture_output=True,\n                text=True,\n                check=True,\n                timeout=30\n            )\n            results_data = json.loads(result.stdout)\n            results_map = {res['race_id']: res for res in results_data}\n\n            for race in races:\n                if race.race_id in results_map:\n                    res = results_map[race.race_id]\n                    race.checkmate_score = res.get('checkmate_score')\n                    race.is_qualified = res.get('qualified')\n                    race.trifecta_factors_json = json.dumps(res.get('trifecta_factors'))\n            return races\n        except FileNotFoundError:\n            self.logger.warning(\"Rust engine not found. Falling back to Python analyzer.\")\n            return None\n        except (subprocess.CalledProcessError, json.JSONDecodeError, subprocess.TimeoutExpired) as e:\n            self.logger.error(f\"Rust engine execution failed: {e}. Falling back to Python analyzer.\")\n            return None\n\n    def _analyze_with_python(self, races: List[Race]) -> List[Race]:\n        self.logger.info(\"Performing analysis with internal Python engine.\")\n        return [self.python_analyzer.analyze_race_advanced(race) for race in races]\n\n    def run_continuously(self, interval_seconds: int = 60):\n        self.logger.info(\"Background service thread starting continuous run.\")\n\n        while not self.stop_event.is_set():\n            try:\n                self.logger.info(\"Starting data collection and analysis cycle.\")\n                races, statuses = self.orchestrator.get_races_parallel()\n\n                analyzed_races = None\n                if os.path.exists(self.rust_engine_path):\n                    analyzed_races = self._analyze_with_rust(races)\n\n                if analyzed_races is None: # Fallback condition\n                    analyzed_races = self._analyze_with_python(races)\n\n                if analyzed_races: # Ensure we have something to update\n                    self.db_handler.update_races_and_status(analyzed_races, statuses)\n\n            except Exception as e:\n                self.logger.critical(f\"Unhandled exception in service loop: {e}\", exc_info=True)\n\n            self.logger.info(f\"Cycle complete. Sleeping for {interval_seconds} seconds.\")\n            self.stop_event.wait(interval_seconds)\n        self.logger.info(\"Background service run loop has terminated.\")\n\n\n    def start(self):\n        self.stop_event.clear()\n        self.thread = threading.Thread(target=self.run_continuously)\n        self.thread.daemon = True\n        self.thread.start()\n        self.logger.info(\"CheckmateBackgroundService started.\")\n\n    def stop(self):\n        self.stop_event.set()\n        if hasattr(self, 'thread') and self.thread.is_alive():\n            self.thread.join(timeout=10)\n        self.logger.info(\"CheckmateBackgroundService stopped.\")",
    "python_service/config.py": "#!/usr/bin/env python3\n# ==============================================================================\n#  Fortuna Faucet: Centralized Configuration\n# =================================\u00e1=============================================\n# This module, restored by the Great Correction, provides a centralized and\n# validated source for all application settings using pydantic-settings.\n# ==============================================================================\n\nfrom pydantic_settings import BaseSettings\nfrom functools import lru_cache\nfrom typing import List, Optional\n\nclass Settings(BaseSettings):\n    # --- Application Security ---\n    API_KEY: str\n\n    # --- Betfair API Credentials ---\n    BETFAIR_APP_KEY: str = \"\"\n    BETFAIR_USERNAME: str = \"\"\n    BETFAIR_PASSWORD: str = \"\"\n\n    # --- Other Adapter Keys ---\n    TVG_API_KEY: str = \"\"\n    RACING_AND_SPORTS_TOKEN: str = \"\"\n    POINTSBET_API_KEY: str = \"\"\n    GREYHOUND_API_URL: Optional[str] = None\n    THE_RACING_API_KEY: Optional[str] = None\n\n    # --- Placeholder keys for restored scrapers (not currently used but good practice) ---\n    AT_THE_RACES_KEY: Optional[str] = None\n    SPORTING_LIFE_KEY: Optional[str] = None\n    TIMEFORM_KEY: Optional[str] = None\n\n    # --- Caching Configuration ---\n    REDIS_URL: str = \"redis://localhost\"\n\n    # --- CORS Configuration ---\n    ALLOWED_ORIGINS: List[str] = [\"http://localhost:3000\", \"http://localhost:3001\"]\n\n    model_config = {\n        \"env_file\": \".env\",\n        \"case_sensitive\": True\n    }\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"Returns a cached instance of the application settings.\"\"\"\n    return Settings()",
    "python_service/engine.py": "# python_service/engine.py\n\nimport asyncio\nimport structlog\nimport httpx\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Tuple\nfrom decimal import Decimal\nfrom datetime import timezone\n\nfrom .models import Race, Runner, OddsData, AggregatedResponse\nfrom .models_v3 import NormalizedRace, NormalizedRunner\nfrom .cache_manager import cache_async_result\nfrom .health import health_monitor\nfrom .adapters.base import BaseAdapter\nfrom .adapters.betfair_adapter import BetfairAdapter\nfrom .adapters.betfair_greyhound_adapter import BetfairGreyhoundAdapter\nfrom .adapters.racing_and_sports_greyhound_adapter import RacingAndSportsGreyhoundAdapter\nfrom .adapters.tvg_adapter import TVGAdapter\nfrom .adapters.racing_and_sports_adapter import RacingAndSportsAdapter\nfrom .adapters.at_the_races_adapter import AtTheRacesAdapter\nfrom .adapters.sporting_life_adapter import SportingLifeAdapter\nfrom .adapters.timeform_adapter import TimeformAdapter\nfrom .adapters.harness_adapter import HarnessAdapter\nfrom .adapters.the_racing_api_adapter import TheRacingApiAdapter\nfrom .adapters.gbgb_api_adapter import GbgbApiAdapter\nfrom .adapters.betfair_datascientist_adapter import BetfairDataScientistAdapter\n\nlog = structlog.get_logger(__name__)\n\nclass FortunaEngine:\n    def __init__(self, config=None):\n        from .config import get_settings\n        self.config = config or get_settings()\n        self.log = structlog.get_logger(self.__class__.__name__)\n        self.adapters: List[BaseAdapter] = [\n            BetfairAdapter(config=self.config),\n            BetfairGreyhoundAdapter(config=self.config),\n            TVGAdapter(config=self.config),\n            RacingAndSportsAdapter(config=self.config),\n            RacingAndSportsGreyhoundAdapter(config=self.config),\n            AtTheRacesAdapter(config=self.config),\n            SportingLifeAdapter(config=self.config),\n            TimeformAdapter(config=self.config),\n            TheRacingApiAdapter(config=self.config),\n            GbgbApiAdapter(config=self.config),\n            HarnessAdapter(config=self.config)\n        ]\n        # V3 ADAPTERS\n        self.v3_adapters = [\n            BetfairDataScientistAdapter(\n                model_name=\"ThoroughbredModel\",\n                url=\"https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kvs-ratings/datasets?id=thoroughbred-model&date=\"\n            )\n        ]\n        self.http_client = httpx.AsyncClient()\n\n    async def close(self):\n        await self.http_client.aclose()\n\n    def get_all_adapter_statuses(self) -> List[Dict[str, Any]]:\n        return [adapter.get_status() for adapter in self.adapters]\n\n    async def _time_adapter_fetch(self, adapter: BaseAdapter, date: str) -> Tuple[str, Dict[str, Any], float]:\n        \"\"\"Wraps an adapter's fetch call, catches all exceptions, and returns a consistent payload.\"\"\"\n        start_time = datetime.now()\n        try:\n            result = await adapter.fetch_races(date, self.http_client)\n            duration = (datetime.now() - start_time).total_seconds()\n            health_monitor.record_adapter_response(adapter.source_name, success=True, duration=duration)\n            return (adapter.source_name, result, duration)\n        except Exception as e:\n            duration = (datetime.now() - start_time).total_seconds()\n            log.error(\"Adapter raised an unhandled exception\", adapter=adapter.source_name, error=str(e), exc_info=True)\n            health_monitor.record_adapter_response(adapter.source_name, success=False, duration=duration)\n            failed_result = {\n                'races': [],\n                'source_info': {\n                    'name': adapter.source_name,\n                    'status': 'FAILED',\n                    'races_fetched': 0,\n                    'error_message': str(e),\n                    'fetch_duration': duration\n                }\n            }\n            return (adapter.source_name, failed_result, duration)\n\n    def _race_key(self, race: Race) -> str:\n        return f\"{race.venue.lower().strip()}|{race.race_number}|{race.start_time.strftime('%H:%M')}\"\n\n    def _dedupe_races(self, races: List[Race]) -> List[Race]:\n        \"\"\"Deduplicates races from multiple sources and reconciles odds.\"\"\"\n        race_map: Dict[str, Race] = {}\n        for race in races:\n            # Use a robust key: venue, date, and race number\n            key = f\"{race.venue.upper()}-{race.start_time.strftime('%Y-%m-%d')}-{race.race_number}\"\n\n            if key not in race_map:\n                race_map[key] = race\n            else:\n                # Merge runners and odds into the existing race object\n                existing_race = race_map[key]\n                runner_map = {r.number: r for r in existing_race.runners}\n\n                for new_runner in race.runners:\n                    if new_runner.number in runner_map:\n                        # Runner exists, reconcile odds\n                        existing_runner = runner_map[new_runner.number]\n                        updated_odds = existing_runner.odds.copy()\n                        updated_odds.update(new_runner.odds)\n                        existing_runner.odds = updated_odds\n                    else:\n                        # New runner, add to the existing race\n                        existing_race.runners.append(new_runner)\n\n                # Update source count\n                existing_race.source += f\", {race.source}\"\n\n        return list(race_map.values())\n\n    async def get_races(self, date: str, background_tasks: set, source_filter: str = None) -> Dict[str, Any]:\n        if source_filter:\n            self.log.info(\"Bypassing cache for source-specific request\", source=source_filter)\n            return await self._fetch_races_from_sources(date, source_filter=source_filter)\n\n        return await self._get_all_races_cached(date, background_tasks=background_tasks)\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"fortuna_engine_races\")\n    async def _get_all_races_cached(self, date: str, background_tasks: set) -> Dict[str, Any]:\n        \"\"\"This method fetches races for all sources and its result is cached.\"\"\"\n        self.log.info(\"CACHE MISS: Fetching all races from sources.\", date=date)\n        return await self._fetch_races_from_sources(date)\n\n    def _convert_v3_race_to_v2(self, v3_race: NormalizedRace) -> Race:\n        \"\"\"Converts a V3 NormalizedRace object to a V2 Race object.\"\"\"\n        import re\n        race_number = 0\n        match = re.search(r'\\d+', v3_race.race_name)\n        if match:\n            race_number = int(match.group())\n\n        runners = []\n        for v3_runner in v3_race.runners:\n            odds_data = OddsData(\n                win=Decimal(str(v3_runner.odds_decimal)),\n                source=v3_race.source_ids[0],\n                last_updated=datetime.now(timezone.utc)\n            )\n            runner = Runner(\n                id=v3_runner.runner_id,\n                name=v3_runner.name,\n                number=int(v3_runner.saddle_cloth) if v3_runner.saddle_cloth and v3_runner.saddle_cloth.isdigit() else 99,\n                odds={v3_race.source_ids[0]: odds_data}\n            )\n            runners.append(runner)\n\n        return Race(\n            id=v3_race.race_key,\n            venue=v3_race.track_key,\n            race_number=race_number,\n            start_time=datetime.fromisoformat(v3_race.start_time_iso),\n            runners=runners,\n            source=v3_race.source_ids[0],\n            race_name=v3_race.race_name\n        )\n\n    @cache_async_result(ttl_seconds=300, key_prefix=\"odds_engine_fetch\")\n    async def _fetch_races_from_sources(self, date: str, source_filter: str = None) -> Dict[str, Any]:\n        \"\"\"Helper method to contain the logic for fetching and aggregating races.\"\"\"\n        target_adapters = self.adapters\n        if source_filter:\n            target_adapters = [a for a in self.adapters if a.source_name.lower() == source_filter.lower()]\n\n        tasks = [self._time_adapter_fetch(adapter, date) for adapter in target_adapters]\n\n        # Run V3 (synchronous) adapters in a thread pool\n        v3_tasks = [asyncio.to_thread(adapter.fetch_and_normalize) for adapter in self.v3_adapters]\n\n        # Gather results from both V2 (async) and V3 (sync) adapters\n        all_tasks = tasks + v3_tasks\n        results = await asyncio.gather(*all_tasks, return_exceptions=True)\n\n        source_infos = []\n        all_races = []\n\n        v3_start_time = datetime.now() # Approximate start time for all V3 adapters\n\n        for result in results:\n            try:\n                if isinstance(result, Exception):\n                    self.log.error(\"Adapter fetch failed\", error=result, exc_info=False)\n                    continue\n\n                # Correctly differentiate between V2 and V3 results\n                if isinstance(result, tuple) and len(result) == 3:  # V2 Adapter Result\n                    adapter_name, adapter_result, duration = result\n                    source_info = adapter_result.get('source_info', {})\n                    source_info['fetch_duration'] = round(duration, 2)\n                    source_infos.append(source_info)\n                    if source_info.get('status') == 'SUCCESS':\n                        all_races.extend(adapter_result.get('races', []))\n                elif isinstance(result, list) and all(isinstance(r, NormalizedRace) for r in result):  # V3 Adapter Result\n                    if result:\n                        v3_races = result\n                        adapter_name = v3_races[0].source_ids[0]\n                        translated_races = [self._translate_v3_race_to_v2(nr) for nr in result]\n                        all_races.extend(translated_races)\n\n                        v3_duration = (datetime.now() - v3_start_time).total_seconds()\n                        source_infos.append({\n                            'name': adapter_name,\n                            'status': 'SUCCESS',\n                            'races_fetched': len(translated_races),\n                            'error_message': None,\n                            'fetch_duration': round(v3_duration, 2)\n                        })\n            except Exception:\n                self.log.error(\"Failed to process result from an adapter.\", exc_info=True)\n\n        deduped_races = self._dedupe_races(all_races)\n\n        response_obj = AggregatedResponse(\n            date=datetime.strptime(date, '%Y-%m-%d').date(),\n            races=deduped_races,\n            sources=source_infos,\n            metadata={\n                'fetch_time': datetime.now(),\n                'sources_queried': [a.source_name for a in target_adapters],\n                'sources_successful': len([s for s in source_infos if s['status'] == 'SUCCESS']),\n                'total_races': len(deduped_races)\n            }\n        )\n\n        # --- Windows Operator Experience Enhancement ---\n        try:\n            from win10toast import ToastNotifier\n            toaster = ToastNotifier()\n            toaster.show_toast(\n                \"Fortuna Faucet\",\n                f\"Initial data load complete. {len(deduped_races)} races are ready for analysis.\",\n                duration=10,\n                threaded=True\n            )\n        except (ImportError, RuntimeError):\n            # Fails gracefully if not on Windows or library is missing\n            pass\n\n        return response_obj.model_dump()\n\n\n    def _translate_v3_race_to_v2(self, norm_race: NormalizedRace) -> Race:\n        \"\"\"Translates a V3 NormalizedRace into a V2 Race object.\"\"\"\n        runners = []\n        for norm_runner in norm_race.runners:\n            adapter_name = norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\"\n            odds_data = OddsData(\n                win=Decimal(str(norm_runner.odds_decimal)),\n                source=adapter_name,\n                last_updated=datetime.now(timezone.utc)\n            )\n\n            try:\n                runner_number = int(norm_runner.saddle_cloth)\n            except (ValueError, TypeError):\n                runner_number = None\n\n            runner = Runner(\n                id=norm_runner.runner_id,\n                name=norm_runner.name,\n                number=runner_number,\n                odds={adapter_name: odds_data}\n            )\n            runners.append(runner)\n\n        return Race(\n            id=norm_race.race_key,\n            venue=norm_race.track_key,\n            start_time=datetime.fromisoformat(norm_race.start_time_iso),\n            race_number=0, # V3 model does not have race_number, placeholder\n            runners=runners,\n            source=norm_race.source_ids[0] if norm_race.source_ids else \"UnknownV3\",\n            # Store extra V3 data in metadata for future use\n            metadata={\"v3_race_name\": norm_race.race_name}\n        )\n",
    "python_service/health.py": "# python_service/health.py\nfrom fastapi import APIRouter\nfrom datetime import datetime\nfrom typing import Dict, List\nimport psutil\nimport structlog\n\nrouter = APIRouter()\nlog = structlog.get_logger(__name__)\n\nclass HealthMonitor:\n    def __init__(self):\n        self.adapter_health: Dict[str, Dict] = {}\n        self.system_metrics: List[Dict] = []\n        self.max_metrics_history = 100\n\n    def record_adapter_response(self, adapter_name: str, success: bool, duration: float):\n        if adapter_name not in self.adapter_health:\n            self.adapter_health[adapter_name] = {\n                \"total_requests\": 0,\n                \"successful_requests\": 0,\n                \"failed_requests\": 0,\n                \"avg_response_time\": 0.0,\n                \"last_success\": None,\n                \"last_failure\": None\n            }\n\n        health = self.adapter_health[adapter_name]\n        health[\"total_requests\"] += 1\n\n        if success:\n            health[\"successful_requests\"] += 1\n            health[\"last_success\"] = datetime.now().isoformat()\n        else:\n            health[\"failed_requests\"] += 1\n            health[\"last_failure\"] = datetime.now().isoformat()\n\n        health[\"avg_response_time\"] = (\n            (health[\"avg_response_time\"] * (health[\"total_requests\"] - 1) + duration) /\n            health[\"total_requests\"]\n        )\n\n    def get_system_metrics(self) -> Dict:\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory = psutil.virtual_memory()\n        disk = psutil.disk_usage('/')\n\n        metrics = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"cpu_percent\": cpu_percent,\n            \"memory_percent\": memory.percent,\n            \"memory_available_gb\": round(memory.available / (1024 ** 3), 2),\n            \"disk_percent\": disk.percent,\n            \"disk_free_gb\": round(disk.free / (1024 ** 3), 2)\n        }\n\n        self.system_metrics.append(metrics)\n        if len(self.system_metrics) > self.max_metrics_history:\n            self.system_metrics.pop(0)\n\n        return metrics\n\n    def get_health_report(self) -> Dict:\n        system_metrics = self.get_system_metrics()\n        return {\n            \"status\": \"healthy\" if self.is_system_healthy() else \"degraded\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"system\": system_metrics,\n            \"adapters\": self.adapter_health,\n            \"metrics_history\": self.system_metrics[-10:]\n        }\n\n    def is_system_healthy(self) -> bool:\n        if not self.system_metrics: return True\n        latest = self.system_metrics[-1]\n        return (\n            latest[\"cpu_percent\"] < 80 and\n            latest[\"memory_percent\"] < 85 and\n            latest[\"disk_percent\"] < 90\n        )\n\n# Global instance for the application to use\nhealth_monitor = HealthMonitor()\n\n@router.get(\"/health/detailed\", tags=[\"Health\"])\nasync def get_detailed_health():\n    \"\"\"Provides a comprehensive health check of the system.\"\"\"\n    return health_monitor.get_health_report()\n\n@router.get(\"/health\", tags=[\"Health\"])\nasync def get_basic_health():\n    \"\"\"Provides a basic health check for load balancers and uptime monitoring.\"\"\"\n    return {\"status\": \"ok\", \"timestamp\": datetime.now().isoformat()}\n",
    "python_service/middleware/error_handler.py": "# python_service/middleware/error_handler.py\nfrom fastapi import Request, status\nfrom fastapi.responses import JSONResponse\nimport structlog\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any\n\nlog = structlog.get_logger(__name__)\n\nclass ErrorRecoveryMiddleware:\n    def __init__(self, app):\n        self.app = app\n        self.error_counts = {}\n        self.circuit_breaker_open = {}\n\n    async def __call__(self, scope, receive, send):\n        if scope[\"type\"] != \"http\":\n            await self.app(scope, receive, send)\n            return\n\n        # Correctly extract adapter from raw scope without consuming the request body\n        adapter_name = self._extract_adapter_from_scope(scope)\n\n        if adapter_name and self._is_circuit_open(adapter_name):\n            log.warning(\"Circuit breaker is open for adapter\", adapter=adapter_name)\n            response = JSONResponse(\n                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n                content={\"error\": f\"Service temporarily unavailable for adapter '{adapter_name}' due to repeated failures.\"}\n            )\n            await response(scope, receive, send)\n            return\n\n        try:\n            await self.app(scope, receive, send)\n        except Exception as exc:\n            # The error handler needs a request object, but we must be careful.\n            # We create it here, knowing that for GET requests this is safe.\n            # A more advanced implementation might be needed for POST requests if they\n            # were to trigger adapter-specific errors.\n            request_on_error = Request(scope, receive)\n            await self._handle_error(request_on_error, exc, send)\n\n    async def _handle_error(self, request: Request, exc: Exception, send):\n        error_id = f\"{type(exc).__name__}_{datetime.now().timestamp()}\"\n        log.error(\"Unhandled error\", error_id=error_id, path=request.url.path, error_type=type(exc).__name__, exc_info=True)\n\n        adapter_name = self._extract_adapter_from_request(request)\n        if adapter_name:\n            self._increment_error_count(adapter_name)\n            if self._should_open_circuit(adapter_name):\n                log.warning(\"Circuit breaker opened for adapter\", adapter=adapter_name)\n                self.circuit_breaker_open[adapter_name] = datetime.now()\n\n        response = JSONResponse(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            content={\"error\": \"Internal Server Error\", \"error_id\": error_id}\n        )\n        # The response must be awaited with the original scope, receive, and send callables\n        await response(request.scope, request.receive, send)\n\n    def _extract_adapter_from_scope(self, scope: Dict[str, Any]) -> str | None:\n        \"\"\"Extracts the adapter name from the 'source' query parameter in the raw scope.\"\"\"\n        query_string = scope.get(\"query_string\", b\"\").decode()\n        if \"source=\" in query_string:\n            params = dict(param.split(\"=\") for param in query_string.split(\"&\"))\n            return params.get(\"source\")\n        return None\n\n    def _increment_error_count(self, adapter_name: str):\n        if adapter_name not in self.error_counts:\n            self.error_counts[adapter_name] = []\n        self.error_counts[adapter_name].append(datetime.now())\n        cutoff = datetime.now() - timedelta(minutes=5)\n        self.error_counts[adapter_name] = [t for t in self.error_counts[adapter_name] if t > cutoff]\n\n    def _should_open_circuit(self, adapter_name: str) -> bool:\n        return len(self.error_counts.get(adapter_name, [])) > 5\n\n    def _is_circuit_open(self, adapter_name: str) -> bool:\n        if adapter_name not in self.circuit_breaker_open:\n            return False\n        opened_at = self.circuit_breaker_open[adapter_name]\n        if (datetime.now() - opened_at) > timedelta(minutes=10):\n            del self.circuit_breaker_open[adapter_name]\n            log.info(\"Circuit breaker auto-closed for adapter\", adapter=adapter_name)\n            return False\n        return True\n",
    "python_service/models.py": "# python_service/models.py\n\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom typing import Dict, List, Optional, Any\nfrom pydantic import BaseModel, Field\n\n# --- Configuration for Aliases (BUG #4 Fix) ---\nclass FortunaBaseModel(BaseModel):\n    class Config:\n        populate_by_name = True\n        arbitrary_types_allowed = True\n\n# --- Core Data Models ---\nclass OddsData(FortunaBaseModel):\n    win: Optional[Decimal] = None\n    place: Optional[Decimal] = None\n    show: Optional[Decimal] = None\n    source: str\n    last_updated: datetime\n\nclass Runner(FortunaBaseModel):\n    id: Optional[str] = None\n    name: str\n    number: Optional[int] = Field(None, alias='saddleClothNumber')\n    scratched: bool = False\n    odds: Dict[str, OddsData] = {}\n    jockey: Optional[str] = None\n    trainer: Optional[str] = None\n\nclass Race(FortunaBaseModel):\n    id: str\n    venue: str\n    race_number: int = Field(..., alias='raceNumber')\n    start_time: datetime = Field(..., alias='startTime')\n    runners: List[Runner]\n    source: str\n    qualification_score: Optional[float] = Field(None, alias='qualificationScore')\n    favorite: Optional[Runner] = None\n    race_name: Optional[str] = None\n    distance: Optional[str] = None\n\nclass SourceInfo(FortunaBaseModel):\n    name: str\n    status: str\n    races_fetched: int = Field(..., alias='racesFetched')\n    fetch_duration: float = Field(..., alias='fetchDuration')\n    error_message: Optional[str] = Field(None, alias='errorMessage')\n\nclass AggregatedResponse(FortunaBaseModel):\n    races: List[Race]\n    source_info: List[SourceInfo] = Field(..., alias='sourceInfo')\n\nclass QualifiedRacesResponse(FortunaBaseModel):\n    criteria: Dict[str, Any]\n    races: List[Race]\n\nclass TipsheetRace(FortunaBaseModel):\n    race_id: str = Field(..., alias='raceId')\n    track_name: str = Field(..., alias='trackName')\n    race_number: int = Field(..., alias='raceNumber')\n    post_time: str = Field(..., alias='postTime')\n    score: float\n    factors: Any # JSON string stored as Any\n",
    "python_service/models_v3.py": "# python_service/models_v3.py\n# Defines the data structures for the V3 adapter architecture.\n\nfrom dataclasses import dataclass, field\nfrom typing import List\n\n@dataclass\nclass NormalizedRunner:\n    runner_id: str\n    name: str\n    saddle_cloth: str\n    odds_decimal: float\n\n@dataclass\nclass NormalizedRace:\n    race_key: str\n    track_key: str\n    start_time_iso: str\n    race_name: str\n    runners: List[NormalizedRunner] = field(default_factory=list)\n    source_ids: List[str] = field(default_factory=list)\n",
    "python_service/windows_service_wrapper.py": "# windows_service_wrapper.py\n\nimport servicemanager\nimport win32service\nimport win32serviceutil\nimport win32event\nimport sys\nimport os\nimport logging\n\n# Add the service's directory to the Python path\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom checkmate_service import CheckmateBackgroundService\n\nclass CheckmateWindowsService(win32serviceutil.ServiceFramework):\n    _svc_name_ = \"CheckmateV8Service\"\n    _svc_display_name_ = \"Checkmate V8 Racing Analysis Service\"\n    _svc_description_ = \"Continuously fetches and analyzes horse racing data.\"\n\n    def __init__(self, args):\n        win32serviceutil.ServiceFramework.__init__(self, args)\n        self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)\n        self.checkmate_service = CheckmateBackgroundService()\n        # Configure logging to use the Windows Event Log\n        logging.basicConfig(level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s', handlers=[servicemanager.LogHandler()])\n\n    def SvcStop(self):\n        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)\n        self.checkmate_service.stop()\n        win32event.SetEvent(self.hWaitStop)\n        self.ReportServiceStatus(win32service.SERVICE_STOPPED)\n\n    def SvcDoRun(self):\n        servicemanager.LogMsg(servicemanager.EVENTLOG_INFORMATION_TYPE, servicemanager.PYS_SERVICE_STARTED, (self._svc_name_, ''))\n        self.main()\n\n    def main(self):\n        self.checkmate_service.start()\n        win32event.WaitForSingleObject(self.hWaitStop, win32event.INFINITE)\n\nif __name__ == '__main__':\n    if len(sys.argv) == 1:\n        servicemanager.Initialize()\n        servicemanager.PrepareToHostSingle(CheckmateWindowsService)\n        servicemanager.StartServiceCtrlDispatcher()\n    else:\n        win32serviceutil.HandleCommandLine(CheckmateWindowsService)"
}